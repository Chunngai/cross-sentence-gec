{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text: str) -> str:\n",
    "    return \"\".join(text.split())\n",
    "\n",
    "def sentence_in_raw_document(sentence: str, raw_document: str) -> bool:\n",
    "    return remove_spaces(sentence) in remove_spaces(raw_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from glob import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "basename = os.path.basename\n",
    "splitext = os.path.splitext\n",
    "\n",
    "\n",
    "def get_tokenized_sentences_from_m2_files(m2_file_paths: List[str]) -> List[str]:\n",
    "    sentence_start_index = 2\n",
    "    tokenized_sentences: List[str] = []\n",
    "    for m2_file_path in m2_file_paths:\n",
    "        with open(m2_file_path) as m2_file:\n",
    "            for line in m2_file.readlines():\n",
    "                if line.startswith(\"S \"):  # \"S \" marks the start of a sentence.\n",
    "                    tokenized_sentence = line[sentence_start_index:].strip()  # Removes the trailing linefeed.\n",
    "                    tokenized_sentences.append(tokenized_sentence)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_documents_from_json(document_file_paths: List[str], m2_file_paths: List[str]) -> List[List[str]]:\n",
    "    def _normalize(text) -> str:\n",
    "        norm_dict = {\"’\": \"'\",\n",
    "                     \"´\": \"'\",\n",
    "                     \"‘\": \"'\",\n",
    "                     \"′\": \"'\",\n",
    "                     \"`\": \"'\",\n",
    "                     '“': '\"',\n",
    "                     '”': '\"',\n",
    "                     '˝': '\"',\n",
    "                     '¨': '\"',\n",
    "                     '„': '\"',\n",
    "                     '『': '\"',\n",
    "                     '』': '\"',\n",
    "                     '–': '-',\n",
    "                     '—': '-',\n",
    "                     '―': '-',\n",
    "                     '¬': '-',\n",
    "                     '、': ',',\n",
    "                     '，': ',',\n",
    "                     '：': ':',\n",
    "                     '；': ';',\n",
    "                     '？': '?',\n",
    "                     '！': '!',\n",
    "                     'ِ': ' ',\n",
    "                     '\\u200b': ' '}\n",
    "        norm_dict = {ord(k): v for k, v in norm_dict.items()}\n",
    "        return text.translate(norm_dict)\n",
    "    \n",
    "    # Extracts tokenized sentences.\n",
    "    tokenized_sentences = get_tokenized_sentences_from_m2_files(m2_file_paths=m2_file_paths)\n",
    "    \n",
    "    # Extracts documents.\n",
    "    documents: List[List[str]] = []\n",
    "    current_tokenized_sentence_index = 0\n",
    "    current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            # Each line in the document file is a json object.\n",
    "            raw_json_objects: List[str] = document_file.readlines()\n",
    "            \n",
    "            for raw_json_object in raw_json_objects:\n",
    "                json_object = json.loads(raw_json_object)\n",
    "                \n",
    "                # Gets a raw document (simple text) and nomalizes it.\n",
    "                raw_document: str = json_object[\"text\"]\n",
    "                normalized_raw_document: str = _normalize(raw_document)\n",
    "                \n",
    "                document: List[str] = []\n",
    "                partial_normalized_raw_document = remove_spaces(normalized_raw_document)\n",
    "                while sentence_in_raw_document(sentence=current_tokenized_sentence, raw_document=partial_normalized_raw_document):\n",
    "                    document.append(current_tokenized_sentence)\n",
    "                    \n",
    "                    # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "                    # E.g. a sentence exists in 2 consecutive documents.\n",
    "                    partial_normalized_raw_document = partial_normalized_raw_document.replace(remove_spaces(current_tokenized_sentence), \"\", 1)\n",
    "                    \n",
    "                    # Evaluates the next sentence which is extracted from a .m2 file.\n",
    "                    current_tokenized_sentence_index += 1\n",
    "                    try:\n",
    "                        current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "                    except IndexError:  # When the error occurs, the last sentence has been evaluated.\n",
    "                        break\n",
    "                                    \n",
    "                documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents_from_sgml(document_file_paths: List[str], m2_file_paths: List[str], dataset_name: str) -> List[str]:\n",
    "    # Extracts tokenized sentences.\n",
    "    tokenized_sentences = get_tokenized_sentences_from_m2_files(m2_file_paths=m2_file_paths)\n",
    "    \n",
    "    # Extracts documents.\n",
    "    documents: List[List[str]] = []\n",
    "    current_tokenized_sentence_index = 0\n",
    "    current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            raw_sgml: str = document_file.read()\n",
    "            \n",
    "            # Fixes a parsing error in nucle.\n",
    "            if dataset_name == \"nucle\":\n",
    "                raw_sgml = raw_sgml.replace(\"<nuclearstreet.com/files/folders/1654/download.aspx>\", \"nuclearstreet.com/filesfolders/1654/download.aspx>\")\n",
    "\n",
    "            doc_pattern = re.compile(r\"<DOC.+?/DOC>\", flags=re.DOTALL)\n",
    "            docs = doc_pattern.findall(raw_sgml)\n",
    "            for doc in docs:\n",
    "#                 nid_pattern = re.compile(r'<DOC nid=\"(\\d+)\">')\n",
    "#                 nid = nid_pattern.search(doc).group(1)\n",
    "                \n",
    "                partial_normalized_raw_document: str = \"\"\n",
    "                    \n",
    "                title_pattern = re.compile(r\"<TITLE>(.+?)</TITLE>\", flags=re.DOTALL)\n",
    "                try:\n",
    "                    title = title_pattern.search(doc).group(1).strip()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                else:\n",
    "                    partial_normalized_raw_document += title\n",
    "                \n",
    "                p_pattern = re.compile(r\"<P>(.+?)</?P>\", flags=re.DOTALL)\n",
    "                paras: List[str] = [para.strip() for para in p_pattern.findall(doc)]\n",
    "                partial_normalized_raw_document += \" \".join(paras)\n",
    "                \n",
    "                # Fixes parsing errors in nucle.\n",
    "                if dataset_name == \"nucle\":\n",
    "                    fake_tag_pattern = re.compile(r\"<[^(\\{)|^(\\s)].+?>\")\n",
    "                    partial_normalized_raw_document = fake_tag_pattern.sub(\"\", partial_normalized_raw_document)\n",
    "    \n",
    "                partial_normalized_raw_document = remove_spaces(partial_normalized_raw_document)\n",
    "    \n",
    "#                 condition = False\n",
    "# #                 condition = 2281 <= int(nid) <= 2284\n",
    "#                 if condition:\n",
    "#                     print(f\"nid:\\t{nid}\")\n",
    "#                     print(\"------\")\n",
    "#                     try:\n",
    "#                         print(f\"title:\\t{title}\")\n",
    "#                         print(\"------\")\n",
    "#                     except:\n",
    "#                         pass\n",
    "#                     print(f\"paras:\\t{paras}\")\n",
    "#                     print(\"------\")\n",
    "#                     try:\n",
    "#                         print(f\"refs:\\t{reference_paras}\")\n",
    "#                         print(\"------\")\n",
    "#                     except:\n",
    "#                         pass\n",
    "#                     print(f\"source: {current_tokenized_sentence}\")\n",
    "#                     print(\"------\")\n",
    "#                     print(f\"source rm spaces: {remove_spaces(current_tokenized_sentence)}\")\n",
    "#                     print(\"------\")\n",
    "#                     print(f\"partial:\\t{partial_normalized_raw_document}\")\n",
    "#                     print(\"------\")\n",
    "#                     print(remove_spaces(current_tokenized_sentence) in partial_normalized_raw_document)\n",
    "#                     print()\n",
    "# #                     input()\n",
    "    \n",
    "                document: List[str] = []\n",
    "                while sentence_in_raw_document(sentence=current_tokenized_sentence, raw_document=partial_normalized_raw_document):\n",
    "                    document.append(current_tokenized_sentence)\n",
    "                    \n",
    "#                     if condition:\n",
    "#                         print(f\"nid:\\t{nid}\")\n",
    "#                         print(\"======\")\n",
    "#                         try:\n",
    "#                             print(f\"title:\\t{title}\")\n",
    "#                             print(\"======\")\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         print(f\"paras:\\t{paras}\")\n",
    "#                         print(\"======\")\n",
    "#                         try:\n",
    "#                             print(f\"refs:\\t{reference_paras}\")\n",
    "#                             print(\"======\")\n",
    "#                         except:\n",
    "#                             pass\n",
    "#                         print(f\"source: {current_tokenized_sentence}\")\n",
    "#                         print(\"======\")\n",
    "#                         print(f\"source rm spaces: {remove_spaces(current_tokenized_sentence)}\")\n",
    "#                         print(\"======\")\n",
    "#                         print(f\"partial:\\t{partial_normalized_raw_document}\")\n",
    "#                         print(\"======\")\n",
    "#                         print(remove_spaces(current_tokenized_sentence) in partial_normalized_raw_document)\n",
    "#                         print()\n",
    "# #                         input()\n",
    "                    \n",
    "                    # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "                    # E.g. a sentence exists in 2 consecutive documents.\n",
    "                    partial_normalized_raw_document = partial_normalized_raw_document.replace(remove_spaces(current_tokenized_sentence), \"\", 1)\n",
    "                    \n",
    "                    # Evaluates the next sentence which is extracted from a .m2 file.\n",
    "                    current_tokenized_sentence_index += 1\n",
    "                    try:\n",
    "                        current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "                    except IndexError:  # When the error occurs, the last sentence has been evaluated.\n",
    "                        break\n",
    "                \n",
    "                documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents_from_lang8_entries_train(document_file_paths: List[str]) -> List[str]:\n",
    "    # Extracts documents.\n",
    "    documents = []\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            lines: List[str] = document_file.readlines()\n",
    "            \n",
    "            document: List[str] = []\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    document_sentence = line.split(\"\\t\")[4].strip()\n",
    "                    document.append(document_sentence)\n",
    "                except IndexError:  # An '\\n' seperating 2 documents.\n",
    "                    documents.append(document)\n",
    "                    document = []\n",
    "    \n",
    "    return documents\n",
    "    \n",
    "\n",
    "def get_documents(document_file_paths: List[str], m2_file_paths: List[str], source_file_path: str) -> List[str]:\n",
    "    # Gets the name of the dataset.\n",
    "    dataset_name = splitext(basename(source_file_path))[0]\n",
    "    \n",
    "    if dataset_name in [\"fce\", \"wi.train\", \"wi.dev\"]:  # json.\n",
    "        return get_documents_from_json(document_file_paths, m2_file_paths)\n",
    "    elif dataset_name in [\"conll2013\", \"conll2014\", \"nucle\"]:  # sgml.\n",
    "        return get_documents_from_sgml(document_file_paths, m2_file_paths, dataset_name)\n",
    "    else:  # lang8.\n",
    "        return get_documents_from_lang8_entries_train(document_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(document_level_index: int, document: List[str], \n",
    "               previous_sentences_number: int, following_sentences_number: int) -> str:\n",
    "\n",
    "    def _is_valid_document_level_index(document_level_index: int, document: List[str]) -> bool:\n",
    "        # Negative indices, which may be generated when making previous context, \n",
    "        # are not allowed, as corresponding sentences are following context \n",
    "        # for the sentence being evaluated.\n",
    "        return 0 <= document_level_index < len(document)\n",
    "    \n",
    "    # Gets previous context.\n",
    "    previous_context_sentences = \"\"\n",
    "    for previous_context_sentence_index in range(document_level_index - previous_sentences_number, document_level_index):\n",
    "        if _is_valid_document_level_index(document_level_index=previous_context_sentence_index, document=document):\n",
    "            previous_context_sentence = document[previous_context_sentence_index]\n",
    "            previous_context_sentences = previous_context_sentences + \"<prev>\" + previous_context_sentence\n",
    "            \n",
    "    # Gets following context.\n",
    "    following_context_sentences = \"\"\n",
    "    for following_context_sentence_index in range(document_level_index + 1, document_level_index + following_sentences_number + 1):\n",
    "        if _is_valid_document_level_index(document_level_index=following_context_sentence_index, document=document):\n",
    "            following_context_sentence = document[following_context_sentence_index]\n",
    "            following_context_sentences = following_context_sentences + \"<fol>\" + following_context_sentence\n",
    "            \n",
    "    context = previous_context_sentences + following_context_sentences\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context(source_file_path: str, document_file_paths: List[str], m2_file_paths: List[str], previous_sentences_number: int, following_sentences_number: int):\n",
    "    # Gets all source sentences from `ori_path`.\n",
    "    with open(source_file_path) as source_file:\n",
    "        source_sentences = source_file.readlines()\n",
    "    \n",
    "    # Gets all documents.\n",
    "    documents: List[List[str]] = get_documents(document_file_paths=document_file_paths, m2_file_paths=m2_file_paths, source_file_path=source_file_path)\n",
    "    \n",
    "    # Writes documents.\n",
    "    if save_documents:\n",
    "        document_path = f\"{splitext(source_file_path)[0]}.documents\"\n",
    "        with open(f\"{document_path}\", 'w') as f:\n",
    "            for i in range(len(documents)):\n",
    "                f.write(f\"{i}\\t\")\n",
    "                f.write(str(documents[i]))\n",
    "                f.write(\"\\n\\n\")\n",
    "    \n",
    "    # Gets the path of the context file.\n",
    "    source_file_path_without_extention = splitext(source_file_path)[0]\n",
    "    context_file_path = f\"{source_file_path_without_extention}.ctx\"\n",
    "\n",
    "    global verbose\n",
    "    if verbose:\n",
    "        count = 0\n",
    "\n",
    "    current_document_index = 0\n",
    "    current_document: List[str] = documents[current_document_index]\n",
    "    current_document_masked_spaces_removed: List[str] = [remove_spaces(document_sentence)\n",
    "                                                         for document_sentence in current_document]\n",
    "    previous_source_sentence = None\n",
    "    previous_source_sentence_document_level_index = None\n",
    "    with open(context_file_path, \"w\") as context_file:\n",
    "        for source_sentence in source_sentences:   \n",
    "            if source_sentence != previous_source_sentence:\n",
    "                # In most cases the loop will be executed only once. \n",
    "                # But for nucle the loop may be executed twice \n",
    "                # when no sentence is wrong in a document in nucle\n",
    "                # since correct sentences in the m2 file of nucle \n",
    "                # are ignored when creating nucle.ori.\n",
    "                while  remove_spaces(source_sentence) not in current_document_masked_spaces_removed:\n",
    "                    current_document_index += 1\n",
    "                    current_document = documents[current_document_index]\n",
    "                    current_document_masked_spaces_removed: List[str] = [remove_spaces(document_sentence)\n",
    "                                                             for document_sentence in current_document]\n",
    "\n",
    "                # Gets the document-level index of the source sentence.\n",
    "                try:\n",
    "                    document_level_index = current_document_masked_spaces_removed.index(remove_spaces(source_sentence))\n",
    "                except:\n",
    "                    print(source_sentence)\n",
    "                    print(remove_spaces(source_sentence))\n",
    "                    print(current_document_masked_spaces_removed)\n",
    "                    raise\n",
    "            else:\n",
    "                document_level_index = previous_source_sentence_document_level_index\n",
    "            \n",
    "            previous_source_sentence = source_sentence\n",
    "            previous_source_sentence_document_level_index = document_level_index\n",
    "            \n",
    "            if verbose:\n",
    "                count += 1\n",
    "                print(f\"ln:\\t{count}\")\n",
    "                print(\"---\")\n",
    "                print(f\"document-level index:\\t{document_level_index}\")\n",
    "                print(\"---\")\n",
    "                print(f\"src sent:\\t{source_sentence}\")\n",
    "                print(f\"src sent spaces removed:\\t{remove_spaces(source_sentence)}\")\n",
    "                print(\"---\")\n",
    "                print(f\"current doc masked spaces removed:\\t{current_document_masked_spaces_removed}\")\n",
    "                print(\"---\")\n",
    "                print()\n",
    "                input()\n",
    "            \n",
    "            # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "            # E.g. a sentence exists in 2 consecutive documents.\n",
    "            current_document_masked_spaces_removed[document_level_index] = \"\"\n",
    "            \n",
    "            context = get_context(document_level_index=document_level_index, document=current_document, \n",
    "                                 previous_sentences_number=previous_sentences_number, following_sentences_number=following_sentences_number)\n",
    "            \n",
    "            # Writes the context to file.\n",
    "            context_file.write(context)\n",
    "            context_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fce: True\n",
      "wi.train: True\n",
      "wi.dev: True\n",
      "conll2013: True\n",
      "conll2014: True\n",
      "nucle: True\n",
      "lang8: True\n"
     ]
    }
   ],
   "source": [
    "import filepath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    fp = filepath.FilePath()\n",
    "    \n",
    "    # fce.\n",
    "    fce_ori_path = fp.FCE_ORI\n",
    "    fce_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/fce/json/fce.*.json\"))\n",
    "    fce_m2_paths = sorted(glob(f'{fp.fce_m2}/*m2'))\n",
    "\n",
    "    # wi train.\n",
    "    wi_train_ori_path = fp.WI_TRAIN_ORI\n",
    "    wi_train_doc_paths = [\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "    ]\n",
    "    wi_train_m2_paths = sorted(glob(f'{fp.wi_m2}/*train*m2'))\n",
    "    \n",
    "    # wi dev.\n",
    "    wi_dev_ori_path = fp.WI_DEV_ORI\n",
    "    wi_dev_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/*.dev.json\"))\n",
    "    wi_dev_m2_paths = sorted(glob(f'{fp.wi_m2}/ABCN.dev.gold.bea19.m2'))\n",
    "    \n",
    "    # conll2013.\n",
    "    conll2013_ori_path = fp.CONLL2013_ORI\n",
    "    conll2013_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2013/release2.3.1/revised/data/official.sgml\"))\n",
    "    conll2013_m2_paths = sorted(glob(f'{fp.conll2013_m2}/official-preprocessed.m2'))\n",
    "    \n",
    "    # conll2014.\n",
    "    conll2014_ori_path = fp.CONLL2014_ORI\n",
    "    conll2014_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2014/conll14st-test-data/noalt/official-2014.0.sgml\"))\n",
    "    conll2014_m2_paths = sorted(glob(f'{fp.conll2014_m2}/official-2014.combined.m2'))\n",
    "    \n",
    "    # nucle.\n",
    "    nucle_ori_path = fp.NUCLE_ORI\n",
    "    nucle_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/nucle3.3/data/nucle3.2.sgml\"))\n",
    "    nucle_m2_paths = sorted(glob(f'{fp.nucle_m2}/*m2'))\n",
    "    \n",
    "    # lang8.\n",
    "    lang8_ori_path = fp.LANG8_ORI\n",
    "    # TODO: entries.train is in lang8_en, and it should be included in the datasets.\n",
    "    lang8_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/lang8.bea19/entries.train\"))\n",
    "    lang8_m2_paths = None\n",
    "    \n",
    "    # ------\n",
    "    \n",
    "    verbose = False\n",
    "    save_documents = True\n",
    "    \n",
    "    \n",
    "    # Single dataset.\n",
    "#     ori_path = lang8_ori_path\n",
    "#     doc_paths = lang8_doc_paths\n",
    "#     m2_paths = lang8_m2_paths\n",
    "    \n",
    "    n_prev = 3\n",
    "    n_fol = 3\n",
    "    \n",
    "#     make_context(ori_path, doc_paths, m2_paths, n_prev, n_fol)\n",
    "    \n",
    "    # Batch testing.\n",
    "    import os\n",
    "    correct_ver = {\n",
    "        \"fce\": 2, \n",
    "        \"wi.train\": 3,\n",
    "        \"wi.dev\": 3, \n",
    "        \"conll2013\": 2, \n",
    "        \"conll2014\": 2,\n",
    "        \"nucle\": 1,\n",
    "        \"lang8\": 1\n",
    "    }\n",
    "    for dataset in [\"fce\", \"wi_train\", \"wi_dev\", \n",
    "                   \"conll2013\", \"conll2014\", \"nucle\",\n",
    "                    \"lang8\"\n",
    "                   ]:\n",
    "        make_context(eval(f\"{dataset}_ori_path\"), eval(f\"{dataset}_doc_paths\"), eval(f\"{dataset}_m2_paths\"), \n",
    "                    n_prev, n_fol)\n",
    "        \n",
    "        dataset = \".\".join(dataset.split(\"_\"))\n",
    "        with open(f\"/home/neko/GEC/helo_word-master_restricted/data/parallel/raw/{dataset}.ctx\") as f_ctx, \\\n",
    "            open(f'/home/neko/GEC/helo_word-master_restricted/data/parallel/raw/{dataset}.ctx_v{correct_ver[dataset]}_correct') as f_ctx_correct:\n",
    "            print(f\"{dataset}: {f_ctx.read() == f_ctx_correct.read()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
