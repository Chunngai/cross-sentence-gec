{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text: str) -> str:\n",
    "    return \"\".join(text.split())\n",
    "\n",
    "def sentence_in_raw_document(sentence: str, raw_document: str) -> bool:\n",
    "    return remove_spaces(sentence) in remove_spaces(raw_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "from glob import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "basename = os.path.basename\n",
    "splitext = os.path.splitext\n",
    "\n",
    "\n",
    "def get_tokenized_sentences_from_m2_files(m2_file_paths: List[str]) -> List[str]:\n",
    "    sentence_start_index = 2\n",
    "    tokenized_sentences: List[str] = []\n",
    "    for m2_file_path in m2_file_paths:\n",
    "        with open(m2_file_path) as m2_file:\n",
    "            for line in m2_file.readlines():\n",
    "                if line.startswith(\"S \"):\n",
    "                    tokenized_sentence = line[sentence_start_index:].strip()\n",
    "                    tokenized_sentences.append(tokenized_sentence)\n",
    "    \n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_documents_from_json(document_file_paths: List[str], m2_file_paths: List[str]) -> List[List[str]]:\n",
    "    def _normalize(text) -> str:\n",
    "        # Replaces characters.\n",
    "        norm_dict = {\"’\": \"'\",\n",
    "                     \"´\": \"'\",\n",
    "                     \"‘\": \"'\",\n",
    "                     \"′\": \"'\",\n",
    "                     \"`\": \"'\",\n",
    "                     '“': '\"',\n",
    "                     '”': '\"',\n",
    "                     '˝': '\"',\n",
    "                     '¨': '\"',\n",
    "                     '„': '\"',\n",
    "                     '『': '\"',\n",
    "                     '』': '\"',\n",
    "                     '–': '-',\n",
    "                     '—': '-',\n",
    "                     '―': '-',\n",
    "                     '¬': '-',\n",
    "                     '、': ',',\n",
    "                     '，': ',',\n",
    "                     '：': ':',\n",
    "                     '；': ';',\n",
    "                     '？': '?',\n",
    "                     '！': '!',\n",
    "                     'ِ': ' ',\n",
    "                     '\\u200b': ' '}\n",
    "        norm_dict = {ord(k): v for k, v in norm_dict.items()}\n",
    "        return text.translate(norm_dict)\n",
    "    \n",
    "    # Extracts tokenized sentences.\n",
    "    tokenized_sentences = get_tokenized_sentences_from_m2_files(m2_file_paths=m2_file_paths)\n",
    "    \n",
    "    # Extracts documents.\n",
    "    documents: List[List[str]] = []\n",
    "    current_tokenized_sentence_index = 0\n",
    "    current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            # Each line in `f` is a json object.\n",
    "            raw_json_objects: List[str] = document_file.readlines()\n",
    "            \n",
    "            for raw_json_object in raw_json_objects:\n",
    "                json_object = json.loads(raw_json_object)\n",
    "                \n",
    "                # Gets the raw document (simple text) and nomalizes it.\n",
    "                raw_document: str = json_object[\"text\"]\n",
    "                normalized_raw_document: str = _normalize(raw_document)\n",
    "                \n",
    "                document: List[str] = []\n",
    "                partial_normalized_raw_document = remove_spaces(normalized_raw_document)\n",
    "                while sentence_in_raw_document(sentence=current_tokenized_sentence, raw_document=partial_normalized_raw_document):\n",
    "                    document.append(current_tokenized_sentence)\n",
    "                    \n",
    "                    # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "                    # E.g. a sentence exists in 2 consecutive documents.\n",
    "                    partial_normalized_raw_document = partial_normalized_raw_document.replace(remove_spaces(current_tokenized_sentence), \"\", 1)\n",
    "                    \n",
    "                    # Evaluates the next sentence which is extracted in a .m2 file.\n",
    "                    current_tokenized_sentence_index += 1\n",
    "                    try:\n",
    "                        current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "                    except IndexError:  # When the error occurs, the last sentence has been evaluated.\n",
    "                        break\n",
    "                                    \n",
    "                documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents_from_sgml(document_file_paths: List[str], m2_file_paths: List[str]) -> List[str]:\n",
    "    # Extracts tokenized sentences.\n",
    "    tokenized_sentences = get_tokenized_sentences_from_m2_files(m2_file_paths=m2_file_paths)\n",
    "    \n",
    "    # Extracts documents.\n",
    "    documents: List[List[str]] = []\n",
    "    current_tokenized_sentence_index = 0\n",
    "    current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            raw_sgml: str = document_file.read()\n",
    "            \n",
    "#             sgml_object = BeautifulSoup(raw_sgml)\n",
    "#             \n",
    "#             for doc in sgml_object.find_all(\"doc\"):\n",
    "#                 partial_normalized_raw_document: str = \"\"\n",
    "#                 try:  # Most documents in conll2013 have no titles, while all in conll2014 do.\n",
    "#                     title = doc.find(\"title\").text.strip()\n",
    "#                     partial_normalized_raw_document += title\n",
    "#                 except AttributeError:\n",
    "#                     pass\n",
    "#                 partial_normalized_raw_document += \" \".join([p.text.strip() for p in doc.find_all(\"p\")])\n",
    "#                 partial_normalized_raw_document = remove_spaces(partial_normalized_raw_document)\n",
    "\n",
    "#                 # Fix sgml parsing errors.\n",
    "#                 if current_tokenized_sentence == r\"Revival of nuclear reactor , molten salt reactor\":\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(\"713826ae4a9b〈=en\", \"713826ae4a9b&lang=en\")\n",
    "#                 if current_tokenized_sentence == r\"Generation IV nuclear reactor Very high Temperature Reactor versus Sodium Cooled Fast Reactor\":\n",
    "#                     partial_normalized_raw_document += \"<nuclearstreet.com/filesfolders/1654/download.aspx>\"\n",
    "#                 if current_tokenized_sentence == r\"The new millennium brought about promising medical advancements that have positive impacts on human 's lives .\":\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(\"=gene∂=fap\", \"=gene&part=fap\")\n",
    "#                 if current_tokenized_sentence == r\"Old age is a reality that we all have to face one day .\":\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(\"1993){M\", \"1993)<{http://www.ipu.org/english/home.htm}[HYPERLINK:http://www.ipu.org/english/home.htm]>{M\")\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(\"for.html]E\", \"for.html]<{http://aroundsingapore2008.blogspot.com/2009/04/medical-help-subsidy-subsidies-for.html}[HYPERLINK:http://aroundsingapore2008.blogspot.com/2009/04/medical-help-subsidy-subsidies-for.html]>E\")\n",
    "    \n",
    "# #                 if int(doc[\"nid\"]) > 1065:\n",
    "# #                     print(\"current tok sent:\\t\", remove_spaces(current_tokenized_sentence))\n",
    "# #                     print(\"------ ------\")\n",
    "# #                     print(\"partial norm raw doc\\t\", partial_normalized_raw_document)\n",
    "# #                     print(\"------ ------\")\n",
    "# #                     input()\n",
    "    \n",
    "#                 document: List[str] = []\n",
    "#                 while sentence_in_raw_document(sentence=current_tokenized_sentence, raw_document=partial_normalized_raw_document):\n",
    "#                     document.append(current_tokenized_sentence)\n",
    "                    \n",
    "#                     # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "#                     # E.g. a sentence exists in 2 consecutive documents.\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(remove_spaces(current_tokenized_sentence), \"\", 1)\n",
    "                    \n",
    "#                     # Evaluates the next sentence which is extracted in a .m2 file.\n",
    "#                     current_tokenized_sentence_index += 1\n",
    "#                     try:\n",
    "#                         current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "#                     except IndexError:  # When the error occurs, the last sentence has been evaluated.\n",
    "#                         break\n",
    "                \n",
    "# #                 if int(doc[\"nid\"]) > 2270:\n",
    "# #                     print(\"document:\\t\", document)\n",
    "# #                     print(\"------ ------\")\n",
    "# #                     input()\n",
    "                \n",
    "#                 documents.append(document)\n",
    "    \n",
    "            doc_pattern = re.compile(r\"<DOC.+?\\/DOC>\", flags=re.DOTALL)\n",
    "            docs = doc_pattern.findall(raw_sgml)\n",
    "            for doc in docs:\n",
    "                partial_normalized_raw_document: str = \"\"\n",
    "                    \n",
    "                title_pattern = re.compile(r\"<TITLE>(.+?)</TITLE>\")\n",
    "                try:\n",
    "                    title = title_pattern.search(doc).group(1)\n",
    "                    print(title)\n",
    "                    input()\n",
    "                    partial_normalized_raw_document += title\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                \n",
    "#                 try:  # Most documents in conll2013 have no titles, while all in conll2014 do.\n",
    "#                     title = doc.find(\"title\").text.strip()\n",
    "#                     partial_normalized_raw_document += title\n",
    "#                 except AttributeError:\n",
    "#                     pass\n",
    "\n",
    "#                 partial_normalized_raw_document += \" \".join([p.text.strip() for p in doc.find_all(\"p\")])\n",
    "#                 partial_normalized_raw_document = remove_spaces(partial_normalized_raw_document)\n",
    "    \n",
    "#                 document: List[str] = []\n",
    "#                 while sentence_in_raw_document(sentence=current_tokenized_sentence, raw_document=partial_normalized_raw_document):\n",
    "#                     document.append(current_tokenized_sentence)\n",
    "                    \n",
    "#                     # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "#                     # E.g. a sentence exists in 2 consecutive documents.\n",
    "#                     partial_normalized_raw_document = partial_normalized_raw_document.replace(remove_spaces(current_tokenized_sentence), \"\", 1)\n",
    "                    \n",
    "#                     # Evaluates the next sentence which is extracted in a .m2 file.\n",
    "#                     current_tokenized_sentence_index += 1\n",
    "#                     try:\n",
    "#                         current_tokenized_sentence = tokenized_sentences[current_tokenized_sentence_index]\n",
    "#                     except IndexError:  # When the error occurs, the last sentence has been evaluated.\n",
    "#                         break\n",
    "                \n",
    "#                 documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents_from_lang8_entries_train(document_file_paths: List[str]) -> List[str]:\n",
    "    # Extracts documents.\n",
    "    documents = []\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            raw_documents = document_file.readlines()\n",
    "            \n",
    "            document = \"\"\n",
    "            for raw_document in raw_documents:\n",
    "                try:\n",
    "                    sentence = raw_document.split(\"\\t\")[4].strip()\n",
    "                    document += sentence\n",
    "                except IndexError:  # An '\\n' between 2 documents.\n",
    "                    documents.append(document)\n",
    "                    document = \"\"\n",
    "    \n",
    "    return documents\n",
    "    \n",
    "\n",
    "def get_documents(document_file_paths: List[str], m2_file_paths: List[str], source_file_path: str) -> List[str]:\n",
    "    # Gets the name of the dataset.\n",
    "    dataset_name = splitext(basename(source_file_path))[0]\n",
    "    \n",
    "    if dataset_name in [\"fce\", \"wi.train\", \"wi.dev\"]:  # json.\n",
    "        return get_documents_from_json(document_file_paths, m2_file_paths)\n",
    "    elif dataset_name in [\"conll2013\", \"conll2014\", \"nucle\"]:  # sgml.\n",
    "        return get_documents_from_sgml(document_file_paths, m2_file_paths)\n",
    "    else:  # lang8.\n",
    "        return get_documents_from_lang8_entries_train(document_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(document_sentence_index: int, document: List[str], \n",
    "               previous_sentences_number: int, following_sentences_number: int) -> str:\n",
    "\n",
    "    def _is_valid_document_sentence_index(document_sentence_index: int, document: List[str]) -> bool:\n",
    "        # Negative indices, which may be generated when making previous context, \n",
    "        # are not allowed, as corresponding sentences are following context \n",
    "        # for the sentence being evaluated.\n",
    "        return 0 <= document_sentence_index < len(document)\n",
    "    \n",
    "    # Gets previous context.\n",
    "    previous_context_sentences = \"\"\n",
    "    for previous_context_sentence_index in range(document_sentence_index - previous_sentences_number, document_sentence_index):\n",
    "        if _is_valid_document_sentence_index(document_sentence_index=previous_context_sentence_index, document=document):\n",
    "            previous_context_sentence = document[previous_context_sentence_index]\n",
    "            previous_context_sentences = previous_context_sentences + \"<prev>\" + previous_context_sentence\n",
    "            \n",
    "    # Gets following context.\n",
    "    following_context_sentences = \"\"\n",
    "    for following_context_sentence_index in range(document_sentence_index + 1, document_sentence_index + following_sentences_number + 1):\n",
    "        if _is_valid_document_sentence_index(document_sentence_index=following_context_sentence_index, document=document):\n",
    "            following_context_sentence = document[following_context_sentence_index]\n",
    "            following_context_sentences = following_context_sentences + \"<fol>\" + following_context_sentence\n",
    "            \n",
    "    context = previous_context_sentences + following_context_sentences\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context(source_file_path: str, document_file_paths: List[str], m2_file_paths: List[str], previous_sentences_number: int, following_sentences_number: int):\n",
    "    # Gets all source sentences from `ori_path`.\n",
    "    with open(source_file_path) as source_file:\n",
    "        source_sentences = source_file.readlines()\n",
    "    \n",
    "    # Gets all documents.\n",
    "    documents: List[List[str]] = get_documents(document_file_paths=document_file_paths, m2_file_paths=m2_file_paths, source_file_path=source_file_path)\n",
    "    \n",
    "    # Writes documents.\n",
    "    if save_documents:\n",
    "        document_path = f\"{splitext(source_file_path)[0]}.documents\"\n",
    "        with open(f\"{document_path}\", 'w') as f:\n",
    "            for i in range(len(documents)):\n",
    "                f.write(f\"{i}\\t\")\n",
    "                f.write(str(documents[i]))\n",
    "                f.write(\"\\n\\n\")\n",
    "    \n",
    "    # Gets the path of the context file.\n",
    "    context_file_path = f\"{splitext(source_file_path)[0]}.ctx\"\n",
    "\n",
    "    if verbose:\n",
    "        count = 0\n",
    "\n",
    "    current_document_index = 0\n",
    "    current_document: List[str] = documents[current_document_index]\n",
    "    current_document_masked_spaces_removed: List[str] = [remove_spaces(document_sentence)\n",
    "                                                         for document_sentence in current_document]\n",
    "    with open(context_file_path, \"w\") as context_file:\n",
    "        for source_sentence in source_sentences:   \n",
    "            if remove_spaces(source_sentence) not in current_document_masked_spaces_removed:\n",
    "                current_document_index += 1\n",
    "                current_document = documents[current_document_index]\n",
    "                current_document_masked_spaces_removed: List[str] = [remove_spaces(document_sentence)\n",
    "                                                         for document_sentence in current_document]\n",
    "            \n",
    "            # Gets the document-level index of the source sentence.\n",
    "            try:\n",
    "                document_sentence_index = current_document_masked_spaces_removed.index(remove_spaces(source_sentence))\n",
    "            except:\n",
    "                print(source_sentence)\n",
    "                print(current_document_masked_spaces_removed)\n",
    "                continue\n",
    "            \n",
    "            if verbose:\n",
    "                count += 1\n",
    "                print(f\"ln: {count}\")\n",
    "                print(f\"document-level index: {document_sentence_index}\")\n",
    "                print(source_sentence)\n",
    "                print(current_document_masked_removed)\n",
    "                print(\"---\")\n",
    "                input()\n",
    "            \n",
    "            # Prevents sentences in the next document from being recognized as sentences in the current document.\n",
    "            # E.g. a sentence exists in 2 consecutive documents.\n",
    "            current_document_masked_spaces_removed[document_sentence_index] = \"\"\n",
    "            \n",
    "            context = get_context(document_sentence_index=document_sentence_index, document=current_document, \n",
    "                                 previous_sentences_number=previous_sentences_number, following_sentences_number=following_sentences_number)\n",
    "            \n",
    "            # Writes the context to file.\n",
    "            context_file.write(context)\n",
    "            context_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_in_raw_document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a48a995d72fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mn_fol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mmake_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mori_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_fol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Batch testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f17f23249d79>\u001b[0m in \u001b[0;36mmake_context\u001b[0;34m(source_file_path, document_file_paths, m2_file_paths, previous_sentences_number, following_sentences_number)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Gets all documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_file_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument_file_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2_file_paths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm2_file_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Writes documents.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b5e9ac0c7f0e>\u001b[0m in \u001b[0;36mget_documents\u001b[0;34m(document_file_paths, m2_file_paths, source_file_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_documents_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_file_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2_file_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"conll2013\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"conll2014\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nucle\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# sgml.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_documents_from_sgml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_file_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm2_file_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# lang8.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_documents_from_lang8_entries_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_file_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b5e9ac0c7f0e>\u001b[0m in \u001b[0;36mget_documents_from_sgml\u001b[0;34m(document_file_paths, m2_file_paths)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0msentence_in_raw_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_tokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_document\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial_normalized_raw_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m                     \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_tokenized_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_in_raw_document' is not defined"
     ]
    }
   ],
   "source": [
    "import filepath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    fp = filepath.FilePath()\n",
    "    \n",
    "    # fce.\n",
    "    fce_ori_path = fp.FCE_ORI\n",
    "    fce_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/fce/json/fce.*.json\"))\n",
    "    fce_m2_paths = sorted(glob(f'{fp.fce_m2}/*m2'))\n",
    "\n",
    "    # wi train.\n",
    "    wi_train_ori_path = fp.WI_TRAIN_ORI\n",
    "    wi_train_doc_paths = [\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "    ]\n",
    "    wi_train_m2_paths = sorted(glob(f'{fp.wi_m2}/*train*m2'))\n",
    "    \n",
    "    # wi dev.\n",
    "    wi_dev_ori_path = fp.WI_DEV_ORI\n",
    "    wi_dev_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/*.dev.json\"))\n",
    "    wi_dev_m2_paths = sorted(glob(f'{fp.wi_m2}/ABCN.dev.gold.bea19.m2'))\n",
    "    \n",
    "    # conll2013.\n",
    "    conll2013_ori_path = fp.CONLL2013_ORI\n",
    "    conll2013_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2013/release2.3.1/revised/data/official.sgml\"))\n",
    "    conll2013_m2_paths = sorted(glob(f'{fp.conll2013_m2}/official-preprocessed.m2'))\n",
    "    \n",
    "    # conll2014.\n",
    "    conll2014_ori_path = fp.CONLL2014_ORI\n",
    "    conll2014_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2014/conll14st-test-data/noalt/official-2014.0.sgml\"))\n",
    "    conll2014_m2_paths = sorted(glob(f'{fp.conll2014_m2}/official-2014.combined.m2'))\n",
    "    \n",
    "    # nucle.\n",
    "    nucle_ori_path = fp.NUCLE_ORI\n",
    "    nucle_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/nucle3.3/data/nucle3.2.sgml\"))\n",
    "    nucle_m2_paths = sorted(glob(f'{fp.nucle_m2}/*m2'))\n",
    "    \n",
    "    # lang8.\n",
    "    lang8_ori_path = fp.LANG8_ORI\n",
    "    # TODO: entries.train is in lang8_en, and it should be included in the datasets.\n",
    "    lang8_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/lang8.bea19/entries.train\"))\n",
    "    \n",
    "    # ------\n",
    "    \n",
    "    verbose = False\n",
    "    save_documents = True\n",
    "    \n",
    "    \n",
    "    # Single dataset.\n",
    "    ori_path = nucle_ori_path\n",
    "    doc_paths = nucle_doc_paths\n",
    "    m2_paths = nucle_m2_paths\n",
    "    \n",
    "    n_prev = 3\n",
    "    n_fol = 3\n",
    "    \n",
    "    make_context(ori_path, doc_paths, m2_paths, n_prev, n_fol)\n",
    "    \n",
    "    # Batch testing.\n",
    "#     import os\n",
    "#     for dataset in [\"fce\", \"wi_train\", \"wi_dev\", \n",
    "#                    \"conll2013\", \"conll2014\", \n",
    "# #                     \"nucle\"\n",
    "#                    ]:\n",
    "#         make_context(eval(f\"{dataset}_ori_path\"), eval(f\"{dataset}_doc_paths\"), eval(f\"{dataset}_m2_paths\"), \n",
    "#                     previous_sentences_number=n_prev, following_sentences_number=n_fol)\n",
    "        \n",
    "#         dataset = \".\".join(dataset.split(\"_\"))\n",
    "#         with open(f\"/home/neko/GEC/helo_word-master_restricted/data/parallel/raw/{dataset}.ctx\") as f_ctx, \\\n",
    "#             open(f\"/home/neko/GEC/helo_word-master_restricted/data/parallel/raw/{dataset}.ctx_v2_correct\") as f_ctx_correct:\n",
    "#             print(f_ctx.read() == f_ctx_correct.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
