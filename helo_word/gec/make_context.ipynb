{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "basename = os.path.basename\n",
    "splitext = os.path.splitext\n",
    "\n",
    "\n",
    "def get_documents_from_json(document_file_paths: List[str]) -> List[str]:\n",
    "    # Extracts documents.\n",
    "    documents = []\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            # Each line in `f` is a json object.\n",
    "            raw_json_objects = document_file.readlines()\n",
    "            \n",
    "            # Extracts the documents.\n",
    "            for raw_json_object in raw_json_objects:\n",
    "                json_object = json.loads(raw_json_object)\n",
    "\n",
    "                document = json_object[\"text\"]\n",
    "                documents.append(document)\n",
    "    \n",
    "    # Replaces characters.\n",
    "    norm_dict = {\"’\": \"'\",\n",
    "                 \"´\": \"'\",\n",
    "                 \"‘\": \"'\",\n",
    "                 \"′\": \"'\",\n",
    "                 \"`\": \"'\",\n",
    "                 '“': '\"',\n",
    "                 '”': '\"',\n",
    "                 '˝': '\"',\n",
    "                 '¨': '\"',\n",
    "                 '„': '\"',\n",
    "                 '『': '\"',\n",
    "                 '』': '\"',\n",
    "                 '–': '-',\n",
    "                 '—': '-',\n",
    "                 '―': '-',\n",
    "                 '¬': '-',\n",
    "                 '、': ',',\n",
    "                 '，': ',',\n",
    "                 '：': ':',\n",
    "                 '；': ';',\n",
    "                 '？': '?',\n",
    "                 '！': '!',\n",
    "                 'ِ': ' ',\n",
    "                 '\\u200b': ' '}\n",
    "    norm_dict = {ord(k): v for k, v in norm_dict.items()}\n",
    "    documents = [document.translate(norm_dict) for document in documents]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents_from_sgml(document_file_paths: List[str]) -> List[str]:\n",
    "    # Extracts documents.\n",
    "    documents = []\n",
    "    for document_file_path in document_file_paths:\n",
    "        with open(document_file_path) as document_file:\n",
    "            raw_sgml = document_file.read()\n",
    "            sgml_object = BeautifulSoup(raw_sgml)\n",
    "            \n",
    "            for doc in sgml_object.find_all(\"doc\"):\n",
    "                document = \"\"\n",
    "                \n",
    "                try:  # Most documents in conll2013 have no titles, while all in conll2014 do.\n",
    "                    document += doc.find(\"title\").text.strip()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                \n",
    "                for p in doc.find_all(\"p\"):\n",
    "                    document += p.text.strip()\n",
    "                documents.append(document)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_documents(document_file_paths: List[str], source_file_path: str) -> List[str]:\n",
    "    # Gets the name of the dataset from `ori_path`.\n",
    "    dataset_name = splitext(basename(source_file_path))[0]\n",
    "    \n",
    "    if dataset_name in [\"fce\", \"wi.train\", \"wi.dev\"]:  # json.\n",
    "        documents = get_documents_from_json(document_file_paths)\n",
    "        return documents\n",
    "    elif dataset_name in [\"conll2013\", \"conll2014\", \"nucle\"]:  # sgml.\n",
    "        documents = get_documents_from_sgml(document_file_paths)\n",
    "        return documents\n",
    "    else:  # lang8, xml.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(text: str) -> str:\n",
    "    return \"\".join(text.split())\n",
    "\n",
    "# def get_document_indices(query: str, documents: List[str]) -> List[int]:\n",
    "#     # Removes all spaces in every document.\n",
    "#     documents_without_spaces = [remove_spaces(document)\n",
    "#                                 for document in documents]\n",
    "    \n",
    "#     # Finds documents consisting the query.\n",
    "#     doc_indices = []\n",
    "#     for i in range(len(documents_without_spaces)):\n",
    "#         doc = documents_without_spaces[i]\n",
    "        \n",
    "#         # If the query is in the document, \n",
    "#         # records the index of the document.\n",
    "#         if query in doc:\n",
    "#             doc_indices.append(i)\n",
    "    \n",
    "#     return doc_indices\n",
    "    \n",
    "\n",
    "# def get_document(i: int, oris: List[str], documents: List[str]) -> str:\n",
    "#     def _get_query(oris: List[str], start: int, end: int) -> str:\n",
    "#         # Sentences for locating the document.\n",
    "#         ori_sentences = oris[start:end]\n",
    "#         # Removes all spaces.\n",
    "#         ori_sentences_without_spaces = [remove_spaces(ori_sentence) \n",
    "#                                         for ori_sentence in ori_sentences]\n",
    "#         # Concatenation.\n",
    "#         query = \"\".join(ori_sentences_without_spaces)\n",
    "        \n",
    "#         return query\n",
    "    \n",
    "    \n",
    "#     for j in range(1, 10):\n",
    "#         # Gets the query for locating the document.\n",
    "#         query = _get_query(oris, i, i+j)\n",
    "#         # Gets all documents consisting the query.\n",
    "#         doc_indices = get_document_indices(query, documents)\n",
    "#         # If only one document is found, that is the needed document.\n",
    "#         if len(doc_indices) == 1:\n",
    "#             doc_index = doc_indices[0]\n",
    "#             return documents[doc_index]\n",
    "        \n",
    "#         # Gets the query for locating the document.\n",
    "#         query = _get_query(oris, i-j+1, i+1)\n",
    "#         # Gets all documents consisting the query.\n",
    "#         doc_indices = get_document_indices(query, documents)\n",
    "#         # If only one document is found, that is the needed document.\n",
    "#         if len(doc_indices) == 1:\n",
    "#             doc_index = doc_indices[0]\n",
    "#             return documents[doc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_in_document(sentence: str, document: str) -> bool:\n",
    "    return remove_spaces(sentence) in remove_spaces(document)\n",
    "\n",
    "\n",
    "def get_context(source_sentence_index: int, \n",
    "                previous_sentences_number: int, following_sentences_number: int, \n",
    "                source_sentences: List[str], document: str) -> str:\n",
    "    \n",
    "    def _remove_line_feed(text):\n",
    "        return text.split(\"\\n\")[0]\n",
    "        \n",
    "    # Gets previous context.\n",
    "    previous_context_sentences = \"\"\n",
    "    for previous_context_sentence_index in range(source_sentence_index - previous_sentences_number, source_sentence_index):\n",
    "        try:\n",
    "            previous_context_sentence = source_sentences[previous_context_sentence_index]\n",
    "        except IndexError:\n",
    "            continue\n",
    "            \n",
    "        if sentence_in_document(sentence=previous_context_sentence, document=document):\n",
    "            previous_context_sentences = previous_context_sentences + \"<prev>\" + _remove_line_feed(previous_context_sentence)\n",
    "    \n",
    "    # Gets following context.\n",
    "    following_context_sentences = \"\"\n",
    "    for following_context_sentence_index in range(source_sentence_index + 1, source_sentence_index + following_sentences_number + 1):\n",
    "        try:\n",
    "            following_context_sentence = source_sentences[following_context_sentence_index]\n",
    "        except IndexError:\n",
    "            continue\n",
    "        \n",
    "        if sentence_in_document(sentence=following_context_sentence, document=document):\n",
    "            following_context_sentences = following_context_sentences + \"<fol>\" + _remove_line_feed(following_context_sentence)\n",
    "    \n",
    "    context = previous_context_sentences + following_context_sentences\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "\n",
    "def make_context(source_file_path: str, document_file_paths: List[str], previous_sentences_number: int, following_sentences_number: int):\n",
    "    # Gets all source sentences from `ori_path`.\n",
    "    with open(source_file_path) as source_file:\n",
    "        source_sentences = source_file.readlines()\n",
    "    \n",
    "    # Gets all documents.\n",
    "    documents = get_documents(document_file_paths=document_file_paths, source_file_path=source_file_path)\n",
    "    \n",
    "    # Gets the path of the context file.\n",
    "    context_file_path = f\"{splitext(source_file_path)[0]}.ctx\"\n",
    "    \n",
    "#     with open(f_context, \"w\") as f:\n",
    "#         for i in range(len(oris)):        \n",
    "#             # Gets the document consisting of `ori_sentence`.\n",
    "#             doc = get_document(i, oris, documents)\n",
    "\n",
    "#             # Gets the context.\n",
    "#             context = get_context(i, n_prev, n_fol, oris, doc)\n",
    "\n",
    "#             # Writes the context to file.\n",
    "#             f.write(context)\n",
    "#             f.write(\"\\n\")\n",
    "\n",
    "    current_document_index = 0\n",
    "    current_document = documents[current_document_index]\n",
    "    current_document_spaces_removed = remove_spaces(current_document)\n",
    "    with open(context_file_path, \"w\") as context_file:\n",
    "        for source_sentence_index in range(len(source_sentences)):        \n",
    "#             document = get_document(source_sentence_index, source_sentences, documents)\n",
    "#             context = get_context(source_sentence_index=source_sentence_index, \n",
    "#                                   previous_sentences_number=previous_sentences_number, following_sentences_number=following_sentences_number, \n",
    "#                                   source_sentences=source_sentences, document=document)\n",
    "\n",
    "            source_sentence = source_sentences[source_sentence_index]\n",
    "\n",
    "            # Gets the context.\n",
    "            source_sentence_spaces_removed = remove_spaces(source_sentence)\n",
    "            if not sentence_in_document(sentence=source_sentence, document=current_document_spaces_removed):\n",
    "                current_document_index += 1\n",
    "                current_document = documents[current_document_index]\n",
    "                current_document_spaces_removed = remove_spaces(current_document)\n",
    "            \n",
    "            if verbose:\n",
    "                print(source_sentence_spaces_removed)\n",
    "                print(current_document_spaces_removed)\n",
    "            current_document_spaces_removed = current_document_spaces_removed.replace(source_sentence_spaces_removed, \"\", 1)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"----- -----\")\n",
    "                print(f\"ln: {source_sentence_index + 1}\", current_document_spaces_removed)\n",
    "                print()\n",
    "                input()\n",
    "        \n",
    "            context = get_context(source_sentence_index=source_sentence_index, \n",
    "                                  previous_sentences_number=previous_sentences_number, following_sentences_number=following_sentences_number, \n",
    "                                  source_sentences=source_sentences, document=current_document)\n",
    "\n",
    "            # Writes the context to file.\n",
    "            context_file.write(context)\n",
    "            context_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import filepath\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    fp = filepath.FilePath()\n",
    "    \n",
    "    # fce.\n",
    "    fce_ori_path = fp.FCE_ORI\n",
    "    fce_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/fce/json/fce.*.json\"))\n",
    "\n",
    "    # wi.\n",
    "    wi_train_ori_path = fp.WI_TRAIN_ORI\n",
    "    wi_train_doc_paths = [\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/A.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/B.train.json\",\n",
    "        \"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/C.train.json\",\n",
    "    ]\n",
    "    \n",
    "    wi_dev_ori_path = fp.WI_DEV_ORI\n",
    "    wi_dev_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/wi+locness/json/*.dev.json\"))\n",
    "    \n",
    "    # conll2013.\n",
    "    conll2013_ori_path = fp.CONLL2013_ORI\n",
    "    conll2013_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2013/release2.3.1/revised/data/official.sgml\"))\n",
    "    \n",
    "    # conll2014.\n",
    "    conll2014_ori_path = fp.CONLL2014_ORI\n",
    "    conll2014_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/conll2014/conll14st-test-data/noalt/official-2014.0.sgml\"))\n",
    "    \n",
    "    # nucle.\n",
    "    nucle_ori_path = fp.NUCLE_ORI\n",
    "    nucle_doc_paths = sorted(glob(\"/home/neko/GEC/helo_word-master_restricted/data/bea19/nucle3.3/data/nucle3.2.sgml\"))\n",
    "    \n",
    "    # ------\n",
    "    \n",
    "    verbose = False\n",
    "    \n",
    "    ori_path = nucle_ori_path\n",
    "    doc_paths = nucle_doc_paths\n",
    "    n_prev = 3\n",
    "    n_fol = 3\n",
    "    \n",
    "    make_context(ori_path, doc_paths, n_prev, n_fol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
