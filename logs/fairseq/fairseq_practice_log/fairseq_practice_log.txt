neko@neko:~$ ssh -p 29323 root@hz-t2.matpool.com
root@hz-t2.matpool.com's password: 
Last login: Fri Jul 24 02:48:05 2020 from 10.0.0.19
(myconda) root@576918a23912:~# cd fairseq/
-bash: cd: fairseq/: No such file or directory
(myconda) root@576918a23912:~# cd /mnt/fairseq/
(myconda) root@576918a23912:/mnt/fairseq# ls
fairseq  init.sh  playground
(myconda) root@576918a23912:/mnt/fairseq# cd fairseq/examples/translation
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md           prepare-iwslt17-multilingual.sh  prepare-wmt14en2fr.sh
prepare-iwslt14.sh  prepare-wmt14en2de.sh            subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
Cloning into 'mosesdecoder'...
remote: Enumerating objects: 46, done.
remote: Counting objects: 100% (46/46), done.
remote: Compressing objects: 100% (38/38), done.
^Cceiving objects:  16% (24247/147560), 5.15 MiB | 11.00 KiB/s     
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md     mosesdecoder.zip    prepare-iwslt17-multilingual.sh  prepare-wmt14en2fr.sh
mosesdecoder  prepare-iwslt14.sh  prepare-wmt14en2de.sh            subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
--2020-07-24 07:14:37--  https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
Resolving wit3.fbk.eu (wit3.fbk.eu)... failed: Temporary failure in name resolution.
wget: unable to resolve host address ‘wit3.fbk.eu’
Data not successfully downloaded.
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
--2020-07-24 07:15:07--  https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
Resolving wit3.fbk.eu (wit3.fbk.eu)... 217.77.80.8
Connecting to wit3.fbk.eu (wit3.fbk.eu)|217.77.80.8|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 19982877 (19M) [application/x-gzip]
Saving to: ‘de-en.tgz’

de-en.tgz                  51%[=================>                 ]   9.88M  5.01KB/s    eta 31m 20s^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cp ../../../
fairseq/    init.sh     playground/ 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cp ../../../../
.de-en.tgz          42427.log           crosent/            init.sh             
.ipynb_checkpoints/ 42432.log           de-en.tgz           mosesdecoder.zip    
.mosesdecoder.zip   42436.log           fairseq/            
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cp ../../../../de-en.tgz `pwd` 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
--2020-07-24 07:52:08--  https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
Resolving wit3.fbk.eu (wit3.fbk.eu)... 217.77.80.8
Connecting to wit3.fbk.eu (wit3.fbk.eu)|217.77.80.8|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 19982877 (19M) [application/x-gzip]
Saving to: ‘de-en.tgz.1’

de-en.tgz.1                 0%[                                   ] 104.00K  13.0KB/s    eta 24m 49s^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# vim prepare-iwslt14.sh 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
--2020-07-24 07:53:47--  https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
Resolving wit3.fbk.eu (wit3.fbk.eu)... 217.77.80.8
Connecting to wit3.fbk.eu (wit3.fbk.eu)|217.77.80.8|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 19982877 (19M) [application/x-gzip]
Saving to: ‘de-en.tgz.2’

de-en.tgz.2                 0%[                                   ]  40.00K  18.2KB/s               ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll
total 0
drwxr-xr-x 0 root root 235380244 Jul 24 07:53 ./
drwxr-xr-x 0 root root 215892701 Jul 23 11:23 ../
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
-rw------- 0 root root  19982877 Jul 24 07:51 de-en.tgz
drwxr-xr-x 0 root root         0 Jul 24 07:14 iwslt14.tokenized.de-en/
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root  10485760 Jul 24 07:53 orig/
-rw-r--r-- 0 root root      3007 Jul 24 07:53 prepare-iwslt14.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cd iwslt14.tokenized.de-en/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/iwslt14.tokenized.de-en# cd ..
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# vim prepare-iwslt14.sh 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
Data successfully downloaded.
de-en/
de-en/IWSLT14.TED.dev2010.de-en.de.xml
de-en/IWSLT14.TED.dev2010.de-en.en.xml
de-en/IWSLT14.TED.tst2010.de-en.de.xml
de-en/IWSLT14.TED.tst2010.de-en.en.xml
de-en/IWSLT14.TED.tst2011.de-en.de.xml
de-en/IWSLT14.TED.tst2011.de-en.en.xml
de-en/IWSLT14.TED.tst2012.de-en.de.xml
de-en/IWSLT14.TED.tst2012.de-en.en.xml
de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
de-en/README
de-en/train.en
de-en/train.tags.de-en.de

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now
pre-processing train data...
Tokenizer Version 1.1
Language: de
Number of threads: 8

cat: orig/de-en/train.tags.de-en.en: No such file or directory
Tokenizer Version 1.1
Language: en
Number of threads: 8

clean-corpus.perl: processing iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.de & .en to iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean, cutoff 1-175, ratio 1.5
iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.en is too short! at mosesdecoder/scripts/training/clean-corpus-n.perl line 96.
pre-processing valid/test data...
orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

creating train, valid, test...
learn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...
Traceback (most recent call last):
  File "subword-nmt/subword_nmt/learn_bpe.py", line 361, in <module>
    learn_bpe(args.input, args.output, args.symbols, args.min_frequency, args.verbose, is_dict=args.dict_input, total_symbols=args.total_symbols, num_workers=args.num_workers)
  File "subword-nmt/subword_nmt/learn_bpe.py", line 296, in learn_bpe
    threshold = max(stats.values()) / 10
ValueError: max() arg is an empty sequence
apply_bpe.py to train.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to valid.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to test.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to train.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to valid.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to test.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md  de-en.tgz  iwslt14.tokenized.de-en  mosesdecoder  orig  prepare-iwslt14.sh  prepare-iwslt17-multilingual.sh  prepare-wmt14en2de.sh  prepare-wmt14en2fr.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll -t
total 0
drwxr-xr-x 0 root root  12151608 Jul 24 07:55 iwslt14.tokenized.de-en/
drwxr-xr-x 0 root root  39333215 Jul 24 07:55 orig/
drwxr-xr-x 0 root root 276379303 Jul 24 07:55 ./
-rw-r--r-- 0 root root      3003 Jul 24 07:55 prepare-iwslt14.sh
-rw------- 0 root root  19982877 Jul 24 07:51 de-en.tgz
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
drwxr-xr-x 0 root root 215892701 Jul 23 11:23 ../
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls iwslt14.tokenized.de-en/
code  test.de  test.en  tmp  train.de  train.en  valid.de  valid.en
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
Cloning Moses github repository (for tokenization scripts)...
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
Cloning Subword NMT repository (for BPE pre-processing)...
fatal: destination path 'subword-nmt' already exists and is not an empty directory.
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
Data successfully downloaded.
de-en/
de-en/IWSLT14.TED.dev2010.de-en.de.xml
de-en/IWSLT14.TED.dev2010.de-en.en.xml
de-en/IWSLT14.TED.tst2010.de-en.de.xml
de-en/IWSLT14.TED.tst2010.de-en.en.xml
de-en/IWSLT14.TED.tst2011.de-en.de.xml
de-en/IWSLT14.TED.tst2011.de-en.en.xml
de-en/IWSLT14.TED.tst2012.de-en.de.xml
de-en/IWSLT14.TED.tst2012.de-en.en.xml
de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
de-en/README
de-en/train.en
de-en/train.tags.de-en.de

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now
pre-processing train data...
Tokenizer Version 1.1
Language: de
Number of threads: 8

cat: orig/de-en/train.tags.de-en.en: No such file or directory
Tokenizer Version 1.1
Language: en
Number of threads: 8

clean-corpus.perl: processing iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.de & .en to iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean, cutoff 1-175, ratio 1.5
iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.en is too short! at mosesdecoder/scripts/training/clean-corpus-n.perl line 96.
pre-processing valid/test data...
orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de
Tokenizer Version 1.1
Language: de
Number of threads: 8

orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en
Tokenizer Version 1.1
Language: en
Number of threads: 8

creating train, valid, test...
learn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...
Traceback (most recent call last):
  File "subword-nmt/subword_nmt/learn_bpe.py", line 361, in <module>
    learn_bpe(args.input, args.output, args.symbols, args.min_frequency, args.verbose, is_dict=args.dict_input, total_symbols=args.total_symbols, num_workers=args.num_workers)
  File "subword-nmt/subword_nmt/learn_bpe.py", line 296, in learn_bpe
    threshold = max(stats.values()) / 10
ValueError: max() arg is an empty sequence
apply_bpe.py to train.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to valid.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to test.de...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to train.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to valid.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
apply_bpe.py to test.en...
Error: invalid line 2 in BPE codes file: 
The line should exist of exactly two subword units, separated by whitespace
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md  de-en.tgz  iwslt14.tokenized.de-en  mosesdecoder  orig  prepare-iwslt14.sh  prepare-iwslt17-multilingual.sh  prepare-wmt14en2de.sh  prepare-wmt14en2fr.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# rm -rf iwslt14.tokenized.de-en/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md  de-en.tgz  mosesdecoder  orig  prepare-iwslt14.sh  prepare-iwslt17-multilingual.sh  prepare-wmt14en2de.sh  prepare-wmt14en2fr.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# rm -rf de-en.tgz 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md  mosesdecoder  orig  prepare-iwslt14.sh  prepare-iwslt17-multilingual.sh  prepare-wmt14en2de.sh  prepare-wmt14en2fr.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll
total 0
drwxr-xr-x 0 root root 244244818 Jul 24 07:57 ./
drwxr-xr-x 0 root root 244854869 Jul 23 11:23 ../
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root  39333215 Jul 24 07:55 orig/
-rw-r--r-- 0 root root      3003 Jul 24 07:55 prepare-iwslt14.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# vim prepare-iwslt14.sh 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cp ../../../../de-en.tgz `pwd`
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
+ echo 'Cloning Moses github repository (for tokenization scripts)...'
Cloning Moses github repository (for tokenization scripts)...
+ git clone https://github.com/moses-smt/mosesdecoder.git
fatal: destination path 'mosesdecoder' already exists and is not an empty directory.
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# vim prepare-iwslt14.sh 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
+ '[' '!' -d mosesdecoder ']'
+ '[' '!' -d subword-nmt ']'
+ SCRIPTS=mosesdecoder/scripts
+ TOKENIZER=mosesdecoder/scripts/tokenizer/tokenizer.perl
+ LC=mosesdecoder/scripts/tokenizer/lowercase.perl
+ CLEAN=mosesdecoder/scripts/training/clean-corpus-n.perl
+ BPEROOT=subword-nmt/subword_nmt
+ BPE_TOKENS=10000
+ URL=https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
+ GZ=de-en.tgz
+ '[' '!' -d mosesdecoder/scripts ']'
+ src=de
+ tgt=en
+ lang=de-en
+ prep=iwslt14.tokenized.de-en
+ tmp=iwslt14.tokenized.de-en/tmp
+ orig=orig
+ mkdir -p orig iwslt14.tokenized.de-en/tmp iwslt14.tokenized.de-en
+ echo 'Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...'
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
+ cd orig
+ '[' '!' -f de-en.tgz ']'
+ '[' -f de-en.tgz ']'
+ echo 'Data successfully downloaded.'
Data successfully downloaded.
+ tar zxvf de-en.tgz
de-en/
de-en/IWSLT14.TED.dev2010.de-en.de.xml
de-en/IWSLT14.TED.dev2010.de-en.en.xml
de-en/IWSLT14.TED.tst2010.de-en.de.xml
de-en/IWSLT14.TED.tst2010.de-en.en.xml
de-en/IWSLT14.TED.tst2011.de-en.de.xml
de-en/IWSLT14.TED.tst2011.de-en.en.xml
de-en/IWSLT14.TED.tst2012.de-en.de.xml
de-en/IWSLT14.TED.tst2012.de-en.en.xml
de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
de-en/README
de-en/train.en
de-en/train.tags.de-en.de

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# vim prepare-iwslt14.sh 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
+ '[' '!' -d mosesdecoder ']'
+ '[' '!' -d subword-nmt ']'
+ SCRIPTS=mosesdecoder/scripts
+ TOKENIZER=mosesdecoder/scripts/tokenizer/tokenizer.perl
+ LC=mosesdecoder/scripts/tokenizer/lowercase.perl
+ CLEAN=mosesdecoder/scripts/training/clean-corpus-n.perl
+ BPEROOT=subword-nmt/subword_nmt
+ BPE_TOKENS=10000
+ URL=https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
+ GZ=de-en.tgz
+ '[' '!' -d mosesdecoder/scripts ']'
+ src=de
+ tgt=en
+ lang=de-en
+ prep=iwslt14.tokenized.de-en
+ tmp=iwslt14.tokenized.de-en/tmp
+ orig=orig
+ mkdir -p orig iwslt14.tokenized.de-en/tmp iwslt14.tokenized.de-en
+ echo 'Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...'
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
+ cd orig
+ '[' '!' -f de-en.tgz ']'
+ '[' -f de-en.tgz ']'
+ echo 'Data successfully downloaded.'
Data successfully downloaded.
+ tar zxvf de-en.tgz
de-en/
de-en/IWSLT14.TED.dev2010.de-en.de.xml
de-en/IWSLT14.TED.dev2010.de-en.en.xml
de-en/IWSLT14.TED.tst2010.de-en.de.xml
de-en/IWSLT14.TED.tst2010.de-en.en.xml
de-en/IWSLT14.TED.tst2011.de-en.de.xml
de-en/IWSLT14.TED.tst2011.de-en.en.xml
de-en/IWSLT14.TED.tst2012.de-en.de.xml
de-en/IWSLT14.TED.tst2012.de-en.en.xml
de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
de-en/README
de-en/train.en
de-en/train.tags.de-en.de

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls orig/
de-en  de-en.tgz  de-en.tgz.1  de-en.tgz.2
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# rm de-en.tgz.*
rm: cannot remove 'de-en.tgz.*': No such file or directory
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cd orig/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# ls
de-en  de-en.tgz  de-en.tgz.1  de-en.tgz.2
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# rm -rf de-en*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# ls
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation/orig# cd ..
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
+ '[' '!' -d mosesdecoder ']'
+ '[' '!' -d subword-nmt ']'
+ SCRIPTS=mosesdecoder/scripts
+ TOKENIZER=mosesdecoder/scripts/tokenizer/tokenizer.perl
+ LC=mosesdecoder/scripts/tokenizer/lowercase.perl
+ CLEAN=mosesdecoder/scripts/training/clean-corpus-n.perl
+ BPEROOT=subword-nmt/subword_nmt
+ BPE_TOKENS=10000
+ URL=https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
+ GZ=de-en.tgz
+ '[' '!' -d mosesdecoder/scripts ']'
+ src=de
+ tgt=en
+ lang=de-en
+ prep=iwslt14.tokenized.de-en
+ tmp=iwslt14.tokenized.de-en/tmp
+ orig=orig
+ mkdir -p orig iwslt14.tokenized.de-en/tmp iwslt14.tokenized.de-en
+ echo 'Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...'
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
+ cd orig
+ '[' '!' -f de-en.tgz ']'
+ wget https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
--2020-07-24 08:01:34--  https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
Resolving wit3.fbk.eu (wit3.fbk.eu)... 217.77.80.8
Connecting to wit3.fbk.eu (wit3.fbk.eu)|217.77.80.8|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 19982877 (19M) [application/x-gzip]
Saving to: ‘de-en.tgz’

de-en.tgz                   0%[                                   ]  24.00K  25.4KB/s               ^de-en.tgz                   0%[                                   ]  40.00K  14.1KB/s               ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ^C
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                mosesdecoder        prepare-iwslt17-multilingual.sh  subword-nmt
de-en.tgz                orig                prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh  prepare-wmt14en2fr.sh
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls orig/
de-en.tgz
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# rm orig/de-en.tgz 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# mv de-en.tgz orig/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                orig                             prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh               prepare-wmt14en2fr.sh
mosesdecoder             prepare-iwslt17-multilingual.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll
total 0
drwxr-xr-x 0 root root 204911687 Jul 24 08:02 ./
drwxr-xr-x 0 root root 205521738 Jul 23 11:23 ../
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
drwxr-xr-x 0 root root         0 Jul 24 07:59 iwslt14.tokenized.de-en/
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root  19982877 Jul 24 08:02 orig/
-rw-r--r-- 0 root root      3087 Jul 24 07:59 prepare-iwslt14.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# rm -rf iwslt14.tokenized.de-en/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll
total 0
drwxr-xr-x 0 root root 224894564 Jul 24 08:02 ./
drwxr-xr-x 0 root root 225504615 Jul 23 11:23 ../
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root  19982877 Jul 24 08:02 orig/
-rw-r--r-- 0 root root      3087 Jul 24 07:59 prepare-iwslt14.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# bash prepare-iwslt14.sh 
+ '[' '!' -d mosesdecoder ']'
+ '[' '!' -d subword-nmt ']'
+ SCRIPTS=mosesdecoder/scripts
+ TOKENIZER=mosesdecoder/scripts/tokenizer/tokenizer.perl
+ LC=mosesdecoder/scripts/tokenizer/lowercase.perl
+ CLEAN=mosesdecoder/scripts/training/clean-corpus-n.perl
+ BPEROOT=subword-nmt/subword_nmt
+ BPE_TOKENS=10000
+ URL=https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz
+ GZ=de-en.tgz
+ '[' '!' -d mosesdecoder/scripts ']'
+ src=de
+ tgt=en
+ lang=de-en
+ prep=iwslt14.tokenized.de-en
+ tmp=iwslt14.tokenized.de-en/tmp
+ orig=orig
+ mkdir -p orig iwslt14.tokenized.de-en/tmp iwslt14.tokenized.de-en
+ echo 'Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...'
Downloading data from https://wit3.fbk.eu/archive/2014-01/texts/de/en/de-en.tgz...
+ cd orig
+ '[' '!' -f de-en.tgz ']'
+ '[' -f de-en.tgz ']'
+ echo 'Data successfully downloaded.'
Data successfully downloaded.
+ tar zxvf de-en.tgz
de-en/
de-en/IWSLT14.TED.dev2010.de-en.de.xml
de-en/IWSLT14.TED.dev2010.de-en.en.xml
de-en/IWSLT14.TED.tst2010.de-en.de.xml
de-en/IWSLT14.TED.tst2010.de-en.en.xml
de-en/IWSLT14.TED.tst2011.de-en.de.xml
de-en/IWSLT14.TED.tst2011.de-en.en.xml
de-en/IWSLT14.TED.tst2012.de-en.de.xml
de-en/IWSLT14.TED.tst2012.de-en.en.xml
de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
de-en/README
de-en/train.en
de-en/train.tags.de-en.de
de-en/train.tags.de-en.en
+ cd ..
+ echo 'pre-processing train data...'
pre-processing train data...
+ for l in $src $tgt
+ f=train.tags.de-en.de
+ tok=train.tags.de-en.tok.de
+ cat orig/de-en/train.tags.de-en.de
+ grep -v '<url>'
+ grep -v '<talkid>'
+ sed -e 's/<title>//g'
+ grep -v '<keywords>'
+ sed -e 's/<\/title>//g'
+ sed -e 's/<description>//g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
+ sed -e 's/<\/description>//g'
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for l in $src $tgt
+ f=train.tags.de-en.en
+ tok=train.tags.de-en.tok.en
+ cat orig/de-en/train.tags.de-en.en
+ grep -v '<talkid>'
+ grep -v '<keywords>'
+ sed -e 's/<title>//g'
+ sed -e 's/<\/title>//g'
+ sed -e 's/<description>//g'
+ sed -e 's/<\/description>//g'
+ grep -v '<url>'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ perl mosesdecoder/scripts/training/clean-corpus-n.perl -ratio 1.5 iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok de en iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean 1 175
clean-corpus.perl: processing iwslt14.tokenized.de-en/tmp/train.tags.de-en.tok.de & .en to iwslt14.tokenized.de-en/tmp/train.tags.de-en.clean, cutoff 1-175, ratio 1.5
..........(100000).......
Input sentences: 174443  Output sentences:  167522
+ for l in $src $tgt
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ for l in $src $tgt
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ echo 'pre-processing valid/test data...'
pre-processing valid/test data...
+ for l in $src $tgt
++ ls orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.dev2010.de-en.de.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de
+ echo orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de
orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de
+ grep '<seg id' orig/de-en/IWSLT14.TED.dev2010.de-en.de.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
+ sed -e 's/\s*<\/seg>\s*//g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2010.de-en.de.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de
+ echo orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de
orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2010.de-en.de.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\s*<\/seg>\s*//g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2011.de-en.de.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de
+ echo orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de
orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2011.de-en.de.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2012.de-en.de.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
+ echo orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2012.de-en.de.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TEDX.dev2012.de-en.de.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de
+ echo orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de
orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de
+ grep '<seg id' orig/de-en/IWSLT14.TEDX.dev2012.de-en.de.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l de
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
Tokenizer Version 1.1
Language: de
Number of threads: 8
+ echo ''

+ for l in $src $tgt
++ ls orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.dev2010.de-en.en.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en
+ echo orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en
orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en
+ grep '<seg id' orig/de-en/IWSLT14.TED.dev2010.de-en.en.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2010.de-en.en.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en
+ echo orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en
orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2010.de-en.en.xml
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2011.de-en.en.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en
+ echo orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en
orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2011.de-en.en.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
+ sed -e 's/\s*<\/seg>\s*//g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TED.tst2012.de-en.en.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
+ echo orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
+ grep '<seg id' orig/de-en/IWSLT14.TED.tst2012.de-en.en.xml
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ sed -e 's/\s*<\/seg>\s*//g'
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ for o in `ls $orig/$lang/IWSLT14.TED*.$l.xml`
+ fname=IWSLT14.TEDX.dev2012.de-en.en.xml
+ f=iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en
+ echo orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en
orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en
+ sed -e 's/<seg id="[0-9]*">\s*//g'
+ grep '<seg id' orig/de-en/IWSLT14.TEDX.dev2012.de-en.en.xml
+ sed -e 's/\s*<\/seg>\s*//g'
+ sed -e 's/\’/\'\''/g'
+ perl mosesdecoder/scripts/tokenizer/lowercase.perl
+ perl mosesdecoder/scripts/tokenizer/tokenizer.perl -threads 8 -l en
Tokenizer Version 1.1
Language: en
Number of threads: 8
+ echo ''

+ echo 'creating train, valid, test...'
creating train, valid, test...
+ for l in $src $tgt
+ awk '{if (NR%23 == 0)  print $0; }' iwslt14.tokenized.de-en/tmp/train.tags.de-en.de
+ awk '{if (NR%23 != 0)  print $0; }' iwslt14.tokenized.de-en/tmp/train.tags.de-en.de
+ cat iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.de iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.de iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.de iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.de iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.de
+ for l in $src $tgt
+ awk '{if (NR%23 == 0)  print $0; }' iwslt14.tokenized.de-en/tmp/train.tags.de-en.en
+ awk '{if (NR%23 != 0)  print $0; }' iwslt14.tokenized.de-en/tmp/train.tags.de-en.en
+ cat iwslt14.tokenized.de-en/tmp/IWSLT14.TED.dev2010.de-en.en iwslt14.tokenized.de-en/tmp/IWSLT14.TEDX.dev2012.de-en.en iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2010.de-en.en iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2011.de-en.en iwslt14.tokenized.de-en/tmp/IWSLT14.TED.tst2012.de-en.en
+ TRAIN=iwslt14.tokenized.de-en/tmp/train.en-de
+ BPE_CODE=iwslt14.tokenized.de-en/code
+ rm -f iwslt14.tokenized.de-en/tmp/train.en-de
+ for l in $src $tgt
+ cat iwslt14.tokenized.de-en/tmp/train.de
+ for l in $src $tgt
+ cat iwslt14.tokenized.de-en/tmp/train.en
+ echo 'learn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...'
learn_bpe.py on iwslt14.tokenized.de-en/tmp/train.en-de...
+ python subword-nmt/subword_nmt/learn_bpe.py -s 10000
+ for L in $src $tgt
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to train.de...'
apply_bpe.py to train.de...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to valid.de...'
apply_bpe.py to valid.de...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to test.de...'
apply_bpe.py to test.de...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
+ for L in $src $tgt
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to train.en...'
apply_bpe.py to train.en...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to valid.en...'
apply_bpe.py to valid.en...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
+ for f in train.$L valid.$L test.$L
+ echo 'apply_bpe.py to test.en...'
apply_bpe.py to test.en...
+ python subword-nmt/subword_nmt/apply_bpe.py -c iwslt14.tokenized.de-en/code
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# pwd
/mnt/fairseq/fairseq/examples/translation
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls
README.md                orig                             prepare-wmt14en2de.sh
iwslt14.tokenized.de-en  prepare-iwslt14.sh               prepare-wmt14en2fr.sh
mosesdecoder             prepare-iwslt17-multilingual.sh  subword-nmt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ll
total 0
drwxr-xr-x 0 root root 503187866 Jul 24 08:02 ./
drwxr-xr-x 0 root root 225504615 Jul 23 11:23 ../
-rw-r--r-- 0 root root     14758 Jul 23 11:23 README.md
drwxr-xr-x 0 root root 222958996 Jul 24 08:03 iwslt14.tokenized.de-en/
drwxrwxrwx 0 root root 204202516 Jul 24 07:14 mosesdecoder/
drwxr-xr-x 0 root root  75317183 Jul 24 08:02 orig/
-rw-r--r-- 0 root root      3087 Jul 24 07:59 prepare-iwslt14.sh
-rw-r--r-- 0 root root      4386 Jul 23 11:23 prepare-iwslt17-multilingual.sh
-rw-r--r-- 0 root root      3962 Jul 23 11:23 prepare-wmt14en2de.sh
-rw-r--r-- 0 root root      3724 Jul 23 11:23 prepare-wmt14en2fr.sh
drwxr-xr-x 0 root root    679254 Jul 24 06:32 subword-nmt/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls iwslt14.tokenized.de-en/
code  test.de  test.en  tmp  train.de  train.en  valid.de  valid.en
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# ls orig/
de-en  de-en.tgz
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/examples/translation# cd ../..
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# ls
CODE_OF_CONDUCT.md  LICENSE    build  examples  fairseq.egg-info  hubconf.py      scripts   tests
CONTRIBUTING.md     README.md  docs   fairseq   fairseq_cli       pyproject.toml  setup.py  train.py
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# TEXT=examples/translation/iwslt14.tokenized.de-en/
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# fairseq-preprocess --source-lang de --target-lang en \
>     --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \
>     --destdir data-bin/iwslt14.tokenized.de-en
2020-07-24 08:10:50 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/iwslt14.tokenized.de-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, profile=False, quantization_config_path=None, seed=None, source_lang='de', srcdict=None, target_lang='en', task='translation', tensorboard_logdir='', testpref='examples/translation/iwslt14.tokenized.de-en//test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='examples/translation/iwslt14.tokenized.de-en//train', user_dir=None, validpref='examples/translation/iwslt14.tokenized.de-en//valid', workers=1)
2020-07-24 08:11:03 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types
2020-07-24 08:11:24 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en//train.de: 160239 sents, 4035591 tokens, 0.0% replaced by <unk>
2020-07-24 08:11:24 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types
2020-07-24 08:11:25 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en//valid.de: 7283 sents, 182592 tokens, 0.0192% replaced by <unk>
2020-07-24 08:11:25 | INFO | fairseq_cli.preprocess | [de] Dictionary: 8848 types
2020-07-24 08:11:26 | INFO | fairseq_cli.preprocess | [de] examples/translation/iwslt14.tokenized.de-en//test.de: 6750 sents, 161838 tokens, 0.0636% replaced by <unk>
2020-07-24 08:11:26 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types
2020-07-24 08:11:46 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en//train.en: 160239 sents, 3949114 tokens, 0.0% replaced by <unk>
2020-07-24 08:11:46 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types
2020-07-24 08:11:47 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en//valid.en: 7283 sents, 178622 tokens, 0.00448% replaced by <unk>
2020-07-24 08:11:47 | INFO | fairseq_cli.preprocess | [en] Dictionary: 6632 types
2020-07-24 08:11:47 | INFO | fairseq_cli.preprocess | [en] examples/translation/iwslt14.tokenized.de-en//test.en: 6750 sents, 156928 tokens, 0.00892% replaced by <unk>
2020-07-24 08:11:47 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iwslt14.tokenized.de-en
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# vim data-bin/iwslt14.tokenized.de-en/
.ipynb_checkpoints/ test.de-en.de.bin   train.de-en.de.bin  valid.de-en.de.bin  
dict.de.txt         test.de-en.de.idx   train.de-en.de.idx  valid.de-en.de.idx  
dict.en.txt         test.de-en.en.bin   train.de-en.en.bin  valid.de-en.en.bin  
preprocess.log      test.de-en.en.idx   train.de-en.en.idx  valid.de-en.en.idx  
(myconda) root@576918a23912:/mnt/fairseq/fairseq# vim data-bin/iwslt14.tokenized.de-en/
.ipynb_checkpoints/ test.de-en.de.bin   train.de-en.de.bin  valid.de-en.de.bin  
dict.de.txt         test.de-en.de.idx   train.de-en.de.idx  valid.de-en.de.idx  
dict.en.txt         test.de-en.en.bin   train.de-en.en.bin  valid.de-en.en.bin  
preprocess.log      test.de-en.en.idx   train.de-en.en.idx  valid.de-en.en.idx  
(myconda) root@576918a23912:/mnt/fairseq/fairseq# vim data-bin/iwslt14.tokenized.de-en/
.ipynb_checkpoints/ test.de-en.de.bin   train.de-en.de.bin  valid.de-en.de.bin  
dict.de.txt         test.de-en.de.idx   train.de-en.de.idx  valid.de-en.de.idx  
dict.en.txt         test.de-en.en.bin   train.de-en.en.bin  valid.de-en.en.bin  
preprocess.log      test.de-en.en.idx   train.de-en.en.idx  valid.de-en.en.idx  
(myconda) root@576918a23912:/mnt/fairseq/fairseq# vim data-bin/iwslt14.tokenized.de-en/train.de-en.de.idx 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# ls
CODE_OF_CONDUCT.md  README.md  docs      fairseq.egg-info  pyproject.toml  tests
CONTRIBUTING.md     build      examples  fairseq_cli       scripts         train.py
LICENSE             data-bin   fairseq   hubconf.py        setup.py
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# mkdir -p checkpoints/fconv
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en \
>     --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \
>     --arch fconv_iwslt_de_en --save-dir checkpoints/fconv
2020-07-24 08:13:49 | INFO | fairseq_cli.train | Namespace(all_gather_list_size=16384, arch='fconv_iwslt_de_en', best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.1, cpu=False, criterion='cross_entropy', curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention='True', decoder_embed_dim=256, decoder_embed_path=None, decoder_layers='[(256, 3)] * 3', decoder_out_embed_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.2, empty_cache_freq=0, encoder_embed_dim=256, encoder_embed_path=None, encoder_layers='[(256, 3)] * 4', eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.25], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_tokens_valid=4000, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, momentum=0.99, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=True, nprocs_per_node=1, num_batch_buckets=0, num_workers=1, optimizer='nag', optimizer_overrides='{}', patience=-1, profile=False, quantization_config_path=None, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/fconv', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=0, weight_decay=0.0)
2020-07-24 08:13:49 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2020-07-24 08:13:49 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2020-07-24 08:13:49 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2020-07-24 08:13:49 | INFO | fairseq.data.data_utils | loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2020-07-24 08:13:49 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2020-07-24 08:13:49 | INFO | fairseq_cli.train | FConvModel(
  (encoder): FConvEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 256, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1024, 256, padding_idx=1)
    (fc1): Linear(in_features=256, out_features=256, bias=True)
    (projections): ModuleList(
      (0): None
      (1): None
      (2): None
      (3): None
    )
    (convolutions): ModuleList(
      (0): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))
      (1): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))
      (2): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))
      (3): ConvTBC(256, 512, kernel_size=(3,), padding=(1,))
    )
    (fc2): Linear(in_features=256, out_features=256, bias=True)
  )
  (decoder): FConvDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 256, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1024, 256, padding_idx=1)
    (fc1): Linear(in_features=256, out_features=256, bias=True)
    (projections): ModuleList(
      (0): None
      (1): None
      (2): None
    )
    (convolutions): ModuleList(
      (0): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))
      (1): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))
      (2): LinearizedConvolution(256, 512, kernel_size=(3,), padding=(2,))
    )
    (attention): ModuleList(
      (0): AttentionLayer(
        (in_projection): Linear(in_features=256, out_features=256, bias=True)
        (out_projection): Linear(in_features=256, out_features=256, bias=True)
      )
      (1): AttentionLayer(
        (in_projection): Linear(in_features=256, out_features=256, bias=True)
        (out_projection): Linear(in_features=256, out_features=256, bias=True)
      )
      (2): AttentionLayer(
        (in_projection): Linear(in_features=256, out_features=256, bias=True)
        (out_projection): Linear(in_features=256, out_features=256, bias=True)
      )
    )
    (fc2): Linear(in_features=256, out_features=256, bias=True)
    (fc3): Linear(in_features=256, out_features=6632, bias=True)
  )
)
2020-07-24 08:13:49 | INFO | fairseq_cli.train | model fconv_iwslt_de_en, criterion CrossEntropyCriterion
2020-07-24 08:13:49 | INFO | fairseq_cli.train | num. model params: 9618384 (num. trained: 9618384)
2020-07-24 08:13:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-07-24 08:13:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 7.795 GB ; name = GeForce RTX 2070                        
2020-07-24 08:13:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2020-07-24 08:13:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2020-07-24 08:13:51 | INFO | fairseq_cli.train | max tokens per GPU = 4000 and max sentences per GPU = None
2020-07-24 08:13:51 | INFO | fairseq.trainer | no existing checkpoint found checkpoints/fconv/checkpoint_last.pt
2020-07-24 08:13:51 | INFO | fairseq.trainer | loading train data for epoch 1
2020-07-24 08:13:51 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2020-07-24 08:13:51 | INFO | fairseq.data.data_utils | loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2020-07-24 08:13:51 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2020-07-24 08:13:53 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2020-07-24 08:13:53 | INFO | fairseq_cli.train | begin training epoch 1
epoch 001:   0%|                                                            | 0/1131 [00:00<?, ?it/s]/mnt/fairseq/fairseq/fairseq/utils.py:301: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
epoch 001: 100%|▉| 1129/1131 [00:37<00:00, 30.22it/s, loss=6.388, ppl=83.76, wps=105378, ups=30.1, wp2020-07-24 08:14:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
                                                                                                    2020-07-24 08:14:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 5.951 | ppl 61.87 | wps 260441 | wpb 2835.3 | bsz 115.6 | num_updates 1131
2020-07-24 08:14:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:14:36 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint1.pt (epoch 1 @ 1131 updates, score 5.951) (writing took 4.273003853857517 seconds)
2020-07-24 08:14:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)          
2020-07-24 08:14:36 | INFO | train | epoch 001 | loss 7.551 | ppl 187.51 | wps 92255.7 | ups 26.42 | wpb 3491.7 | bsz 141.7 | num_updates 1131 | lr 0.25 | gnorm 0.551 | clip 100 | train_wall 38 | wall 45
2020-07-24 08:14:36 | INFO | fairseq_cli.train | begin training epoch 1
epoch 002: 100%|▉| 1129/1131 [00:37<00:00, 30.09it/s, loss=5.085, ppl=33.95, wps=104435, ups=29.91, w2020-07-24 08:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:15:15 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 4.694 | ppl 25.88 | wps 259944 | wpb 2835.3 | bsz 115.6 | num_updates 2262 | best_loss 4.694                                
2020-07-24 08:15:15 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:15:17 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint2.pt (epoch 2 @ 2262 updates, score 4.694) (writing took 2.7580623775720596 seconds)
2020-07-24 08:15:17 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)          
2020-07-24 08:15:17 | INFO | train | epoch 002 | loss 5.543 | ppl 46.63 | wps 95428.5 | ups 27.33 | wpb 3491.7 | bsz 141.7 | num_updates 2262 | lr 0.25 | gnorm 0.444 | clip 100 | train_wall 37 | wall 86
2020-07-24 08:15:17 | INFO | fairseq_cli.train | begin training epoch 2
epoch 003: 100%|███████▉| 1130/1131 [00:37<00:00, 29.99it/s, loss=4.508, ppl=22.75, wps=102656, ups=30.04, wpb=3417.6, bsz=144.7, num_updates=3300, lr=0.25, gnorm=0.475, clip=100, train_wall=3, wall=121]2020-07-24 08:15:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:15:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 4.148 | ppl 17.73 | wps 259487 | wpb 2835.3 | bsz 115.6 | num_updates 3393 | best_loss 4.148                                
2020-07-24 08:15:56 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:16:00 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint3.pt (epoch 3 @ 3393 updates, score 4.148) (writing took 3.4815482906997204 seconds)
2020-07-24 08:16:00 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)                                                                                                                
2020-07-24 08:16:00 | INFO | train | epoch 003 | loss 4.672 | ppl 25.49 | wps 93498.5 | ups 26.78 | wpb 3491.7 | bsz 141.7 | num_updates 3393 | lr 0.25 | gnorm 0.472 | clip 100 | train_wall 38 | wall 128
2020-07-24 08:16:00 | INFO | fairseq_cli.train | begin training epoch 3
epoch 004: 100%|███████████▉| 1128/1131 [00:37<00:00, 29.96it/s, loss=4.094, ppl=17.08, wps=104384, ups=29.6, wpb=3526.8, bsz=142, num_updates=4500, lr=0.25, gnorm=0.44, clip=100, train_wall=3, wall=166]2020-07-24 08:16:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:16:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 3.833 | ppl 14.25 | wps 259229 | wpb 2835.3 | bsz 115.6 | num_updates 4524 | best_loss 3.833                                
2020-07-24 08:16:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:16:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint4.pt (epoch 4 @ 4524 updates, score 3.833) (writing took 2.3564857952296734 seconds)
2020-07-24 08:16:41 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)                                                                                                                
2020-07-24 08:16:41 | INFO | train | epoch 004 | loss 4.22 | ppl 18.64 | wps 95973.3 | ups 27.49 | wpb 3491.7 | bsz 141.7 | num_updates 4524 | lr 0.25 | gnorm 0.453 | clip 100 | train_wall 38 | wall 169
2020-07-24 08:16:41 | INFO | fairseq_cli.train | begin training epoch 4
epoch 005: 100%|████████▉| 1129/1131 [00:37<00:00, 29.83it/s, loss=3.961, ppl=15.57, wps=103628, ups=29.85, wpb=3471.9, bsz=128.2, num_updates=5600, lr=0.25, gnorm=0.44, clip=100, train_wall=3, wall=206]2020-07-24 08:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:17:20 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 3.611 | ppl 12.21 | wps 259628 | wpb 2835.3 | bsz 115.6 | num_updates 5655 | best_loss 3.611                                
2020-07-24 08:17:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:17:22 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint5.pt (epoch 5 @ 5655 updates, score 3.611) (writing took 2.1323170587420464 seconds)
2020-07-24 08:17:22 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)                                                                                                                
2020-07-24 08:17:22 | INFO | train | epoch 005 | loss 3.943 | ppl 15.38 | wps 96446.5 | ups 27.62 | wpb 3491.7 | bsz 141.7 | num_updates 5655 | lr 0.25 | gnorm 0.435 | clip 100 | train_wall 38 | wall 210
2020-07-24 08:17:22 | INFO | fairseq_cli.train | begin training epoch 5
epoch 006: 100%|████████▉| 1128/1131 [00:38<00:00, 29.98it/s, loss=3.799, ppl=13.92, wps=103343, ups=29.7, wpb=3479.6, bsz=133.4, num_updates=6700, lr=0.25, gnorm=0.418, clip=100, train_wall=3, wall=246]2020-07-24 08:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:18:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 3.511 | ppl 11.4 | wps 255351 | wpb 2835.3 | bsz 115.6 | num_updates 6786 | best_loss 3.511                                 
2020-07-24 08:18:01 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:18:03 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint6.pt (epoch 6 @ 6786 updates, score 3.511) (writing took 2.149700492620468 seconds)
2020-07-24 08:18:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)                                                                                                                
2020-07-24 08:18:03 | INFO | train | epoch 006 | loss 3.755 | ppl 13.51 | wps 96045.5 | ups 27.51 | wpb 3491.7 | bsz 141.7 | num_updates 6786 | lr 0.25 | gnorm 0.422 | clip 100 | train_wall 38 | wall 252
2020-07-24 08:18:03 | INFO | fairseq_cli.train | begin training epoch 6
epoch 007: 100%|████████▉| 1129/1131 [00:38<00:00, 30.07it/s, loss=3.635, ppl=12.43, wps=103167, ups=29.9, wpb=3450.5, bsz=139.9, num_updates=7900, lr=0.25, gnorm=0.406, clip=100, train_wall=3, wall=289]2020-07-24 08:18:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:18:42 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 3.397 | ppl 10.53 | wps 258080 | wpb 2835.3 | bsz 115.6 | num_updates 7917 | best_loss 3.397                                
2020-07-24 08:18:42 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:18:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint7.pt (epoch 7 @ 7917 updates, score 3.397) (writing took 2.365590989589691 seconds)
2020-07-24 08:18:44 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)                                                                                                                
2020-07-24 08:18:44 | INFO | train | epoch 007 | loss 3.616 | ppl 12.26 | wps 95711.4 | ups 27.41 | wpb 3491.7 | bsz 141.7 | num_updates 7917 | lr 0.25 | gnorm 0.407 | clip 100 | train_wall 38 | wall 293
2020-07-24 08:18:44 | INFO | fairseq_cli.train | begin training epoch 7
epoch 008: 100%|█████████▉| 1129/1131 [00:38<00:00, 30.00it/s, loss=3.543, ppl=11.66, wps=102024, ups=29.89, wpb=3413.3, bsz=133, num_updates=9000, lr=0.25, gnorm=0.389, clip=100, train_wall=3, wall=329]2020-07-24 08:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:19:23 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 3.292 | ppl 9.8 | wps 260007 | wpb 2835.3 | bsz 115.6 | num_updates 9048 | best_loss 3.292                                  
2020-07-24 08:19:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:19:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint8.pt (epoch 8 @ 9048 updates, score 3.292) (writing took 2.658631533384323 seconds)
2020-07-24 08:19:26 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)                                                                                                                
2020-07-24 08:19:26 | INFO | train | epoch 008 | loss 3.493 | ppl 11.26 | wps 95107.3 | ups 27.24 | wpb 3491.7 | bsz 141.7 | num_updates 9048 | lr 0.25 | gnorm 0.393 | clip 100 | train_wall 38 | wall 334
2020-07-24 08:19:26 | INFO | fairseq_cli.train | begin training epoch 8
epoch 009:  68%|██████▊   | 771/1131 [00:26<00:12, 29.50it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  69%|██████▊   | 775/1131 [00:26<00:11, 29.89it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  69%|██████▉   | 778/1131 [00:26<00:12, 29.36it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  69%|██████▉   | 782/1131 [00:26<00:11, 29.63it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  69%|██████▉   | 785/1131 [00:26<00:11, 29.61it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  70%|██████▉   | 788/1131 [00:26<00:11, 29.37it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  70%|██████▉   | 791/1131 [00:26<00:11, 29.37it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  70%|███████   | 794/1131 [00:26<00:11, 29.25it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  70%|███████   | 797/1131 [00:26<00:11, 29.06it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  71%|███████   | 800/1131 [00:27<00:11, 29.25it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  71%|███████   | 803/1131 [00:27<00:11, 29.37it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  71%|███████▏  | 806/1131 [00:27<00:11, 29.25it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  72%|███████▏  | 809/1131 [00:27<00:10, 29.39it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  72%|███████▏  | 812/1131 [00:27<00:10, 29.24it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  72%|███████▏  | 816/1131 [00:27<00:10, 29.49it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  72%|███████▏  | 819/1131 [00:27<00:10, 29.53it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  73%|███████▎  | 822/1131 [00:27<00:10, 29.55it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  73%|███████▎  | 826/1131 [00:27<00:10, 29.90it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  73%|███████▎  | 830/1131 [00:28<00:09, 30.20it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  74%|███████▎  | 834/1131 [00:28<00:09, 30.24it/s, loss=3.379, ppl=10.4, wps=103646, ups=29.53, wpb=3509.7, bsz=148.3, num_updates=9800, lr=0.25, gnorm=0.385, clip=100, train_wall=3, wall=360epoch 009:  74%|████epoch 009:  77%|█████████▏  | 871/1131 [00:29<00:08, 30.18it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bepoch 009:  77%|█████████▎  | 875/1131 [00:29<00:08, 30.05it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=epoch 009:  78%|█████████▎  | 879/1131 [00:29<00:08, 29.99it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  78%|█████████▎  | 883/1131 [00:29<00:08, 30.15it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  78%|█████████▍  | 887/1131 [00:29<00:08, 29.69it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  79%|█████████▍  | 890/1131 [00:30<00:08, 29.39it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  79%|█████████▍  | 893/1131 [00:30<00:08, 29.51it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  79%|█████████▌  | 896/1131 [00:30<00:07, 29.51it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  80%|█████████▌  | 900/1131 [00:30<00:07, 29.87it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  80%|█████████▌  | 903/1131 [00:30<00:07, 29.69it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  80%|█████████▌  | 906/1131 [00:30<00:07, 29.69it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  80%|█████████▋  | 910/1131 [00:30<00:07, 29.85it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  81%|█████████▋  | 914/1131 [00:30<00:07, 30.08it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  81%|█████████▋  | 918/1131 [00:31<00:07, 29.78it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  82%|█████████▊  | 922/1131 [00:31<00:07, 29.75it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  82%|█████████▊  | 925/1131 [00:31<00:06, 29.68it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  82%|█████████▊  | 928/1131 [00:31<00:06, 29.65it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  82%|█████████▉  | 931/1131 [00:31<00:06, 29.60it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  83%|█████████▉  | 934/1131 [00:31<00:06, 29.61it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  83%|█████████▉  | 937/1131 [00:31<00:06, 29.60it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  83%|█████████▉  | 940/1131 [00:31<00:06, 29.58it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  83%|██████████  | 944/1131 [00:31<00:06, 29.84it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  84%|██████████  | 947/1131 [00:31<00:06, 29.76it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  84%|██████████  | 950/1131 [00:32<00:06, 29.72it/s, loss=3.379, ppl=10.4, wps=104132, ups=29.65, wpb=3512, bsz=146.6, num_updates=9900, lr=0.25, gnorm=0.379, clip=100, train_wall=3, wall=363epoch 009:  84%|███████▌ | 954/1131 [00:32<00:05, 30.16it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  85%|███████▌ | 958/1131 [00:32<00:05, 30.09it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  85%|███████▋ | 962/1131 [00:32<00:05, 30.06it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  85%|███████▋ | 966/1131 [00:32<00:05, 30.08it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  86%|███████▋ | 970/1131 [00:32<00:05, 29.96it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  86%|███████▋ | 973/1131 [00:32<00:05, 29.71it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  86%|███████▊ | 976/1131 [00:32<00:05, 29.47it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  87%|███████▊ | 979/1131 [00:33<00:05, 29.38it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  87%|███████▊ | 982/1131 [00:33<00:05, 29.54it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  87%|███████▊ | 986/1131 [00:33<00:04, 29.76it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  88%|███████▉ | 990/1131 [00:33<00:04, 30.23it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  88%|███████▉ | 994/1131 [00:33<00:04, 30.22it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  88%|███████▉ | 998/1131 [00:33<00:04, 30.31it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  89%|███████ | 1002/1131 [00:33<00:04, 29.94it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  89%|███████ | 1005/1131 [00:33<00:04, 29.90it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  89%|███████▏| 1008/1131 [00:34<00:04, 29.54it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  89%|███████▏| 1011/1131 [00:34<00:04, 29.48it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  90%|███████▏| 1014/1131 [00:34<00:03, 29.61it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  90%|███████▏| 1018/1131 [00:34<00:03, 29.84it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  90%|███████▏| 1021/1131 [00:34<00:03, 29.66it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  91%|███████▏| 1024/1131 [00:34<00:03, 29.66it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  91%|███████▎| 1028/1131 [00:34<00:03, 29.79it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  91%|███████▎| 1032/1131 [00:34<00:03, 30.12it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  92%|███████▎| 1036/1131 [00:34<00:03, 29.89it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  92%|███████▎| 1040/1131 [00:35<00:03, 30.26it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  92%|███████▍| 1044/1131 [00:35<00:02, 30.19it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  93%|███████▍| 1048/1131 [00:35<00:02, 30.03it/s, loss=3.409, ppl=10.62, wps=102529, ups=29.8, wpb=3440.4, bsz=147.8, num_updates=10000, lr=0.25, gnorm=0.395, clip=100, train_wall=3, wall=366epoch 009:  93%|██████▌| 1052/1131 [00:35<00:02, 29.78it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  93%|██████▌| 1055/1131 [00:35<00:02, 29.58it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  94%|██████▌| 1059/1131 [00:35<00:02, 29.68it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  94%|██████▌| 1062/1131 [00:35<00:02, 29.73it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  94%|██████▌| 1065/1131 [00:35<00:02, 29.69it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  94%|██████▌| 1068/1131 [00:36<00:02, 29.57it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  95%|██████▋| 1071/1131 [00:36<00:02, 29.26it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  95%|██████▋| 1074/1131 [00:36<00:01, 28.96it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  95%|██████▋| 1078/1131 [00:36<00:01, 29.32it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  96%|██████▋| 1081/1131 [00:36<00:01, 29.14it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  96%|██████▋| 1084/1131 [00:36<00:01, 29.28it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  96%|██████▋| 1087/1131 [00:36<00:01, 29.30it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  96%|██████▋| 1090/1131 [00:36<00:01, 29.28it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  97%|██████▊| 1093/1131 [00:36<00:01, 29.45it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  97%|██████▊| 1096/1131 [00:36<00:01, 29.52it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  97%|██████▊| 1100/1131 [00:37<00:01, 29.81it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  98%|██████▊| 1104/1131 [00:37<00:00, 29.90it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  98%|██████▊| 1107/1131 [00:37<00:00, 29.93it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  98%|██████▊| 1110/1131 [00:37<00:00, 29.78it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  98%|██████▉| 1113/1131 [00:37<00:00, 29.72it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  99%|██████▉| 1116/1131 [00:37<00:00, 29.62it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  99%|██████▉| 1119/1131 [00:37<00:00, 29.57it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009:  99%|██████▉| 1123/1131 [00:37<00:00, 29.89it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370epoch 009: 100%|██████▉| 1127/1131 [00:38<00:00, 29.86it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370]2020-07-24 08:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
                                                                                                    2020-07-24 08:20:05 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 3.246 | ppl 9.49 | wps 259005 | wpb 2835.3 | bsz 115.6 | num_updates 10179 | best_loss 3.246
2020-07-24 08:20:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:20:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint9.pt (epoch 9 @ 10179 updates, score 3.246) (writing took 2.576776046305895 seconds)
epoch 009: 100%|███████| 1131/1131 [00:41<00:00,  3.54it/s, loss=3.441, ppl=10.86, wps=104328, ups=29.91, wpb=3487.6, bsz=132.3, num_updates=10100, lr=0.25, gnorm=0.375, clip=100, train_wall=3, wall=370                                                                                                                                                                                                          2020-07-24 08:20:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2020-07-24 08:20:07 | INFO | train | epoch 009 | loss 3.41 | ppl 10.63 | wps 95186.6 | ups 27.26 | wpb 3491.7 | bsz 141.7 | num_updates 10179 | lr 0.25 | gnorm 0.386 | clip 100 | train_wall 38 | wall 376
2020-07-24 08:20:07 | INFO | fairseq_cli.train | begin training epoch 9
epoch 010: 100%|▉| 1128/1131 [00:38<00:00, 29.99it/s, loss=3.328, ppl=10.04, wps=103254, ups=29.83, w2020-07-24 08:20:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
                                                                                                    2020-07-24 08:20:46 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 3.192 | ppl 9.14 | wps 258211 | wpb 2835.3 | bsz 115.6 | num_updates 11310 | best_loss 3.192
2020-07-24 08:20:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:20:48 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint10.pt (epoch 10 @ 11310 updates, score 3.192) (writing took 2.3527587428689003 seconds)
2020-07-24 08:20:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)         
2020-07-24 08:20:48 | INFO | train | epoch 010 | loss 3.333 | ppl 10.07 | wps 95576.2 | ups 27.37 | wpb 3491.7 | bsz 141.7 | num_updates 11310 | lr 0.25 | gnorm 0.373 | clip 100 | train_wall 38 | wall 417
2020-07-24 08:20:48 | INFO | fairseq_cli.train | begin training epoch 10
epoch 011: 100%|▉| 1130/1131 [00:38<00:00, 29.95it/s, loss=3.31, ppl=9.92, wps=102945, ups=29.6, wpb=2020-07-24 08:21:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:21:27 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 3.149 | ppl 8.87 | wps 259923 | wpb 2835.3 | bsz 115.6 | num_updates 12441 | best_loss 3.149                                
2020-07-24 08:21:27 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:21:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint11.pt (epoch 11 @ 12441 updates, score 3.149) (writing took 2.814631462097168 seconds)
2020-07-24 08:21:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)         
2020-07-24 08:21:30 | INFO | train | epoch 011 | loss 3.26 | ppl 9.58 | wps 94678.1 | ups 27.12 | wpb 3491.7 | bsz 141.7 | num_updates 12441 | lr 0.25 | gnorm 0.365 | clip 100 | train_wall 38 | wall 459
2020-07-24 08:21:30 | INFO | fairseq_cli.train | begin training epoch 11
epoch 012: 100%|█████████▉| 1130/1131 [00:38<00:00, 29.67it/s, loss=3.218, ppl=9.3, wps=103542, ups=29.65, wpb=3491.8, bsz=138.1, num_updates=13500, lr=0.25, gnorm=0.35, clip=100, train_wall=3, wall=495]2020-07-24 08:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:22:09 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 3.108 | ppl 8.62 | wps 253782 | wpb 2835.3 | bsz 115.6 | num_updates 13572 | best_loss 3.108                                
2020-07-24 08:22:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:22:12 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint12.pt (epoch 12 @ 13572 updates, score 3.108) (writing took 2.855370983481407 seconds)
2020-07-24 08:22:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)                                                                                                               
2020-07-24 08:22:12 | INFO | train | epoch 012 | loss 3.196 | ppl 9.17 | wps 94343 | ups 27.02 | wpb 3491.7 | bsz 141.7 | num_updates 13572 | lr 0.25 | gnorm 0.357 | clip 100 | train_wall 38 | wall 501
2020-07-24 08:22:12 | INFO | fairseq_cli.train | begin training epoch 12
epoch 013: 100%|████████▉| 1130/1131 [00:38<00:00, 29.59it/s, loss=3.116, ppl=8.67, wps=104988, ups=29.54, wpb=3554.2, bsz=152.3, num_updates=14700, lr=0.25, gnorm=0.34, clip=100, train_wall=3, wall=539]2020-07-24 08:22:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:22:51 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 3.078 | ppl 8.45 | wps 257218 | wpb 2835.3 | bsz 115.6 | num_updates 14703 | best_loss 3.078                                
2020-07-24 08:22:51 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:22:53 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint13.pt (epoch 13 @ 14703 updates, score 3.078) (writing took 2.3803828209638596 seconds)
2020-07-24 08:22:53 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)                                                                                                               
2020-07-24 08:22:53 | INFO | train | epoch 013 | loss 3.148 | ppl 8.87 | wps 95482.8 | ups 27.35 | wpb 3491.7 | bsz 141.7 | num_updates 14703 | lr 0.25 | gnorm 0.351 | clip 100 | train_wall 38 | wall 542
2020-07-24 08:22:53 | INFO | fairseq_cli.train | begin training epoch 13
epoch 014: 100%|████████▉| 1130/1131 [00:38<00:00, 29.78it/s, loss=3.144, ppl=8.84, wps=105895, ups=29.47, wpb=3593.1, bsz=139.7, num_updates=15800, lr=0.25, gnorm=0.33, clip=100, train_wall=3, wall=579]2020-07-24 08:23:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:23:32 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 3.054 | ppl 8.3 | wps 257725 | wpb 2835.3 | bsz 115.6 | num_updates 15834 | best_loss 3.054                                 
2020-07-24 08:23:32 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:23:34 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint14.pt (epoch 14 @ 15834 updates, score 3.054) (writing took 2.2447174713015556 seconds)
2020-07-24 08:23:34 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)                                                                                                               
2020-07-24 08:23:34 | INFO | train | epoch 014 | loss 3.107 | ppl 8.61 | wps 96016.9 | ups 27.5 | wpb 3491.7 | bsz 141.7 | num_updates 15834 | lr 0.25 | gnorm 0.348 | clip 100 | train_wall 38 | wall 583
2020-07-24 08:23:34 | INFO | fairseq_cli.train | begin training epoch 14
epoch 015: 100%|███████▉| 1127/1131 [00:38<00:00, 29.69it/s, loss=3.079, ppl=8.45, wps=103172, ups=29.59, wpb=3486.7, bsz=146.4, num_updates=16900, lr=0.25, gnorm=0.334, clip=100, train_wall=3, wall=619]2020-07-24 08:24:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:24:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.029 | ppl 8.16 | wps 258287 | wpb 2835.3 | bsz 115.6 | num_updates 16965 | best_loss 3.029                                
2020-07-24 08:24:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:24:18 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint15.pt (epoch 15 @ 16965 updates, score 3.029) (writing took 4.643003799021244 seconds)
2020-07-24 08:24:18 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)                                                                                                               
2020-07-24 08:24:18 | INFO | train | epoch 015 | loss 3.065 | ppl 8.37 | wps 90617.5 | ups 25.95 | wpb 3491.7 | bsz 141.7 | num_updates 16965 | lr 0.25 | gnorm 0.341 | clip 100 | train_wall 38 | wall 627
2020-07-24 08:24:18 | INFO | fairseq_cli.train | begin training epoch 15
epoch 016: 100%|███████▉| 1127/1131 [00:37<00:00, 29.74it/s, loss=3.057, ppl=8.32, wps=103665, ups=29.43, wpb=3522.3, bsz=143.6, num_updates=18000, lr=0.25, gnorm=0.322, clip=100, train_wall=3, wall=662]2020-07-24 08:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:24:57 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.004 | ppl 8.02 | wps 258856 | wpb 2835.3 | bsz 115.6 | num_updates 18096 | best_loss 3.004                                
2020-07-24 08:24:57 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:24:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint16.pt (epoch 16 @ 18096 updates, score 3.004) (writing took 2.358475726097822 seconds)
2020-07-24 08:24:59 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)                                                                                                               
2020-07-24 08:24:59 | INFO | train | epoch 016 | loss 3.029 | ppl 8.16 | wps 95766.4 | ups 27.43 | wpb 3491.7 | bsz 141.7 | num_updates 18096 | lr 0.25 | gnorm 0.335 | clip 100 | train_wall 38 | wall 668
2020-07-24 08:24:59 | INFO | fairseq_cli.train | begin training epoch 16
epoch 017: 100%|███████▉| 1130/1131 [00:38<00:00, 30.27it/s, loss=3.003, ppl=8.02, wps=104778, ups=29.55, wpb=3546.1, bsz=140.6, num_updates=19200, lr=0.25, gnorm=0.321, clip=100, train_wall=3, wall=705]2020-07-24 08:25:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:25:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 2.992 | ppl 7.96 | wps 258590 | wpb 2835.3 | bsz 115.6 | num_updates 19227 | best_loss 2.992                                
2020-07-24 08:25:38 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:25:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint17.pt (epoch 17 @ 19227 updates, score 2.992) (writing took 3.01807651668787 seconds)
2020-07-24 08:25:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)                                                                                                               
2020-07-24 08:25:41 | INFO | train | epoch 017 | loss 2.989 | ppl 7.94 | wps 94148.4 | ups 26.96 | wpb 3491.7 | bsz 141.7 | num_updates 19227 | lr 0.25 | gnorm 0.329 | clip 100 | train_wall 38 | wall 710
2020-07-24 08:25:41 | INFO | fairseq_cli.train | begin training epoch 17
epoch 018: 100%|███████▉| 1130/1131 [00:38<00:00, 29.81it/s, loss=2.969, ppl=7.83, wps=103016, ups=29.61, wpb=3478.6, bsz=149.3, num_updates=20300, lr=0.25, gnorm=0.318, clip=100, train_wall=3, wall=746]2020-07-24 08:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:26:20 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 2.97 | ppl 7.84 | wps 257714 | wpb 2835.3 | bsz 115.6 | num_updates 20358 | best_loss 2.97                                  
2020-07-24 08:26:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:26:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint18.pt (epoch 18 @ 20358 updates, score 2.97) (writing took 2.606391068547964 seconds)
2020-07-24 08:26:23 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)                                                                                                               
2020-07-24 08:26:23 | INFO | train | epoch 018 | loss 2.963 | ppl 7.8 | wps 95145.6 | ups 27.25 | wpb 3491.7 | bsz 141.7 | num_updates 20358 | lr 0.25 | gnorm 0.328 | clip 100 | train_wall 38 | wall 751
2020-07-24 08:26:23 | INFO | fairseq_cli.train | begin training epoch 18
epoch 019: 100%|███████▉| 1130/1131 [00:38<00:00, 29.73it/s, loss=2.944, ppl=7.69, wps=104228, ups=29.45, wpb=3539.4, bsz=151.3, num_updates=21400, lr=0.25, gnorm=0.317, clip=100, train_wall=3, wall=787]2020-07-24 08:27:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:27:02 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 2.958 | ppl 7.77 | wps 258890 | wpb 2835.3 | bsz 115.6 | num_updates 21489 | best_loss 2.958                                
2020-07-24 08:27:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:27:04 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint19.pt (epoch 19 @ 21489 updates, score 2.958) (writing took 2.4361540488898754 seconds)
2020-07-24 08:27:04 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)                                                                                                               
2020-07-24 08:27:04 | INFO | train | epoch 019 | loss 2.931 | ppl 7.63 | wps 95392.9 | ups 27.32 | wpb 3491.7 | bsz 141.7 | num_updates 21489 | lr 0.25 | gnorm 0.32 | clip 100 | train_wall 38 | wall 793
2020-07-24 08:27:04 | INFO | fairseq_cli.train | begin training epoch 19
epoch 020: 100%|█████████▉| 1130/1131 [00:38<00:00, 29.88it/s, loss=2.947, ppl=7.71, wps=102681, ups=29.55, wpb=3475, bsz=142.3, num_updates=22600, lr=0.25, gnorm=0.324, clip=100, train_wall=3, wall=830]2020-07-24 08:27:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:27:43 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 2.964 | ppl 7.8 | wps 258621 | wpb 2835.3 | bsz 115.6 | num_updates 22620 | best_loss 2.958                                 
2020-07-24 08:27:43 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:27:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint20.pt (epoch 20 @ 22620 updates, score 2.964) (writing took 1.0939395613968372 seconds)
2020-07-24 08:27:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)                                                                                                               
2020-07-24 08:27:44 | INFO | train | epoch 020 | loss 2.909 | ppl 7.51 | wps 98571.3 | ups 28.23 | wpb 3491.7 | bsz 141.7 | num_updates 22620 | lr 0.25 | gnorm 0.32 | clip 100 | train_wall 38 | wall 833
2020-07-24 08:27:44 | INFO | fairseq_cli.train | begin training epoch 20
epoch 021: 100%|███████▉| 1128/1131 [00:38<00:00, 30.13it/s, loss=2.801, ppl=6.97, wps=104389, ups=29.36, wpb=3555.7, bsz=162.3, num_updates=23700, lr=0.25, gnorm=0.301, clip=100, train_wall=3, wall=869]2020-07-24 08:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:28:23 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 2.912 | ppl 7.52 | wps 258015 | wpb 2835.3 | bsz 115.6 | num_updates 23751 | best_loss 2.912                                
2020-07-24 08:28:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:28:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint21.pt (epoch 21 @ 23751 updates, score 2.912) (writing took 3.3337199985980988 seconds)
2020-07-24 08:28:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)                                                                                                               
2020-07-24 08:28:26 | INFO | train | epoch 021 | loss 2.876 | ppl 7.34 | wps 93396.7 | ups 26.75 | wpb 3491.7 | bsz 141.7 | num_updates 23751 | lr 0.25 | gnorm 0.312 | clip 100 | train_wall 38 | wall 875
2020-07-24 08:28:26 | INFO | fairseq_cli.train | begin training epoch 21
epoch 022: 100%|███████▉| 1130/1131 [00:38<00:00, 29.81it/s, loss=2.928, ppl=7.61, wps=102785, ups=29.56, wpb=3476.9, bsz=130.2, num_updates=24800, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=911]2020-07-24 08:29:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:29:05 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 2.917 | ppl 7.55 | wps 258978 | wpb 2835.3 | bsz 115.6 | num_updates 24882 | best_loss 2.912                                
2020-07-24 08:29:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:29:07 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint22.pt (epoch 22 @ 24882 updates, score 2.917) (writing took 1.3919284082949162 seconds)
2020-07-24 08:29:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)                                                                                                               
2020-07-24 08:29:07 | INFO | train | epoch 022 | loss 2.856 | ppl 7.24 | wps 97864.6 | ups 28.03 | wpb 3491.7 | bsz 141.7 | num_updates 24882 | lr 0.25 | gnorm 0.311 | clip 100 | train_wall 38 | wall 916
2020-07-24 08:29:07 | INFO | fairseq_cli.train | begin training epoch 22
epoch 023: 100%|██████████▉| 1127/1131 [00:38<00:00, 29.71it/s, loss=2.829, ppl=7.11, wps=103922, ups=29.7, wpb=3499, bsz=145.7, num_updates=26000, lr=0.25, gnorm=0.305, clip=100, train_wall=3, wall=953]2020-07-24 08:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:29:46 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 2.931 | ppl 7.63 | wps 257052 | wpb 2835.3 | bsz 115.6 | num_updates 26013 | best_loss 2.912                                
2020-07-24 08:29:46 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:29:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint23.pt (epoch 23 @ 26013 updates, score 2.931) (writing took 1.5188802145421505 seconds)
2020-07-24 08:29:47 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)                                                                                                               
2020-07-24 08:29:47 | INFO | train | epoch 023 | loss 2.831 | ppl 7.12 | wps 97683.7 | ups 27.98 | wpb 3491.7 | bsz 141.7 | num_updates 26013 | lr 0.25 | gnorm 0.307 | clip 100 | train_wall 38 | wall 956
2020-07-24 08:29:47 | INFO | fairseq_cli.train | begin training epoch 23
epoch 024:  29%|██▉       | 331/1131 [00:11<00:26, 29.79it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25epoch 024:  30%|██▉       | 334/1131 [00:11<00:27, 29.46it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25epoch 024:  30%|██▉       | 337/1131 [00:11<00:27, 29.37it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25epoch 024:  30%|███       | 341/1131 [00:11<00:26, 29.46it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25epoch 024:  30%|███       | 344/1131 [00:11<00:26, 29.32it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25epoch 024:  31%|███       | 347/1131 [00:11<00:26, 29.51it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1,epoch 024:  31%|███       | 351/1131 [00:11<00:26, 29.66it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  31%|███▏      | 354/1131 [00:11<00:26, 29.52it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  32%|███▏      | 357/1131 [00:12<00:26, 29.31it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  32%|███▏      | 360/1131 [00:12<00:26, 29.35it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  32%|███▏      | 363/1131 [00:12<00:26, 29.35it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  32%|███▏      | 367/1131 [00:12<00:25, 29.59it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  33%|███▎      | 370/1131 [00:12<00:25, 29.53it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  33%|███▎      | 373/1131 [00:12<00:25, 29.50it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  33%|███▎      | 376/1131 [00:12<00:25, 29.48it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  34%|███▎      | 379/1131 [00:12<00:25, 29.51it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  34%|███▍      | 382/1131 [00:12<00:25, 29.16it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  34%|███▍      | 385/1131 [00:13<00:25, 29.36it/s, loss=2.858, ppl=7.25, wps=103446, ups=29.78, wpb=3474.2, bsz=129.1, num_updates=26300, lr=0.25, gnorm=0.31, clip=100, train_wall=3, wall=966epoch 024:  34%|███      | 389/1131 [00:13<00:24, 29.68it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  35%|███      | 392/1131 [00:13<00:24, 29.60it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  35%|███▏     | 395/1131 [00:13<00:24, 29.62it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  35%|███▏     | 399/1131 [00:13<00:24, 29.87it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  36%|███▏     | 402/1131 [00:13<00:24, 29.78it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  36%|███▏     | 405/1131 [00:13<00:24, 29.48it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  36%|███▎     | 409/1131 [00:13<00:24, 29.96it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  36%|███▎     | 412/1131 [00:13<00:24, 29.88it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  37%|███▎     | 415/1131 [00:14<00:24, 29.65it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  37%|███▎     | 418/1131 [00:14<00:23, 29.74it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  37%|███▎     | 422/1131 [00:14<00:23, 29.82it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  38%|███▍     | 426/1131 [00:14<00:23, 29.95it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  38%|███▍     | 430/1131 [00:14<00:23, 29.94it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  38%|███▍     | 433/1131 [00:14<00:23, 29.80it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  39%|███▍     | 436/1131 [00:14<00:23, 29.83it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  39%|███▌     | 440/1131 [00:14<00:23, 29.94it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  39%|███▌     | 443/1131 [00:14<00:23, 29.50it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  39%|███▌     | 446/1131 [00:15<00:23, 29.62it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  40%|███▌     | 449/1131 [00:15<00:23, 29.61it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  40%|███▌     | 453/1131 [00:15<00:22, 29.84it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  40%|███▋     | 456/1131 [00:15<00:22, 29.62it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  41%|███▋     | 460/1131 [00:15<00:22, 29.76it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  41%|███▋     | 463/1131 [00:15<00:22, 29.51it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  41%|███▋     | 466/1131 [00:15<00:22, 29.49it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  41%|███▋     | 469/1131 [00:15<00:22, 29.46it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  42%|███▊     | 473/1131 [00:15<00:22, 29.57it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  42%|███▊     | 477/1131 [00:16<00:21, 29.78it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  42%|███▊     | 480/1131 [00:16<00:21, 29.60it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  43%|███▊     | 484/1131 [00:16<00:21, 29.86it/s, loss=2.773, ppl=6.83, wps=103814, ups=29.58, wpb=3509.8, bsz=138.6, num_updates=26400, lr=0.25, gnorm=0.302, clip=100, train_wall=3, wall=969epoch 024:  43%|███▉     | 488/1131 [00:16<00:21, 30.05it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  44%|███▉     | 492/1131 [00:16<00:21, 30.11it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  44%|███▉     | 496/1131 [00:16<00:21, 29.86it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  44%|███▉     | 499/1131 [00:16<00:21, 29.65it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  44%|███▉     | 502/1131 [00:16<00:21, 29.54it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  45%|████     | 506/1131 [00:17<00:20, 29.90it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  45%|████     | 510/1131 [00:17<00:20, 30.05it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  45%|████     | 514/1131 [00:17<00:20, 29.77it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  46%|████     | 517/1131 [00:17<00:20, 29.55it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  46%|████▏    | 521/1131 [00:17<00:20, 29.56it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  46%|████▏    | 525/1131 [00:17<00:20, 29.83it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  47%|████▏    | 529/1131 [00:17<00:19, 30.12it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  47%|████▏    | 533/1131 [00:17<00:19, 30.12it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  47%|████▎    | 537/1131 [00:18<00:19, 29.91it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.81, wpb=3465.3, bsz=134.1, num_updates=26500, lr=0.25, gnorm=0.308, clip=100, train_wall=3, wall=972epoch 024:  48%|████▎    | 540/1131 [00:18<00:19, 29.81it/s, loss=2.858, ppl=7.25, wps=103289, ups=29.8epoch 024:  48%|████▎    | 543/1131 [00:18<00:19, 29.81it/epoch 024: 100%|█████████▉| 1127/1131 [00:38<00:00, 29.87it/s, loss=2.809, ppl=7.01, wps=103498, ups=29.74, wpb=3480.2, bsz=150, num_updates=27100, lr=0.25, gnorm=0.303, clip=100, train_wall=3, wall=993]2020-07-24 08:30:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:30:26 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 2.89 | ppl 7.41 | wps 258602 | wpb 2835.3 | bsz 115.6 | num_updates 27144 | best_loss 2.89                                  
2020-07-24 08:30:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:30:28 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint24.pt (epoch 24 @ 27144 updates, score 2.89) (writing took 2.2092957459390163 seconds)
2020-07-24 08:30:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)                                                                                                               
2020-07-24 08:30:28 | INFO | train | epoch 024 | loss 2.814 | ppl 7.03 | wps 96084.5 | ups 27.52 | wpb 3491.7 | bsz 141.7 | num_updates 27144 | lr 0.25 | gnorm 0.305 | clip 100 | train_wall 38 | wall 997
2020-07-24 08:30:28 | INFO | fairseq_cli.train | begin training epoch 24
epoch 025: 100%|██████▉| 1127/1131 [00:38<00:00, 30.24it/s, loss=2.784, ppl=6.89, wps=103754, ups=29.49, wpb=3518.5, bsz=162.7, num_updates=28200, lr=0.25, gnorm=0.303, clip=100, train_wall=3, wall=1033]2020-07-24 08:31:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:31:07 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 2.896 | ppl 7.45 | wps 258410 | wpb 2835.3 | bsz 115.6 | num_updates 28275 | best_loss 2.89                                 
2020-07-24 08:31:07 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:31:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint25.pt (epoch 25 @ 28275 updates, score 2.896) (writing took 1.5555965080857277 seconds)
2020-07-24 08:31:09 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)                                                                                                               
2020-07-24 08:31:09 | INFO | train | epoch 025 | loss 2.795 | ppl 6.94 | wps 97508.6 | ups 27.93 | wpb 3491.7 | bsz 141.7 | num_updates 28275 | lr 0.25 | gnorm 0.302 | clip 100 | train_wall 38 | wall 1038
2020-07-24 08:31:09 | INFO | fairseq_cli.train | begin training epoch 25
epoch 026: 100%|███████▉| 1130/1131 [00:38<00:00, 30.31it/s, loss=2.809, ppl=7.01, wps=103886, ups=29.9, wpb=3474.9, bsz=136.8, num_updates=29400, lr=0.25, gnorm=0.298, clip=100, train_wall=3, wall=1076]2020-07-24 08:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:31:48 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 2.876 | ppl 7.34 | wps 258726 | wpb 2835.3 | bsz 115.6 | num_updates 29406 | best_loss 2.876                                
2020-07-24 08:31:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:31:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint26.pt (epoch 26 @ 29406 updates, score 2.876) (writing took 3.466284401714802 seconds)
2020-07-24 08:31:51 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)                                                                                                               
2020-07-24 08:31:51 | INFO | train | epoch 026 | loss 2.774 | ppl 6.84 | wps 93183.9 | ups 26.69 | wpb 3491.7 | bsz 141.7 | num_updates 29406 | lr 0.25 | gnorm 0.297 | clip 100 | train_wall 38 | wall 1080
2020-07-24 08:31:51 | INFO | fairseq_cli.train | begin training epoch 26
epoch 027: 100%|██████▉| 1129/1131 [00:38<00:00, 30.09it/s, loss=2.783, ppl=6.88, wps=102439, ups=29.77, wpb=3441.4, bsz=142.8, num_updates=30500, lr=0.25, gnorm=0.295, clip=100, train_wall=3, wall=1117]2020-07-24 08:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:32:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 2.887 | ppl 7.4 | wps 255201 | wpb 2835.3 | bsz 115.6 | num_updates 30537 | best_loss 2.876                                 
2020-07-24 08:32:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:32:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint27.pt (epoch 27 @ 30537 updates, score 2.887) (writing took 1.4219750203192234 seconds)
2020-07-24 08:32:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)                                                                                                               
2020-07-24 08:32:32 | INFO | train | epoch 027 | loss 2.752 | ppl 6.74 | wps 97892.1 | ups 28.04 | wpb 3491.7 | bsz 141.7 | num_updates 30537 | lr 0.25 | gnorm 0.294 | clip 100 | train_wall 38 | wall 1120
2020-07-24 08:32:32 | INFO | fairseq_cli.train | begin training epoch 27
epoch 028: 100%|██████▉| 1130/1131 [00:38<00:00, 29.19it/s, loss=2.781, ppl=6.87, wps=104578, ups=29.55, wpb=3539.1, bsz=144.3, num_updates=31600, lr=0.25, gnorm=0.289, clip=100, train_wall=3, wall=1156]2020-07-24 08:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:33:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 2.85 | ppl 7.21 | wps 257440 | wpb 2835.3 | bsz 115.6 | num_updates 31668 | best_loss 2.85                                  
2020-07-24 08:33:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:33:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint28.pt (epoch 28 @ 31668 updates, score 2.85) (writing took 3.556126870214939 seconds)
2020-07-24 08:33:14 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)                                                                                                               
2020-07-24 08:33:14 | INFO | train | epoch 028 | loss 2.74 | ppl 6.68 | wps 92891.7 | ups 26.6 | wpb 3491.7 | bsz 141.7 | num_updates 31668 | lr 0.25 | gnorm 0.293 | clip 100 | train_wall 38 | wall 1163
2020-07-24 08:33:14 | INFO | fairseq_cli.train | begin training epoch 28
epoch 029: 100%|██████▉| 1129/1131 [00:38<00:00, 29.73it/s, loss=2.702, ppl=6.51, wps=105185, ups=29.52, wpb=3563.5, bsz=160.4, num_updates=32700, lr=0.25, gnorm=0.282, clip=100, train_wall=3, wall=1198]2020-07-24 08:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:33:53 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 2.841 | ppl 7.17 | wps 256702 | wpb 2835.3 | bsz 115.6 | num_updates 32799 | best_loss 2.841                                
2020-07-24 08:33:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:33:59 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint29.pt (epoch 29 @ 32799 updates, score 2.841) (writing took 5.577997658401728 seconds)
2020-07-24 08:33:59 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)                                                                                                               
2020-07-24 08:33:59 | INFO | train | epoch 029 | loss 2.726 | ppl 6.62 | wps 88693.5 | ups 25.4 | wpb 3491.7 | bsz 141.7 | num_updates 32799 | lr 0.25 | gnorm 0.292 | clip 100 | train_wall 38 | wall 1207
2020-07-24 08:33:59 | INFO | fairseq_cli.train | begin training epoch 29
epoch 030: 100%|██████▉| 1130/1131 [00:38<00:00, 30.34it/s, loss=2.751, ppl=6.73, wps=104092, ups=29.83, wpb=3488.9, bsz=148.6, num_updates=33900, lr=0.25, gnorm=0.284, clip=100, train_wall=3, wall=1244]2020-07-24 08:34:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:34:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 2.843 | ppl 7.17 | wps 258776 | wpb 2835.3 | bsz 115.6 | num_updates 33930 | best_loss 2.841                                
2020-07-24 08:34:37 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:34:41 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint30.pt (epoch 30 @ 33930 updates, score 2.843) (writing took 3.552239790558815 seconds)
2020-07-24 08:34:41 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)                                                                                                               
2020-07-24 08:34:41 | INFO | train | epoch 030 | loss 2.708 | ppl 6.53 | wps 93086.4 | ups 26.66 | wpb 3491.7 | bsz 141.7 | num_updates 33930 | lr 0.25 | gnorm 0.287 | clip 100 | train_wall 38 | wall 1250
2020-07-24 08:34:41 | INFO | fairseq_cli.train | begin training epoch 30
epoch 031: 100%|███████▉| 1127/1131 [00:37<00:00, 29.81it/s, loss=2.692, ppl=6.46, wps=103633, ups=29.8, wpb=3477.8, bsz=144.8, num_updates=35000, lr=0.25, gnorm=0.291, clip=100, train_wall=3, wall=1286]2020-07-24 08:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:35:20 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 2.829 | ppl 7.1 | wps 259270 | wpb 2835.3 | bsz 115.6 | num_updates 35061 | best_loss 2.829                                 
2020-07-24 08:35:20 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:35:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint31.pt (epoch 31 @ 35061 updates, score 2.829) (writing took 6.335302371531725 seconds)
2020-07-24 08:35:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)                                                                                                               
2020-07-24 08:35:26 | INFO | train | epoch 031 | loss 2.695 | ppl 6.48 | wps 87366.8 | ups 25.02 | wpb 3491.7 | bsz 141.7 | num_updates 35061 | lr 0.25 | gnorm 0.286 | clip 100 | train_wall 38 | wall 1295
2020-07-24 08:35:26 | INFO | fairseq_cli.train | begin training epoch 31
epoch 032: 100%|██████▉| 1129/1131 [00:38<00:00, 29.72it/s, loss=2.686, ppl=6.44, wps=104031, ups=29.68, wpb=3505.5, bsz=159.2, num_updates=36100, lr=0.25, gnorm=0.279, clip=100, train_wall=3, wall=1330]2020-07-24 08:36:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:36:05 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 2.852 | ppl 7.22 | wps 258364 | wpb 2835.3 | bsz 115.6 | num_updates 36192 | best_loss 2.829                                
2020-07-24 08:36:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:36:06 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint32.pt (epoch 32 @ 36192 updates, score 2.852) (writing took 1.2863641157746315 seconds)
2020-07-24 08:36:06 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)                                                                                                               
2020-07-24 08:36:06 | INFO | train | epoch 032 | loss 2.679 | ppl 6.41 | wps 98297.5 | ups 28.15 | wpb 3491.7 | bsz 141.7 | num_updates 36192 | lr 0.25 | gnorm 0.284 | clip 100 | train_wall 38 | wall 1335
2020-07-24 08:36:06 | INFO | fairseq_cli.train | begin training epoch 32
epoch 033: 100%|██████▉| 1128/1131 [00:38<00:00, 30.03it/s, loss=2.667, ppl=6.35, wps=104464, ups=29.54, wpb=3536.3, bsz=152.1, num_updates=37300, lr=0.25, gnorm=0.273, clip=100, train_wall=3, wall=1373]2020-07-24 08:36:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:36:45 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 2.836 | ppl 7.14 | wps 257351 | wpb 2835.3 | bsz 115.6 | num_updates 37323 | best_loss 2.829                                
2020-07-24 08:36:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:36:46 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint33.pt (epoch 33 @ 37323 updates, score 2.836) (writing took 1.0898122414946556 seconds)
2020-07-24 08:36:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)                                                                                                               
2020-07-24 08:36:46 | INFO | train | epoch 033 | loss 2.666 | ppl 6.35 | wps 98783.1 | ups 28.29 | wpb 3491.7 | bsz 141.7 | num_updates 37323 | lr 0.25 | gnorm 0.282 | clip 100 | train_wall 38 | wall 1375
2020-07-24 08:36:46 | INFO | fairseq_cli.train | begin training epoch 33
epoch 034: 100%|█████████▉| 1128/1131 [00:38<00:00, 29.67it/s, loss=2.747, ppl=6.71, wps=103507, ups=29.53, wpb=3504.7, bsz=133, num_updates=38400, lr=0.25, gnorm=0.28, clip=100, train_wall=3, wall=1411]2020-07-24 08:37:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:37:25 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 2.831 | ppl 7.12 | wps 257164 | wpb 2835.3 | bsz 115.6 | num_updates 38454 | best_loss 2.829                                
2020-07-24 08:37:25 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:37:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint34.pt (epoch 34 @ 38454 updates, score 2.831) (writing took 1.161462213844061 seconds)
2020-07-24 08:37:26 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)                                                                                                               
2020-07-24 08:37:26 | INFO | train | epoch 034 | loss 2.656 | ppl 6.3 | wps 98604.6 | ups 28.24 | wpb 3491.7 | bsz 141.7 | num_updates 38454 | lr 0.25 | gnorm 0.281 | clip 100 | train_wall 38 | wall 1415
2020-07-24 08:37:26 | INFO | fairseq_cli.train | begin training epoch 34
epoch 035: 100%|██████▉| 1129/1131 [00:38<00:00, 30.02it/s, loss=2.749, ppl=6.72, wps=102972, ups=29.69, wpb=3468.5, bsz=127.6, num_updates=39500, lr=0.25, gnorm=0.284, clip=100, train_wall=3, wall=1451]2020-07-24 08:38:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:38:05 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 2.805 | ppl 6.99 | wps 257545 | wpb 2835.3 | bsz 115.6 | num_updates 39585 | best_loss 2.805                                
2020-07-24 08:38:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:38:09 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint35.pt (epoch 35 @ 39585 updates, score 2.805) (writing took 3.9516132660210133 seconds)
2020-07-24 08:38:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)                                                                                                               
2020-07-24 08:38:09 | INFO | train | epoch 035 | loss 2.643 | ppl 6.25 | wps 92026.4 | ups 26.36 | wpb 3491.7 | bsz 141.7 | num_updates 39585 | lr 0.25 | gnorm 0.278 | clip 100 | train_wall 38 | wall 1458
2020-07-24 08:38:09 | INFO | fairseq_cli.train | begin training epoch 35
epoch 036: 100%|██████▉| 1128/1131 [00:38<00:00, 29.71it/s, loss=2.685, ppl=6.43, wps=103899, ups=29.82, wpb=3484.4, bsz=136.5, num_updates=40700, lr=0.25, gnorm=0.276, clip=100, train_wall=3, wall=1496]2020-07-24 08:38:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:38:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 2.809 | ppl 7.01 | wps 257354 | wpb 2835.3 | bsz 115.6 | num_updates 40716 | best_loss 2.805                                
2020-07-24 08:38:48 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:38:50 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint36.pt (epoch 36 @ 40716 updates, score 2.809) (writing took 1.2460883855819702 seconds)
2020-07-24 08:38:50 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)                                                                                                               
2020-07-24 08:38:50 | INFO | train | epoch 036 | loss 2.631 | ppl 6.19 | wps 98282.7 | ups 28.15 | wpb 3491.7 | bsz 141.7 | num_updates 40716 | lr 0.25 | gnorm 0.275 | clip 100 | train_wall 38 | wall 1498
2020-07-24 08:38:50 | INFO | fairseq_cli.train | begin training epoch 36
epoch 037: 100%|██████▉| 1129/1131 [00:38<00:00, 29.65it/s, loss=2.625, ppl=6.17, wps=102858, ups=29.68, wpb=3465.2, bsz=149.3, num_updates=41800, lr=0.25, gnorm=0.272, clip=100, train_wall=3, wall=1535]2020-07-24 08:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:39:29 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 2.811 | ppl 7.02 | wps 257328 | wpb 2835.3 | bsz 115.6 | num_updates 41847 | best_loss 2.805                                
2020-07-24 08:39:29 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:39:30 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint37.pt (epoch 37 @ 41847 updates, score 2.811) (writing took 1.2896011732518673 seconds)
2020-07-24 08:39:30 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)                                                                                                               
2020-07-24 08:39:30 | INFO | train | epoch 037 | loss 2.618 | ppl 6.14 | wps 98100.6 | ups 28.1 | wpb 3491.7 | bsz 141.7 | num_updates 41847 | lr 0.25 | gnorm 0.274 | clip 100 | train_wall 38 | wall 1539
2020-07-24 08:39:30 | INFO | fairseq_cli.train | begin training epoch 37
epoch 038: 100%|█████████▉| 1130/1131 [00:38<00:00, 30.09it/s, loss=2.705, ppl=6.52, wps=102865, ups=29.7, wpb=3464, bsz=136.5, num_updates=42900, lr=0.25, gnorm=0.274, clip=100, train_wall=3, wall=1574]2020-07-24 08:40:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:40:09 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 2.816 | ppl 7.04 | wps 251966 | wpb 2835.3 | bsz 115.6 | num_updates 42978 | best_loss 2.805                                
2020-07-24 08:40:09 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:40:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint38.pt (epoch 38 @ 42978 updates, score 2.816) (writing took 1.3959245197474957 seconds)
2020-07-24 08:40:10 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)                                                                                                               
2020-07-24 08:40:10 | INFO | train | epoch 038 | loss 2.614 | ppl 6.12 | wps 97895.4 | ups 28.04 | wpb 3491.7 | bsz 141.7 | num_updates 42978 | lr 0.25 | gnorm 0.273 | clip 100 | train_wall 38 | wall 1579
2020-07-24 08:40:10 | INFO | fairseq_cli.train | begin training epoch 38
epoch 039: 100%|███████▉| 1127/1131 [00:38<00:00, 30.31it/s, loss=2.63, ppl=6.19, wps=103179, ups=29.74, wpb=3469.1, bsz=139.4, num_updates=44100, lr=0.25, gnorm=0.268, clip=100, train_wall=3, wall=1617]2020-07-24 08:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:40:49 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 2.788 | ppl 6.91 | wps 258112 | wpb 2835.3 | bsz 115.6 | num_updates 44109 | best_loss 2.788                                
2020-07-24 08:40:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:40:51 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint39.pt (epoch 39 @ 44109 updates, score 2.788) (writing took 1.8783184736967087 seconds)
2020-07-24 08:40:51 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)                                                                                                               
2020-07-24 08:40:51 | INFO | train | epoch 039 | loss 2.598 | ppl 6.06 | wps 96741.8 | ups 27.71 | wpb 3491.7 | bsz 141.7 | num_updates 44109 | lr 0.25 | gnorm 0.271 | clip 100 | train_wall 38 | wall 1620
2020-07-24 08:40:51 | INFO | fairseq_cli.train | begin training epoch 39
epoch 040: 100%|█████████▉| 1130/1131 [00:38<00:00, 29.71it/s, loss=2.559, ppl=5.89, wps=105387, ups=29.32, wpb=3593.8, bsz=167, num_updates=45200, lr=0.25, gnorm=0.26, clip=100, train_wall=3, wall=1657]2020-07-24 08:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:41:30 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 2.798 | ppl 6.96 | wps 251975 | wpb 2835.3 | bsz 115.6 | num_updates 45240 | best_loss 2.788                                
2020-07-24 08:41:30 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:41:31 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint40.pt (epoch 40 @ 45240 updates, score 2.798) (writing took 1.291354663670063 seconds)
2020-07-24 08:41:31 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)                                                                                                               
2020-07-24 08:41:31 | INFO | train | epoch 040 | loss 2.588 | ppl 6.01 | wps 98001 | ups 28.07 | wpb 3491.7 | bsz 141.7 | num_updates 45240 | lr 0.25 | gnorm 0.269 | clip 100 | train_wall 38 | wall 1660
2020-07-24 08:41:31 | INFO | fairseq_cli.train | begin training epoch 40
epoch 041: 100%|███████████▉| 1128/1131 [00:38<00:00, 29.88it/s, loss=2.63, ppl=6.19, wps=105667, ups=29.74, wpb=3553, bsz=133, num_updates=46300, lr=0.25, gnorm=0.262, clip=100, train_wall=3, wall=1696]2020-07-24 08:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:42:10 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 2.8 | ppl 6.97 | wps 257270 | wpb 2835.3 | bsz 115.6 | num_updates 46371 | best_loss 2.788                                  
2020-07-24 08:42:10 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:42:12 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint41.pt (epoch 41 @ 46371 updates, score 2.8) (writing took 1.3156853467226028 seconds)
2020-07-24 08:42:12 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)                                                                                                               
2020-07-24 08:42:12 | INFO | train | epoch 041 | loss 2.579 | ppl 5.98 | wps 98049.3 | ups 28.08 | wpb 3491.7 | bsz 141.7 | num_updates 46371 | lr 0.25 | gnorm 0.268 | clip 100 | train_wall 38 | wall 1700
2020-07-24 08:42:12 | INFO | fairseq_cli.train | begin training epoch 41
epoch 042: 100%|███████▉| 1130/1131 [00:38<00:00, 29.38it/s, loss=2.581, ppl=5.98, wps=104780, ups=29.5, wpb=3551.4, bsz=142.3, num_updates=47500, lr=0.25, gnorm=0.261, clip=100, train_wall=3, wall=1738]2020-07-24 08:42:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:42:50 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 2.797 | ppl 6.95 | wps 257938 | wpb 2835.3 | bsz 115.6 | num_updates 47502 | best_loss 2.788                                
2020-07-24 08:42:50 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:42:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint42.pt (epoch 42 @ 47502 updates, score 2.797) (writing took 1.1374493800103664 seconds)
2020-07-24 08:42:52 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)                                                                                                               
2020-07-24 08:42:52 | INFO | train | epoch 042 | loss 2.569 | ppl 5.93 | wps 98547.3 | ups 28.22 | wpb 3491.7 | bsz 141.7 | num_updates 47502 | lr 0.25 | gnorm 0.265 | clip 100 | train_wall 38 | wall 1740
2020-07-24 08:42:52 | INFO | fairseq_cli.train | begin training epoch 42
epoch 043: 100%|████████▉| 1128/1131 [00:38<00:00, 29.75it/s, loss=2.55, ppl=5.86, wps=105268, ups=29.6, wpb=3556.3, bsz=150.1, num_updates=48600, lr=0.25, gnorm=0.257, clip=100, train_wall=3, wall=1777]2020-07-24 08:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:43:31 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 2.793 | ppl 6.93 | wps 256763 | wpb 2835.3 | bsz 115.6 | num_updates 48633 | best_loss 2.788                                
2020-07-24 08:43:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:43:32 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint43.pt (epoch 43 @ 48633 updates, score 2.793) (writing took 1.3938639871776104 seconds)
2020-07-24 08:43:32 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)                                                                                                               
2020-07-24 08:43:32 | INFO | train | epoch 043 | loss 2.56 | ppl 5.9 | wps 97864 | ups 28.03 | wpb 3491.7 | bsz 141.7 | num_updates 48633 | lr 0.25 | gnorm 0.265 | clip 100 | train_wall 38 | wall 1781
2020-07-24 08:43:32 | INFO | fairseq_cli.train | begin training epoch 43
epoch 044: 100%|█████████▉| 1128/1131 [00:38<00:00, 29.92it/s, loss=2.595, ppl=6.04, wps=104254, ups=29.8, wpb=3498.2, bsz=135, num_updates=49700, lr=0.25, gnorm=0.259, clip=100, train_wall=3, wall=1817]2020-07-24 08:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:44:11 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 2.785 | ppl 6.89 | wps 258124 | wpb 2835.3 | bsz 115.6 | num_updates 49764 | best_loss 2.785                                
2020-07-24 08:44:11 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:44:15 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint44.pt (epoch 44 @ 49764 updates, score 2.785) (writing took 3.9829953871667385 seconds)
2020-07-24 08:44:15 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)                                                                                                               
2020-07-24 08:44:15 | INFO | train | epoch 044 | loss 2.552 | ppl 5.87 | wps 92145.7 | ups 26.39 | wpb 3491.7 | bsz 141.7 | num_updates 49764 | lr 0.25 | gnorm 0.263 | clip 100 | train_wall 38 | wall 1824
2020-07-24 08:44:15 | INFO | fairseq_cli.train | begin training epoch 44
epoch 045: 100%|██████▉| 1129/1131 [00:38<00:00, 29.68it/s, loss=2.622, ppl=6.16, wps=102775, ups=30.06, wpb=3419.3, bsz=135.5, num_updates=50800, lr=0.25, gnorm=0.273, clip=100, train_wall=3, wall=1859]2020-07-24 08:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:44:54 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 2.782 | ppl 6.88 | wps 258450 | wpb 2835.3 | bsz 115.6 | num_updates 50895 | best_loss 2.782                                
2020-07-24 08:44:54 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:44:56 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint45.pt (epoch 45 @ 50895 updates, score 2.782) (writing took 2.4811707735061646 seconds)
2020-07-24 08:44:56 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)                                                                                                               
2020-07-24 08:44:56 | INFO | train | epoch 045 | loss 2.544 | ppl 5.83 | wps 95452.7 | ups 27.34 | wpb 3491.7 | bsz 141.7 | num_updates 50895 | lr 0.25 | gnorm 0.262 | clip 100 | train_wall 38 | wall 1865
2020-07-24 08:44:56 | INFO | fairseq_cli.train | begin training epoch 45
epoch 046: 100%|███████▉| 1128/1131 [00:38<00:00, 29.54it/s, loss=2.594, ppl=6.04, wps=104205, ups=29.4, wpb=3544.2, bsz=131.8, num_updates=52000, lr=0.25, gnorm=0.258, clip=100, train_wall=3, wall=1902]2020-07-24 08:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:45:35 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 2.776 | ppl 6.85 | wps 258342 | wpb 2835.3 | bsz 115.6 | num_updates 52026 | best_loss 2.776                                
2020-07-24 08:45:35 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:45:39 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint46.pt (epoch 46 @ 52026 updates, score 2.776) (writing took 3.8168526366353035 seconds)
2020-07-24 08:45:39 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)                                                                                                               
2020-07-24 08:45:39 | INFO | train | epoch 046 | loss 2.532 | ppl 5.78 | wps 92377.3 | ups 26.46 | wpb 3491.7 | bsz 141.7 | num_updates 52026 | lr 0.25 | gnorm 0.259 | clip 100 | train_wall 38 | wall 1908
2020-07-24 08:45:39 | INFO | fairseq_cli.train | begin training epoch 46
epoch 047: 100%|███████▉| 1129/1131 [00:38<00:00, 29.59it/s, loss=2.605, ppl=6.08, wps=103002, ups=29.97, wpb=3436.6, bsz=138.6, num_updates=53100, lr=0.25, gnorm=0.26, clip=100, train_wall=3, wall=1944]2020-07-24 08:46:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:46:18 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 2.763 | ppl 6.79 | wps 257875 | wpb 2835.3 | bsz 115.6 | num_updates 53157 | best_loss 2.763                                
2020-07-24 08:46:18 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:46:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint47.pt (epoch 47 @ 53157 updates, score 2.763) (writing took 3.513297028839588 seconds)
2020-07-24 08:46:21 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)                                                                                                               
2020-07-24 08:46:21 | INFO | train | epoch 047 | loss 2.523 | ppl 5.75 | wps 93082.2 | ups 26.66 | wpb 3491.7 | bsz 141.7 | num_updates 53157 | lr 0.25 | gnorm 0.259 | clip 100 | train_wall 38 | wall 1950
2020-07-24 08:46:21 | INFO | fairseq_cli.train | begin training epoch 47
epoch 048: 100%|███████▉| 1129/1131 [00:38<00:00, 29.80it/s, loss=2.55, ppl=5.86, wps=102063, ups=29.89, wpb=3414.3, bsz=136.6, num_updates=54200, lr=0.25, gnorm=0.265, clip=100, train_wall=3, wall=1985]2020-07-24 08:47:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:47:00 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 2.791 | ppl 6.92 | wps 257351 | wpb 2835.3 | bsz 115.6 | num_updates 54288 | best_loss 2.763                                
2020-07-24 08:47:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:47:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint48.pt (epoch 48 @ 54288 updates, score 2.791) (writing took 1.646241918206215 seconds)
2020-07-24 08:47:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)                                                                                                               
2020-07-24 08:47:02 | INFO | train | epoch 048 | loss 2.518 | ppl 5.73 | wps 97378.9 | ups 27.89 | wpb 3491.7 | bsz 141.7 | num_updates 54288 | lr 0.25 | gnorm 0.258 | clip 100 | train_wall 38 | wall 1991
2020-07-24 08:47:02 | INFO | fairseq_cli.train | begin training epoch 48
epoch 049: 100%|████████▉| 1129/1131 [00:38<00:00, 29.22it/s, loss=2.511, ppl=5.7, wps=104681, ups=29.7, wpb=3524.9, bsz=153.4, num_updates=55400, lr=0.25, gnorm=0.259, clip=100, train_wall=3, wall=2028]2020-07-24 08:47:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:47:41 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 2.776 | ppl 6.85 | wps 257530 | wpb 2835.3 | bsz 115.6 | num_updates 55419 | best_loss 2.763                                
2020-07-24 08:47:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:47:44 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint49.pt (epoch 49 @ 55419 updates, score 2.776) (writing took 3.647061701864004 seconds)
2020-07-24 08:47:44 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)                                                                                                               
2020-07-24 08:47:44 | INFO | train | epoch 049 | loss 2.512 | ppl 5.7 | wps 92794.4 | ups 26.58 | wpb 3491.7 | bsz 141.7 | num_updates 55419 | lr 0.25 | gnorm 0.257 | clip 100 | train_wall 38 | wall 2033
2020-07-24 08:47:44 | INFO | fairseq_cli.train | begin training epoch 49
epoch 050: 100%|████████▉| 1128/1131 [00:38<00:00, 29.89it/s, loss=2.555, ppl=5.88, wps=104274, ups=29.61, wpb=3521.1, bsz=135, num_updates=56500, lr=0.25, gnorm=0.251, clip=100, train_wall=3, wall=2070]2020-07-24 08:48:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:48:23 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 2.775 | ppl 6.84 | wps 257327 | wpb 2835.3 | bsz 115.6 | num_updates 56550 | best_loss 2.763                                
2020-07-24 08:48:23 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:48:25 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint50.pt (epoch 50 @ 56550 updates, score 2.775) (writing took 1.1036682948470116 seconds)
2020-07-24 08:48:25 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)                                                                                                               
2020-07-24 08:48:25 | INFO | train | epoch 050 | loss 2.506 | ppl 5.68 | wps 98643.4 | ups 28.25 | wpb 3491.7 | bsz 141.7 | num_updates 56550 | lr 0.25 | gnorm 0.257 | clip 100 | train_wall 38 | wall 2073
2020-07-24 08:48:25 | INFO | fairseq_cli.train | begin training epoch 50
epoch 051: 100%|██████▉| 1129/1131 [00:38<00:00, 29.46it/s, loss=2.507, ppl=5.68, wps=101907, ups=29.83, wpb=3416.7, bsz=143.3, num_updates=57600, lr=0.25, gnorm=0.257, clip=100, train_wall=3, wall=2109]2020-07-24 08:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:49:03 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 2.769 | ppl 6.82 | wps 257725 | wpb 2835.3 | bsz 115.6 | num_updates 57681 | best_loss 2.763                                
2020-07-24 08:49:03 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:49:05 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint51.pt (epoch 51 @ 57681 updates, score 2.769) (writing took 1.123587742447853 seconds)
2020-07-24 08:49:05 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)                                                                                                               
2020-07-24 08:49:05 | INFO | train | epoch 051 | loss 2.498 | ppl 5.65 | wps 98579.2 | ups 28.23 | wpb 3491.7 | bsz 141.7 | num_updates 57681 | lr 0.25 | gnorm 0.256 | clip 100 | train_wall 38 | wall 2113
2020-07-24 08:49:05 | INFO | fairseq_cli.train | begin training epoch 51
epoch 052: 100%|███████▉| 1130/1131 [00:38<00:00, 30.14it/s, loss=2.491, ppl=5.62, wps=105068, ups=29.57, wpb=3553.7, bsz=152.4, num_updates=58800, lr=0.25, gnorm=0.25, clip=100, train_wall=3, wall=2151]2020-07-24 08:49:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:49:44 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 2.77 | ppl 6.82 | wps 257028 | wpb 2835.3 | bsz 115.6 | num_updates 58812 | best_loss 2.763                                 
2020-07-24 08:49:44 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:49:45 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint52.pt (epoch 52 @ 58812 updates, score 2.77) (writing took 1.0777043998241425 seconds)
2020-07-24 08:49:45 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)                                                                                                               
2020-07-24 08:49:45 | INFO | train | epoch 052 | loss 2.488 | ppl 5.61 | wps 98594.8 | ups 28.24 | wpb 3491.7 | bsz 141.7 | num_updates 58812 | lr 0.25 | gnorm 0.253 | clip 100 | train_wall 38 | wall 2153
2020-07-24 08:49:45 | INFO | fairseq_cli.train | begin training epoch 52
epoch 053: 100%|███████▉| 1128/1131 [00:38<00:00, 28.68it/s, loss=2.562, ppl=5.9, wps=102030, ups=29.52, wpb=3456.1, bsz=139.8, num_updates=59900, lr=0.25, gnorm=0.261, clip=100, train_wall=3, wall=2190]2020-07-24 08:50:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:50:24 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 2.776 | ppl 6.85 | wps 257732 | wpb 2835.3 | bsz 115.6 | num_updates 59943 | best_loss 2.763                                
2020-07-24 08:50:24 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:50:26 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint53.pt (epoch 53 @ 59943 updates, score 2.776) (writing took 2.04423863068223 seconds)
2020-07-24 08:50:26 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)                                                                                                               
2020-07-24 08:50:26 | INFO | train | epoch 053 | loss 2.484 | ppl 5.6 | wps 96253.8 | ups 27.57 | wpb 3491.7 | bsz 141.7 | num_updates 59943 | lr 0.25 | gnorm 0.253 | clip 100 | train_wall 38 | wall 2194
2020-07-24 08:50:26 | INFO | fairseq_cli.train | begin training epoch 53
epoch 054: 100%|██████▉| 1128/1131 [00:38<00:00, 29.94it/s, loss=2.506, ppl=5.68, wps=101586, ups=29.54, wpb=3439.3, bsz=146.8, num_updates=61000, lr=0.25, gnorm=0.254, clip=100, train_wall=3, wall=2230]2020-07-24 08:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:51:05 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 2.771 | ppl 6.83 | wps 256272 | wpb 2835.3 | bsz 115.6 | num_updates 61074 | best_loss 2.763                                
2020-07-24 08:51:05 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:51:06 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint54.pt (epoch 54 @ 61074 updates, score 2.771) (writing took 1.5284839048981667 seconds)
2020-07-24 08:51:06 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)                                                                                                               
2020-07-24 08:51:06 | INFO | train | epoch 054 | loss 2.478 | ppl 5.57 | wps 97595.2 | ups 27.95 | wpb 3491.7 | bsz 141.7 | num_updates 61074 | lr 0.25 | gnorm 0.251 | clip 100 | train_wall 38 | wall 2235
2020-07-24 08:51:06 | INFO | fairseq_cli.train | begin training epoch 54
epoch 055: 100%|███████▉| 1129/1131 [00:38<00:00, 29.93it/s, loss=2.515, ppl=5.72, wps=104995, ups=29.8, wpb=3523.1, bsz=133.6, num_updates=62200, lr=0.25, gnorm=0.246, clip=100, train_wall=3, wall=2273]2020-07-24 08:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:51:45 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 2.772 | ppl 6.83 | wps 257923 | wpb 2835.3 | bsz 115.6 | num_updates 62205 | best_loss 2.763                                
2020-07-24 08:51:45 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:51:47 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint55.pt (epoch 55 @ 62205 updates, score 2.772) (writing took 1.5344632789492607 seconds)
2020-07-24 08:51:47 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)                                                                                                               
2020-07-24 08:51:47 | INFO | train | epoch 055 | loss 2.472 | ppl 5.55 | wps 97558 | ups 27.94 | wpb 3491.7 | bsz 141.7 | num_updates 62205 | lr 0.25 | gnorm 0.25 | clip 100 | train_wall 38 | wall 2275
2020-07-24 08:51:47 | INFO | fairseq_cli.train | begin training epoch 55
epoch 056: 100%|██████▉| 1130/1131 [00:38<00:00, 29.78it/s, loss=2.436, ppl=5.41, wps=104279, ups=29.68, wpb=3513.6, bsz=153.7, num_updates=63300, lr=0.25, gnorm=0.243, clip=100, train_wall=3, wall=2312]2020-07-24 08:52:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:52:26 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 2.759 | ppl 6.77 | wps 256557 | wpb 2835.3 | bsz 115.6 | num_updates 63336 | best_loss 2.759                                
2020-07-24 08:52:26 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:52:29 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint56.pt (epoch 56 @ 63336 updates, score 2.759) (writing took 3.3647096306085587 seconds)
2020-07-24 08:52:29 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)                                                                                                               
2020-07-24 08:52:29 | INFO | train | epoch 056 | loss 2.465 | ppl 5.52 | wps 93414.4 | ups 26.75 | wpb 3491.7 | bsz 141.7 | num_updates 63336 | lr 0.25 | gnorm 0.25 | clip 100 | train_wall 38 | wall 2318
2020-07-24 08:52:29 | INFO | fairseq_cli.train | begin training epoch 56
epoch 057: 100%|████████▉| 1128/1131 [00:38<00:00, 29.74it/s, loss=2.493, ppl=5.63, wps=103594, ups=29.68, wpb=3490.6, bsz=143, num_updates=64400, lr=0.25, gnorm=0.245, clip=100, train_wall=3, wall=2354]2020-07-24 08:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:53:08 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 2.768 | ppl 6.81 | wps 257486 | wpb 2835.3 | bsz 115.6 | num_updates 64467 | best_loss 2.759                                
2020-07-24 08:53:08 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:53:10 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint57.pt (epoch 57 @ 64467 updates, score 2.768) (writing took 1.931856133043766 seconds)
2020-07-24 08:53:10 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)                                                                                                               
2020-07-24 08:53:10 | INFO | train | epoch 057 | loss 2.459 | ppl 5.5 | wps 96623.2 | ups 27.67 | wpb 3491.7 | bsz 141.7 | num_updates 64467 | lr 0.25 | gnorm 0.248 | clip 100 | train_wall 38 | wall 2358
2020-07-24 08:53:10 | INFO | fairseq_cli.train | begin training epoch 57
epoch 058: 100%|██████▉| 1127/1131 [00:38<00:00, 30.13it/s, loss=2.514, ppl=5.71, wps=104305, ups=29.59, wpb=3525.2, bsz=131.9, num_updates=65500, lr=0.25, gnorm=0.244, clip=100, train_wall=3, wall=2393]2020-07-24 08:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:53:49 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 2.758 | ppl 6.76 | wps 257150 | wpb 2835.3 | bsz 115.6 | num_updates 65598 | best_loss 2.758                                
2020-07-24 08:53:49 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:53:52 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint58.pt (epoch 58 @ 65598 updates, score 2.758) (writing took 3.46287490054965 seconds)
2020-07-24 08:53:52 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)                                                                                                               
2020-07-24 08:53:52 | INFO | train | epoch 058 | loss 2.449 | ppl 5.46 | wps 93005.5 | ups 26.64 | wpb 3491.7 | bsz 141.7 | num_updates 65598 | lr 0.25 | gnorm 0.247 | clip 100 | train_wall 38 | wall 2401
2020-07-24 08:53:52 | INFO | fairseq_cli.train | begin training epoch 58
epoch 059: 100%|██████▉| 1129/1131 [00:38<00:00, 29.56it/s, loss=2.547, ppl=5.84, wps=102521, ups=29.65, wpb=3457.5, bsz=136.8, num_updates=66700, lr=0.25, gnorm=0.256, clip=100, train_wall=3, wall=2438]2020-07-24 08:54:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:54:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 2.778 | ppl 6.86 | wps 257906 | wpb 2835.3 | bsz 115.6 | num_updates 66729 | best_loss 2.758                                
2020-07-24 08:54:31 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:54:33 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint59.pt (epoch 59 @ 66729 updates, score 2.778) (writing took 2.3472423665225506 seconds)
2020-07-24 08:54:33 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)                                                                                                               
2020-07-24 08:54:33 | INFO | train | epoch 059 | loss 2.445 | ppl 5.44 | wps 95697.3 | ups 27.41 | wpb 3491.7 | bsz 141.7 | num_updates 66729 | lr 0.25 | gnorm 0.247 | clip 100 | train_wall 38 | wall 2442
2020-07-24 08:54:33 | INFO | fairseq_cli.train | begin training epoch 59
epoch 060: 100%|██████▉| 1130/1131 [00:38<00:00, 29.14it/s, loss=2.449, ppl=5.46, wps=103065, ups=29.57, wpb=3485.7, bsz=141.9, num_updates=67800, lr=0.25, gnorm=0.242, clip=100, train_wall=3, wall=2478]2020-07-24 08:55:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:55:13 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 2.79 | ppl 6.91 | wps 257286 | wpb 2835.3 | bsz 115.6 | num_updates 67860 | best_loss 2.758                                 
2020-07-24 08:55:13 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:55:14 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint60.pt (epoch 60 @ 67860 updates, score 2.79) (writing took 1.5831671319901943 seconds)
2020-07-24 08:55:14 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)                                                                                                               
2020-07-24 08:55:14 | INFO | train | epoch 060 | loss 2.442 | ppl 5.43 | wps 97255.4 | ups 27.85 | wpb 3491.7 | bsz 141.7 | num_updates 67860 | lr 0.25 | gnorm 0.246 | clip 100 | train_wall 38 | wall 2483
2020-07-24 08:55:14 | INFO | fairseq_cli.train | begin training epoch 60
epoch 061: 100%|██████▉| 1129/1131 [00:38<00:00, 29.59it/s, loss=2.501, ppl=5.66, wps=104080, ups=29.72, wpb=3502.4, bsz=142.7, num_updates=68900, lr=0.25, gnorm=0.249, clip=100, train_wall=3, wall=2518]2020-07-24 08:55:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:55:53 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 2.763 | ppl 6.79 | wps 257526 | wpb 2835.3 | bsz 115.6 | num_updates 68991 | best_loss 2.758                                
2020-07-24 08:55:53 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:55:55 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint61.pt (epoch 61 @ 68991 updates, score 2.763) (writing took 2.4269669465720654 seconds)
2020-07-24 08:55:55 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)                                                                                                               
2020-07-24 08:55:55 | INFO | train | epoch 061 | loss 2.433 | ppl 5.4 | wps 95412.4 | ups 27.33 | wpb 3491.7 | bsz 141.7 | num_updates 68991 | lr 0.25 | gnorm 0.245 | clip 100 | train_wall 38 | wall 2524
2020-07-24 08:55:55 | INFO | fairseq_cli.train | begin training epoch 61
epoch 062: 100%|████████▉| 1130/1131 [00:38<00:00, 29.71it/s, loss=2.442, ppl=5.43, wps=103380, ups=29.38, wpb=3519, bsz=147.2, num_updates=70100, lr=0.25, gnorm=0.245, clip=100, train_wall=3, wall=2562]2020-07-24 08:56:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:56:34 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 2.753 | ppl 6.74 | wps 256529 | wpb 2835.3 | bsz 115.6 | num_updates 70122 | best_loss 2.753                                
2020-07-24 08:56:34 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:56:38 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint62.pt (epoch 62 @ 70122 updates, score 2.753) (writing took 3.953203711658716 seconds)
2020-07-24 08:56:38 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)                                                                                                               
2020-07-24 08:56:38 | INFO | train | epoch 062 | loss 2.428 | ppl 5.38 | wps 92002.7 | ups 26.35 | wpb 3491.7 | bsz 141.7 | num_updates 70122 | lr 0.25 | gnorm 0.243 | clip 100 | train_wall 38 | wall 2567
2020-07-24 08:56:38 | INFO | fairseq_cli.train | begin training epoch 62
epoch 063: 100%|██████▉| 1127/1131 [00:37<00:00, 29.90it/s, loss=2.481, ppl=5.58, wps=104088, ups=29.73, wpb=3500.6, bsz=142.2, num_updates=71200, lr=0.25, gnorm=0.246, clip=100, train_wall=3, wall=2603]2020-07-24 08:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:57:17 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 2.726 | ppl 6.62 | wps 258274 | wpb 2835.3 | bsz 115.6 | num_updates 71253 | best_loss 2.726                                
2020-07-24 08:57:17 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:57:21 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint63.pt (epoch 63 @ 71253 updates, score 2.726) (writing took 3.3482322469353676 seconds)
2020-07-24 08:57:21 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)                                                                                                               
2020-07-24 08:57:21 | INFO | train | epoch 063 | loss 2.426 | ppl 5.37 | wps 93529.2 | ups 26.79 | wpb 3491.7 | bsz 141.7 | num_updates 71253 | lr 0.25 | gnorm 0.244 | clip 100 | train_wall 38 | wall 2609
2020-07-24 08:57:21 | INFO | fairseq_cli.train | begin training epoch 63
epoch 064: 100%|███████▉| 1129/1131 [00:38<00:00, 30.18it/s, loss=2.425, ppl=5.37, wps=104982, ups=29.6, wpb=3547.2, bsz=145.6, num_updates=72300, lr=0.25, gnorm=0.236, clip=100, train_wall=3, wall=2645]2020-07-24 08:57:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:58:00 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 2.747 | ppl 6.71 | wps 257467 | wpb 2835.3 | bsz 115.6 | num_updates 72384 | best_loss 2.726                                
2020-07-24 08:58:00 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:58:02 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint64.pt (epoch 64 @ 72384 updates, score 2.747) (writing took 2.607917584478855 seconds)
2020-07-24 08:58:02 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)                                                                                                               
2020-07-24 08:58:02 | INFO | train | epoch 064 | loss 2.418 | ppl 5.34 | wps 95147.1 | ups 27.25 | wpb 3491.7 | bsz 141.7 | num_updates 72384 | lr 0.25 | gnorm 0.243 | clip 100 | train_wall 38 | wall 2651
2020-07-24 08:58:02 | INFO | fairseq_cli.train | begin training epoch 64
epoch 065: 100%|██████▉| 1129/1131 [00:38<00:00, 29.78it/s, loss=2.524, ppl=5.75, wps=104613, ups=29.65, wpb=3528.8, bsz=129.9, num_updates=73500, lr=0.25, gnorm=0.247, clip=100, train_wall=3, wall=2689]2020-07-24 08:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:58:41 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 2.75 | ppl 6.72 | wps 249920 | wpb 2835.3 | bsz 115.6 | num_updates 73515 | best_loss 2.726                                 
2020-07-24 08:58:41 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:58:43 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint65.pt (epoch 65 @ 73515 updates, score 2.75) (writing took 1.8601607456803322 seconds)
2020-07-24 08:58:43 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)                                                                                                               
2020-07-24 08:58:43 | INFO | train | epoch 065 | loss 2.415 | ppl 5.33 | wps 96627.5 | ups 27.67 | wpb 3491.7 | bsz 141.7 | num_updates 73515 | lr 0.25 | gnorm 0.242 | clip 100 | train_wall 38 | wall 2692
2020-07-24 08:58:43 | INFO | fairseq_cli.train | begin training epoch 65
epoch 066: 100%|███████▉| 1127/1131 [00:38<00:00, 30.08it/s, loss=2.395, ppl=5.26, wps=105547, ups=29.42, wpb=3587.1, bsz=147.7, num_updates=74600, lr=0.25, gnorm=0.23, clip=100, train_wall=3, wall=2728]2020-07-24 08:59:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 08:59:22 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 2.737 | ppl 6.67 | wps 251474 | wpb 2835.3 | bsz 115.6 | num_updates 74646 | best_loss 2.726                                
2020-07-24 08:59:22 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 08:59:23 | INFO | fairseq.checkpoint_utils | saved checkpoint checkpoints/fconv/checkpoint66.pt (epoch 66 @ 74646 updates, score 2.737) (writing took 1.101801574230194 seconds)
2020-07-24 08:59:23 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)                                                                                                               
2020-07-24 08:59:23 | INFO | train | epoch 066 | loss 2.414 | ppl 5.33 | wps 98590.9 | ups 28.24 | wpb 3491.7 | bsz 141.7 | num_updates 74646 | lr 0.25 | gnorm 0.242 | clip 100 | train_wall 38 | wall 2732
2020-07-24 08:59:23 | INFO | fairseq_cli.train | begin training epoch 66
epoch 067: 100%|████████▉| 1128/1131 [00:38<00:00, 29.59it/s, loss=2.425, ppl=5.37, wps=104880, ups=29.7, wpb=3531.3, bsz=136.2, num_updates=75700, lr=0.25, gnorm=0.23, clip=100, train_wall=3, wall=2767]2020-07-24 09:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2020-07-24 09:00:02 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 2.755 | ppl 6.75 | wps 245604 | wpb 2835.3 | bsz 115.6 | num_updates 75777 | best_loss 2.726                                
2020-07-24 09:00:02 | INFO | fairseq_cli.train | begin save checkpoint
2020-07-24 09:00:02 | ERROR | fairseq.checkpoint_utils | Traceback (most recent call last):
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py", line 370, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py", line 443, in _legacy_save
    pickler.dump(obj)
OSError: [Errno 122] Disk quota exceeded

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/fairseq/fairseq/fairseq/checkpoint_utils.py", line 238, in torch_persistent_save
    return torch.save(*args, **kwargs)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py", line 370, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/serialization.py", line 229, in __exit__
    self.file_like.flush()
OSError: [Errno 122] Disk quota exceeded

Traceback (most recent call last):                                                                                                                                                                         
  File "/root/miniconda3/envs/myconda/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/mnt/fairseq/fairseq/fairseq_cli/train.py", line 349, in cli_main
    distributed_utils.call_main(args, main)
  File "/mnt/fairseq/fairseq/fairseq/distributed_utils.py", line 189, in call_main
    main(args, **kwargs)
  File "/mnt/fairseq/fairseq/fairseq_cli/train.py", line 121, in main
    valid_losses, should_stop = train(args, trainer, task, epoch_itr)
  File "/root/miniconda3/envs/myconda/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/fairseq/fairseq/fairseq_cli/train.py", line 231, in train
    valid_losses, should_stop = validate_and_save(
  File "/mnt/fairseq/fairseq/fairseq_cli/train.py", line 279, in validate_and_save
    checkpoint_utils.save_checkpoint(args, trainer, epoch_itr, valid_losses[0])
  File "/mnt/fairseq/fairseq/fairseq/checkpoint_utils.py", line 80, in save_checkpoint
    trainer.save_checkpoint(checkpoints[0], extra_state)
  File "/mnt/fairseq/fairseq/fairseq/trainer.py", line 227, in save_checkpoint
    checkpoint_utils.save_state(
  File "/mnt/fairseq/fairseq/fairseq/checkpoint_utils.py", line 284, in save_state
    torch_persistent_save(state_dict, f)
OSError: [Errno 122] Disk quota exceeded
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# ls
CODE_OF_CONDUCT.md  LICENSE    build        data-bin  examples  fairseq.egg-info  hubconf.py      scripts   tests
CONTRIBUTING.md     README.md  checkpoints  docs      fairseq   fairseq_cli       pyproject.toml  setup.py  train.py
(myconda) root@576918a23912:/mnt/fairseq/fairseq# cd checkpoints/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints# ls
fconv
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints# cd fconv/
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint1.pt   checkpoint16.pt  checkpoint22.pt  checkpoint29.pt  checkpoint35.pt  checkpoint41.pt  checkpoint48.pt  checkpoint54.pt  checkpoint60.pt  checkpoint67.pt
checkpoint10.pt  checkpoint17.pt  checkpoint23.pt  checkpoint3.pt   checkpoint36.pt  checkpoint42.pt  checkpoint49.pt  checkpoint55.pt  checkpoint61.pt  checkpoint7.pt
checkpoint11.pt  checkpoint18.pt  checkpoint24.pt  checkpoint30.pt  checkpoint37.pt  checkpoint43.pt  checkpoint5.pt   checkpoint56.pt  checkpoint62.pt  checkpoint8.pt
checkpoint12.pt  checkpoint19.pt  checkpoint25.pt  checkpoint31.pt  checkpoint38.pt  checkpoint44.pt  checkpoint50.pt  checkpoint57.pt  checkpoint63.pt  checkpoint9.pt
checkpoint13.pt  checkpoint2.pt   checkpoint26.pt  checkpoint32.pt  checkpoint39.pt  checkpoint45.pt  checkpoint51.pt  checkpoint58.pt  checkpoint64.pt  checkpoint_best.pt
checkpoint14.pt  checkpoint20.pt  checkpoint27.pt  checkpoint33.pt  checkpoint4.pt   checkpoint46.pt  checkpoint52.pt  checkpoint59.pt  checkpoint65.pt  checkpoint_last.pt
checkpoint15.pt  checkpoint21.pt  checkpoint28.pt  checkpoint34.pt  checkpoint40.pt  checkpoint47.pt  checkpoint53.pt  checkpoint6.pt   checkpoint66.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint1*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint2.pt   checkpoint25.pt  checkpoint30.pt  checkpoint36.pt  checkpoint41.pt  checkpoint47.pt  checkpoint52.pt  checkpoint58.pt  checkpoint63.pt  checkpoint8.pt
checkpoint20.pt  checkpoint26.pt  checkpoint31.pt  checkpoint37.pt  checkpoint42.pt  checkpoint48.pt  checkpoint53.pt  checkpoint59.pt  checkpoint64.pt  checkpoint9.pt
checkpoint21.pt  checkpoint27.pt  checkpoint32.pt  checkpoint38.pt  checkpoint43.pt  checkpoint49.pt  checkpoint54.pt  checkpoint6.pt   checkpoint65.pt  checkpoint_best.pt
checkpoint22.pt  checkpoint28.pt  checkpoint33.pt  checkpoint39.pt  checkpoint44.pt  checkpoint5.pt   checkpoint55.pt  checkpoint60.pt  checkpoint66.pt  checkpoint_last.pt
checkpoint23.pt  checkpoint29.pt  checkpoint34.pt  checkpoint4.pt   checkpoint45.pt  checkpoint50.pt  checkpoint56.pt  checkpoint61.pt  checkpoint67.pt
checkpoint24.pt  checkpoint3.pt   checkpoint35.pt  checkpoint40.pt  checkpoint46.pt  checkpoint51.pt  checkpoint57.pt  checkpoint62.pt  checkpoint7.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint2*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint3.pt   checkpoint34.pt  checkpoint39.pt  checkpoint43.pt  checkpoint48.pt  checkpoint52.pt  checkpoint57.pt  checkpoint61.pt  checkpoint66.pt  checkpoint_best.pt
checkpoint30.pt  checkpoint35.pt  checkpoint4.pt   checkpoint44.pt  checkpoint49.pt  checkpoint53.pt  checkpoint58.pt  checkpoint62.pt  checkpoint67.pt  checkpoint_last.pt
checkpoint31.pt  checkpoint36.pt  checkpoint40.pt  checkpoint45.pt  checkpoint5.pt   checkpoint54.pt  checkpoint59.pt  checkpoint63.pt  checkpoint7.pt
checkpoint32.pt  checkpoint37.pt  checkpoint41.pt  checkpoint46.pt  checkpoint50.pt  checkpoint55.pt  checkpoint6.pt   checkpoint64.pt  checkpoint8.pt
checkpoint33.pt  checkpoint38.pt  checkpoint42.pt  checkpoint47.pt  checkpoint51.pt  checkpoint56.pt  checkpoint60.pt  checkpoint65.pt  checkpoint9.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint3*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint4*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint5*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint6.pt   checkpoint61.pt  checkpoint63.pt  checkpoint65.pt  checkpoint67.pt  checkpoint8.pt  checkpoint_best.pt
checkpoint60.pt  checkpoint62.pt  checkpoint64.pt  checkpoint66.pt  checkpoint7.pt   checkpoint9.pt  checkpoint_last.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint6*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint7*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint8.pt  checkpoint9.pt  checkpoint_best.pt  checkpoint_last.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint8*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# rm checkpoint9*
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# ls
checkpoint_best.pt  checkpoint_last.pt
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints/fconv# cd ..
(myconda) root@576918a23912:/mnt/fairseq/fairseq/checkpoints# cd ..
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# 
(myconda) root@576918a23912:/mnt/fairseq/fairseq# fairseq-generate data-bin/iwslt14.tokenized.de-en     --path checkpoints/fconv/checkpoint_best.pt     --batch-size 128 --beam 5
2020-07-24 09:20:28 | INFO | fairseq_cli.generate | Namespace(all_gather_list_size=16384, beam=5, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', cpu=False, criterion='cross_entropy', data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, left_pad_source='True', left_pad_target='False', lenpen=1, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_sentences=128, max_source_positions=1024, max_target_positions=1024, max_tokens=None, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, momentum=0.99, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, no_seed_provided=True, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='nag', path='checkpoints/fconv/checkpoint_best.pt', prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, target_lang=None, task='translation', temperature=1.0, tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, warmup_updates=0, weight_decay=0.0)
2020-07-24 09:20:28 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2020-07-24 09:20:28 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2020-07-24 09:20:28 | INFO | fairseq.data.data_utils | loaded 6750 examples from: data-bin/iwslt14.tokenized.de-en/test.de-en.de
2020-07-24 09:20:28 | INFO | fairseq.data.data_utils | loaded 6750 examples from: data-bin/iwslt14.tokenized.de-en/test.de-en.en
2020-07-24 09:20:28 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en test de-en 6750 examples
2020-07-24 09:20:28 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/fconv/checkpoint_best.pt
  0%|                                                                         | 0/53 [00:00<?, ?it/s]S-1951	das geht schnell .
T-1951	it goes fast .
H-1951	-0.91217041015625	this is fast .
D-1951	-0.91217041015625	this is fast .
P-1951	-1.7502 -0.3787 -2.3605 -0.0627 -0.0088
S-1749	sie brauchen zeit .
T-1749	you need time .
H-1749	-0.27530214190483093	they need time .
D-1749	-0.27530214190483093	they need time .
P-1749	-0.9257 -0.2159 -0.1919 -0.0396 -0.0034
S-3406	und er sagt ...
T-3406	and he says ...
H-3406	-0.26381754875183105	and he says ...
D-3406	-0.26381754875183105	and he says ...
P-3406	-0.2876 -0.1228 -0.5719 -0.3199 -0.0169
S-3250	wir wissen das .
T-3250	we know that .
H-3250	-0.28331130743026733	we know that .
D-3250	-0.28331130743026733	we know that .
P-3250	-0.2263 -0.1330 -1.0019 -0.0526 -0.0029
S-3036	ta@@ -@@ da !
T-3036	ta@@ -@@ da !
H-3036	-0.2510853111743927	ta@@ -@@ da !
D-3036	-0.2510853111743927	ta@@ -@@ da !
P-3036	-0.6730 -0.0861 -0.1726 -0.3114 -0.0123
S-4288	ich danke ihnen .
T-4288	thank you .
H-4288	-0.031781017780303955	thank you .
D-4288	-0.031781017780303955	thank you .
P-4288	-0.0903 -0.0018 -0.0220 -0.0130
S-1990	hier ist sie .
T-1990	here it is .
H-1990	-0.4671337604522705	here it is .
D-1990	-0.4671337604522705	here it is .
P-1990	-0.4998 -1.7158 -0.0786 -0.0409 -0.0005
S-3668	das waren wir .
T-3668	that was us .
H-3668	-0.588377833366394	that &apos;s what we were .
D-3668	-0.588377833366394	that &apos;s what we were .
P-3668	-1.6667 -0.4873 -1.3584 -0.0443 -0.3509 -0.2099 -0.0012
S-4176	nach einer woche .
T-4176	after one week .
H-4176	-0.3280562162399292	after a week .
D-4176	-0.3280562162399292	after a week .
P-4176	-1.2875 -0.2040 -0.0168 -0.1315 -0.0005
S-1562	musik als sprache .
T-1562	music as language .
H-1562	-0.6274328827857971	music is a language .
D-1562	-0.6274328827857971	music is a language .
P-1562	-0.1692 -1.3158 -2.1157 -0.1134 -0.0486 -0.0019
S-1211	das ist falsch .
T-1211	that &apos;s wrong .
H-1211	-0.29995423555374146	that &apos;s wrong .
D-1211	-0.29995423555374146	that &apos;s wrong .
P-1211	-1.2884 -0.0837 -0.1190 -0.0060 -0.0027
S-705	das ist schwer .
T-705	this is hard .
H-705	-0.49795857071876526	that &apos;s hard .
D-705	-0.49795857071876526	that &apos;s hard .
P-705	-1.4070 -0.0699 -0.9558 -0.0556 -0.0015
S-49	es ist interessant .
T-49	it &apos;s interesting .
H-49	-0.04618903622031212	it &apos;s interesting .
D-49	-0.04618903622031212	it &apos;s interesting .
P-49	-0.1326 -0.0483 -0.0285 -0.0202 -0.0014
S-4450	... noch nicht .
T-4450	... yet .
H-4450	-0.9848381876945496	... not yet .
D-4450	-0.9848381876945496	... not yet .
P-4450	-1.1415 -2.2690 -1.3535 -0.1577 -0.0024
S-2151	halt zwei@@ mal .
T-2151	like twice over .
H-2151	-1.1541844606399536	wait twice .
D-2151	-1.1541844606399536	wait twice .
P-2151	-4.1340 -0.4767 -0.0052 -0.0009
S-3044	toll@@ er trick ?
T-3044	nice one .
H-3044	-0.714859664440155	great trick ?
D-3044	-0.714859664440155	great trick ?
P-3044	-2.0197 -0.7651 -0.0389 -0.0357
S-3633	ein paar weniger .
T-3633	a few less .
H-3633	-0.33307507634162903	a few less .
D-3633	-0.33307507634162903	a few less .
P-3633	-1.2156 -0.1950 -0.1937 -0.0604 -0.0007
S-5507	solar@@ technologie ist ...
T-5507	solar technology is ...
H-5507	-0.4815324544906616	solar technology is ...
D-5507	-0.4815324544906616	solar technology is ...
P-5507	-0.5912 -0.6237 -0.2134 -0.9730 -0.0064
S-5505	das ist nicht ...
T-5505	this is not ...
H-5505	-0.37869971990585327	that &apos;s not ...
D-5505	-0.37869971990585327	that &apos;s not ...
P-5505	-1.4377 -0.1392 -0.0214 -0.2856 -0.0097
S-1662	nämlich das singen .
T-1662	which is sing@@ ing .
H-1662	-0.751305341720581	that &apos;s the sing@@ ing .
D-1662	-0.751305341720581	that &apos;s the sing@@ ing .
P-1662	-2.4237 -0.3898 -1.8552 -0.4204 -0.0299 -0.1381 -0.0021
S-1530	aber es geht .
T-1530	it is possible though .
H-1530	-0.3614893853664398	but it goes .
D-1530	-0.3614893853664398	but it goes .
P-1530	-0.1637 -0.3523 -1.1169 -0.1729 -0.0015
S-1428	findet nicht statt .
T-1428	that doesn &apos;t happen .
H-1428	-1.2017347812652588	it &apos;s not happening .
D-1428	-1.2017347812652588	it &apos;s not happening .
P-1428	-3.5214 -1.4116 -0.0601 -2.0304 -0.1801 -0.0067
S-1009	ja , hallo .
T-1009	yes , hell@@ o .
H-1009	-0.3108591139316559	yes , hell@@ o .
D-1009	-0.3108591139316559	yes , hell@@ o .
P-1009	-1.0046 -0.0748 -0.7296 -0.0021 -0.0452 -0.0088
S-6040	wir lieben innovation .
T-6040	we love innovation .
H-6040	-0.0649145096540451	we love innovation .
D-6040	-0.0649145096540451	we love innovation .
P-6040	-0.1571 -0.1329 -0.0172 -0.0132 -0.0042
S-4175	von se@@ attle ...
T-4175	from se@@ attle ...
H-4175	-0.8257451057434082	from se@@ attle ...
D-4175	-0.8257451057434082	from se@@ attle ...
P-4175	-3.6252 -0.3056 -0.0633 -0.1306 -0.0040
S-4370	ich danke euch .
T-4370	thank you guys .
H-4370	-0.040470268577337265	thank you .
D-4370	-0.040470268577337265	thank you .
P-4370	-0.1434 -0.0002 -0.0123 -0.0059
S-5306	wir waren arm .
T-5306	we were poor .
H-5306	-0.16278639435768127	we were poor .
D-5306	-0.16278639435768127	we were poor .
P-5306	-0.1036 -0.2255 -0.4274 -0.0548 -0.0026
S-5131	sind das alle ?
T-5131	is that all ?
H-5131	-0.8172257542610168	are that everybody ?
D-5131	-0.8172257542610168	are that everybody ?
P-5131	-1.2833 -1.6871 -0.9592 -0.1399 -0.0166
S-4719	ich bin tom .
T-4719	i &apos;m tom .
H-4719	-0.4317271113395691	i am tom .
D-4719	-0.4317271113395691	i am tom .
P-4719	-0.1212 -1.1678 -0.8133 -0.0352 -0.0212
S-4615	1,@@ 5 millionen .
T-4615	1.@@ 5 million .
H-4615	-0.13070076704025269	1.@@ 5 million .
D-4615	-0.13070076704025269	1.@@ 5 million .
P-4615	-0.6030 -0.0070 -0.0255 -0.0179 -0.0001
S-4567	ja . okay .
T-4567	yeah . okay .
H-4567	-0.2642575204372406	yes . okay .
D-4567	-0.2642575204372406	yes . okay .
P-4567	-1.0106 -0.1534 -0.0826 -0.0225 -0.0522
S-2798	ich weiß nicht .
T-2798	i don &apos;t know .
H-2798	-0.08248241990804672	i don &apos;t know .
D-2798	-0.08248241990804672	i don &apos;t know .
P-2798	-0.1260 -0.2281 -0.0000 -0.0407 -0.0946 -0.0055
S-5495	vielen dank .
T-5495	thank you .
H-5495	-0.06351663172245026	thank you .
D-5495	-0.06351663172245026	thank you .
P-5495	-0.2476 -0.0025 -0.0032 -0.0007
S-5447	vielen dank .
T-5447	thank you .
H-5447	-0.06351663172245026	thank you .
D-5447	-0.06351663172245026	thank you .
P-5447	-0.2476 -0.0025 -0.0032 -0.0007
S-5210	wo@@ ah !
T-5210	woo@@ o !
H-5210	-0.5285102725028992	wo@@ ke !
D-5210	-0.5285102725028992	wo@@ ke !
P-5210	-1.3818 -0.6861 -0.0385 -0.0077
S-5592	ziemlich einfach .
T-5592	pretty simple .
H-5592	-0.5000660419464111	pretty simple .
D-5592	-0.5000660419464111	pretty simple .
P-5592	-0.9754 -1.0009 -0.0183 -0.0057
S-5882	vielen dank .
T-5882	thank you .
H-5882	-0.06351663172245026	thank you .
D-5882	-0.06351663172245026	thank you .
P-5882	-0.2476 -0.0025 -0.0032 -0.0007
S-5269	vielen dank .
T-5269	thank you .
H-5269	-0.06351663172245026	thank you .
D-5269	-0.06351663172245026	thank you .
P-5269	-0.2476 -0.0025 -0.0032 -0.0007
S-6749	vielen dank .
T-6749	thank you .
H-6749	-0.06351663172245026	thank you .
D-6749	-0.06351663172245026	thank you .
P-6749	-0.2476 -0.0025 -0.0032 -0.0007
S-5067	vielen dank .
T-5067	thank you .
H-5067	-0.06351663172245026	thank you .
D-5067	-0.06351663172245026	thank you .
P-5067	-0.2476 -0.0025 -0.0032 -0.0007
S-4968	vielen dank .
T-4968	thank you .
H-4968	-0.06351663172245026	thank you .
D-4968	-0.06351663172245026	thank you .
P-4968	-0.2476 -0.0025 -0.0032 -0.0007
S-4402	vielen dank .
T-4402	thank you .
H-4402	-0.06351663172245026	thank you .
D-4402	-0.06351663172245026	thank you .
P-4402	-0.2476 -0.0025 -0.0032 -0.0007
S-4110	danke schön .
T-4110	thank you .
H-4110	-0.01811940409243107	thank you .
D-4110	-0.01811940409243107	thank you .
P-4110	-0.0662 -0.0021 -0.0014 -0.0029
S-4101	vielen dank .
T-4101	thank you .
H-4101	-0.06351663172245026	thank you .
D-4101	-0.06351663172245026	thank you .
P-4101	-0.2476 -0.0025 -0.0032 -0.0007
S-4053	vielen dank .
T-4053	thank you .
H-4053	-0.06351663172245026	thank you .
D-4053	-0.06351663172245026	thank you .
P-4053	-0.2476 -0.0025 -0.0032 -0.0007
S-6329	danke schön .
T-6329	thank you .
H-6329	-0.01811940409243107	thank you .
D-6329	-0.01811940409243107	thank you .
P-6329	-0.0662 -0.0021 -0.0014 -0.0029
S-3822	vielen dank .
T-3822	thank you .
H-3822	-0.06351663172245026	thank you .
D-3822	-0.06351663172245026	thank you .
P-3822	-0.2476 -0.0025 -0.0032 -0.0007
S-1533	das geht .
T-1533	it &apos;s possible .
H-1533	-0.9315022826194763	it &apos;s going to go .
D-1533	-0.9315022826194763	it &apos;s going to go .
P-1533	-2.6131 -0.6463 -1.0967 -0.6293 -1.4419 -0.0924 -0.0009
S-1377	einen einzigen ?
T-1377	just one single one ?
H-1377	-0.8836475610733032	one one ?
D-1377	-0.8836475610733032	one one ?
P-1377	-0.7649 -2.6311 -0.1280 -0.0106
S-885	vielen dank ,
T-885	thank you very much .
H-885	-0.0755457878112793	thank you .
D-885	-0.0755457878112793	thank you .
P-885	-0.2798 -0.0024 -0.0195 -0.0005
S-6619	vielen dank .
T-6619	thank you very much .
H-6619	-0.06351663172245026	thank you .
D-6619	-0.06351663172245026	thank you .
P-6619	-0.2476 -0.0025 -0.0032 -0.0007
S-5196	vielen dank .
T-5196	thank you very much .
H-5196	-0.06351663172245026	thank you .
D-5196	-0.06351663172245026	thank you .
P-5196	-0.2476 -0.0025 -0.0032 -0.0007
S-3255	vielen dank .
T-3255	thank you so much .
H-3255	-0.06351663172245026	thank you .
D-3255	-0.06351663172245026	thank you .
P-3255	-0.2476 -0.0025 -0.0032 -0.0007
S-2791	vielen dank .
T-2791	thank you very much .
H-2791	-0.06351663172245026	thank you .
D-2791	-0.06351663172245026	thank you .
P-2791	-0.2476 -0.0025 -0.0032 -0.0007
S-1389	allein darüber .
T-1389	just because of that .
H-1389	-1.3513191938400269	just about that .
D-1389	-1.3513191938400269	just about that .
P-1389	-3.1771 -2.0320 -1.4417 -0.1036 -0.0021
S-2227	im ernst .
T-2227	no kid@@ ding .
H-2227	-0.47563156485557556	seriously .
D-2227	-0.47563156485557556	seriously .
P-2227	-1.3360 -0.0860 -0.0049
S-3093	nun ja ,
T-3093	yes , well ...
H-3093	-0.5367053151130676	well , yeah .
D-3093	-0.5367053151130676	well , yeah .
P-3093	-0.7829 -0.5062 -1.2275 -0.1659 -0.0011
S-816	vielen dank .
T-816	thank you very much .
H-816	-0.06351663172245026	thank you .
D-816	-0.06351663172245026	thank you .
P-816	-0.2476 -0.0025 -0.0032 -0.0007
S-207	vielen dank .
T-207	thank you so much .
H-207	-0.06351663172245026	thank you .
D-207	-0.06351663172245026	thank you .
P-207	-0.2476 -0.0025 -0.0032 -0.0007
S-151	vielen dank .
T-151	thank you very much .
H-151	-0.06351663172245026	thank you .
D-151	-0.06351663172245026	thank you .
P-151	-0.2476 -0.0025 -0.0032 -0.0007
S-5198	das stimmt .
T-5198	it &apos;s true .
H-5198	-0.38279128074645996	that &apos;s true .
D-5198	-0.38279128074645996	that &apos;s true .
P-5198	-1.6197 -0.1013 -0.1737 -0.0178 -0.0015
S-4717	geht das ?
T-4717	is that something ?
H-4717	-0.9345458149909973	is that ?
D-4717	-0.9345458149909973	is that ?
P-4717	-2.2412 -1.2788 -0.1946 -0.0235
S-3824	vielen dank .
T-3824	thank you .
H-3824	-0.06351663172245026	thank you .
D-3824	-0.06351663172245026	thank you .
P-3824	-0.2476 -0.0025 -0.0032 -0.0007
S-3774	vielen dank .
T-3774	thank you .
H-3774	-0.06351663172245026	thank you .
D-3774	-0.06351663172245026	thank you .
P-3774	-0.2476 -0.0025 -0.0032 -0.0007
S-3786	vielen dank .
T-3786	thank you .
H-3786	-0.06351663172245026	thank you .
D-3786	-0.06351663172245026	thank you .
P-3786	-0.2476 -0.0025 -0.0032 -0.0007
S-881	vielen dank .
T-881	thank you .
H-881	-0.06351663172245026	thank you .
D-881	-0.06351663172245026	thank you .
P-881	-0.2476 -0.0025 -0.0032 -0.0007
S-631	vielleicht nicht .
T-631	maybe not .
H-631	-0.2391693890094757	maybe not .
D-631	-0.2391693890094757	maybe not .
P-631	-0.4805 -0.3162 -0.1496 -0.0103
S-1726	besch@@ issen .
T-1726	terrible .
H-1726	-1.339903712272644	su@@ ck .
D-1726	-1.339903712272644	su@@ ck .
P-1726	-3.1401 -1.6816 -0.5220 -0.0158
S-2919	f@@ ein .
T-2919	fine .
H-2919	-1.1450952291488647	f@@ a .
D-2919	-1.1450952291488647	f@@ a .
P-2919	-2.0566 -2.4771 -0.0442 -0.0024
S-3789	vielen dank .
T-3789	thanks .
H-3789	-0.06351663172245026	thank you .
D-3789	-0.06351663172245026	thank you .
P-3789	-0.2476 -0.0025 -0.0032 -0.0007
S-4704	einzigarti@@ g .
T-4704	unique .
H-4704	-0.43941524624824524	unique .
D-4704	-0.43941524624824524	unique .
P-4704	-1.2825 -0.0351 -0.0007
S-551	vielen dank .
T-551	thank you .
H-551	-0.06351663172245026	thank you .
D-551	-0.06351663172245026	thank you .
P-551	-0.2476 -0.0025 -0.0032 -0.0007
S-1489	nachhal@@ tig .
T-1489	sustainable .
H-1489	-0.39365726709365845	sustainable .
D-1489	-0.39365726709365845	sustainable .
P-1489	-1.0950 -0.0827 -0.0033
S-723	ol@@ é !
T-723	o@@ le !
H-723	-0.4339727461338043	ol@@ é !
D-723	-0.4339727461338043	ol@@ é !
P-723	-1.6649 -0.0577 -0.0118 -0.0016
S-841	vielen dank .
T-841	thank you .
H-841	-0.06351663172245026	thank you .
D-841	-0.06351663172245026	thank you .
P-841	-0.2476 -0.0025 -0.0032 -0.0007
S-3263	6@@ 0.000 .
T-3263	6@@ 0,000 .
H-3263	-0.1663392037153244	6@@ 0,000 .
D-3263	-0.1663392037153244	6@@ 0,000 .
P-3263	-0.5903 -0.0537 -0.0206 -0.0008
S-2086	vielen dank .
T-2086	thank you .
H-2086	-0.06351663172245026	thank you .
D-2086	-0.06351663172245026	thank you .
P-2086	-0.2476 -0.0025 -0.0032 -0.0007
S-2650	vielen dank .
T-2650	thank you .
H-2650	-0.06351663172245026	thank you .
D-2650	-0.06351663172245026	thank you .
P-2650	-0.2476 -0.0025 -0.0032 -0.0007
S-1197	come on .
T-1197	come on .
H-1197	-0.5721917748451233	come on .
D-1197	-0.5721917748451233	come on .
P-1197	-2.0494 -0.1696 -0.0658 -0.0040
S-2649	vielen dank .
T-2649	thank you .
H-2649	-0.06351663172245026	thank you .
D-2649	-0.06351663172245026	thank you .
P-2649	-0.2476 -0.0025 -0.0032 -0.0007
S-1126	vielen dank .
T-1126	thank you .
H-1126	-0.06351663172245026	thank you .
D-1126	-0.06351663172245026	thank you .
P-1126	-0.2476 -0.0025 -0.0032 -0.0007
S-2051	vielen dank .
T-2051	thank you .
H-2051	-0.06351663172245026	thank you .
D-2051	-0.06351663172245026	thank you .
P-2051	-0.2476 -0.0025 -0.0032 -0.0007
S-1722	danke schön .
T-1722	thank you .
H-1722	-0.01811940409243107	thank you .
D-1722	-0.01811940409243107	thank you .
P-1722	-0.0662 -0.0021 -0.0014 -0.0029
S-1517	posi@@ tiv .
T-1517	posi@@ tively .
H-1517	-0.9546211361885071	positive .
D-1517	-0.9546211361885071	positive .
P-1517	-2.8214 -0.0399 -0.0025
S-1560	danke schön .
T-1560	thank you .
H-1560	-0.01811940409243107	thank you .
D-1560	-0.01811940409243107	thank you .
P-1560	-0.0662 -0.0021 -0.0014 -0.0029
S-3859	ja !
T-3859	yeah .
H-3859	-0.5107898712158203	yes !
D-3859	-0.5107898712158203	yes !
P-3859	-1.4412 -0.0873 -0.0039
S-1552	ja ?
T-1552	right ?
H-1552	-0.5477239489555359	yes ?
D-1552	-0.5477239489555359	yes ?
P-1552	-1.5915 -0.0413 -0.0104
S-3625	kreativität .
T-3625	creativity .
H-3625	-0.2195039838552475	creativity .
D-3625	-0.2195039838552475	creativity .
P-3625	-0.6017 -0.0540 -0.0028
S-4628	ja .
T-4628	yeah .
H-4628	-0.4983488619327545	yes .
D-4628	-0.4983488619327545	yes .
P-4628	-1.4141 -0.0772 -0.0037
S-4562	danke .
T-4562	thanks .
H-4562	-0.02872866950929165	thank you .
D-4562	-0.02872866950929165	thank you .
P-4562	-0.1105 -0.0020 -0.0016 -0.0008
S-4625	okay .
T-4625	okay .
H-4625	-0.07718168199062347	okay .
D-4625	-0.07718168199062347	okay .
P-4625	-0.1391 -0.0737 -0.0187
S-4955	wirklich .
T-4955	really .
H-4955	-0.5254464745521545	really .
D-4955	-0.5254464745521545	really .
P-4955	-1.1528 -0.3741 -0.0495
S-5504	nein .
T-5504	no .
H-5504	-0.06910767406225204	no .
D-5504	-0.06910767406225204	no .
P-5504	-0.1230 -0.0817 -0.0026
S-1546	ja ?
T-1546	right ?
H-1546	-0.5477239489555359	yes ?
D-1546	-0.5477239489555359	yes ?
P-1546	-1.5915 -0.0413 -0.0104
S-721	danke .
T-721	thank you .
H-721	-0.02872866950929165	thank you .
D-721	-0.02872866950929165	thank you .
P-721	-0.1105 -0.0020 -0.0016 -0.0008
S-1456	dabei .
T-1456	whereas .
H-1456	-1.5193297863006592	it &apos;s like this .
D-1456	-1.5193297863006592	it &apos;s like this .
P-1456	-3.3281 -0.3797 -4.1247 -1.2319 -0.0500 -0.0016
S-1464	ja ?
T-1464	right ?
H-1464	-0.5477239489555359	yes ?
D-1464	-0.5477239489555359	yes ?
P-1464	-1.5915 -0.0413 -0.0104
S-3090	ungefähr .
T-3090	sort of .
H-3090	-1.0922329425811768	about that .
D-3090	-1.0922329425811768	about that .
P-3090	-2.1948 -2.1124 -0.0542 -0.0075
S-1357	ja .
T-1357	yes .
H-1357	-0.4983488619327545	yes .
D-1357	-0.4983488619327545	yes .
P-1357	-1.4141 -0.0772 -0.0037
S-1365	ja .
T-1365	yes .
H-1365	-0.4983488619327545	yes .
D-1365	-0.4983488619327545	yes .
P-1365	-1.4141 -0.0772 -0.0037
S-1531	ja .
T-1531	yes .
H-1531	-0.4983488619327545	yes .
D-1531	-0.4983488619327545	yes .
P-1531	-1.4141 -0.0772 -0.0037
S-1514	ja ?
T-1514	right ?
H-1514	-0.5477239489555359	yes ?
D-1514	-0.5477239489555359	yes ?
P-1514	-1.5915 -0.0413 -0.0104
S-1470	ja ?
T-1470	yes ?
H-1470	-0.5477239489555359	yes ?
D-1470	-0.5477239489555359	yes ?
P-1470	-1.5915 -0.0413 -0.0104
S-1486	ja ?
T-1486	right ?
H-1486	-0.5477239489555359	yes ?
D-1486	-0.5477239489555359	yes ?
P-1486	-1.5915 -0.0413 -0.0104
S-1510	ja ?
T-1510	right ?
H-1510	-0.5477239489555359	yes ?
D-1510	-0.5477239489555359	yes ?
P-1510	-1.5915 -0.0413 -0.0104
S-5506	okay .
T-5506	okay .
H-5506	-0.07718168199062347	okay .
D-5506	-0.07718168199062347	okay .
P-5506	-0.1391 -0.0737 -0.0187
S-1347	ja ?
T-1347	right ?
H-1347	-0.5477239489555359	yes ?
D-1347	-0.5477239489555359	yes ?
P-1347	-1.5915 -0.0413 -0.0104
S-3748	danke .
T-3748	thank you .
H-3748	-0.02872866950929165	thank you .
D-3748	-0.02872866950929165	thank you .
P-3748	-0.1105 -0.0020 -0.0016 -0.0008
S-3253	danke .
T-3253	thank you .
H-3253	-0.02872866950929165	thank you .
D-3253	-0.02872866950929165	thank you .
P-3253	-0.1105 -0.0020 -0.0016 -0.0008
S-4626	fantastisch .
T-4626	a@@ wes@@ ome .
H-4626	-0.2882002592086792	fantastic .
D-4626	-0.2882002592086792	fantastic .
P-4626	-0.8265 -0.0361 -0.0020
S-5639	danke .
T-5639	thank you .
H-5639	-0.02872866950929165	thank you .
D-5639	-0.02872866950929165	thank you .
P-5639	-0.1105 -0.0020 -0.0016 -0.0008
S-5136	gut .
T-5136	all right .
H-5136	-0.5299349427223206	good .
D-5136	-0.5299349427223206	good .
P-5136	-1.5214 -0.0633 -0.0051
S-4887	danke .
T-4887	thank you .
H-4887	-0.02872866950929165	thank you .
D-4887	-0.02872866950929165	thank you .
P-4887	-0.1105 -0.0020 -0.0016 -0.0008
S-4461	danke .
T-4461	thank you .
H-4461	-0.02872866950929165	thank you .
D-4461	-0.02872866950929165	thank you .
P-4461	-0.1105 -0.0020 -0.0016 -0.0008
S-4336	danke .
T-4336	thank you .
H-4336	-0.02872866950929165	thank you .
D-4336	-0.02872866950929165	thank you .
P-4336	-0.1105 -0.0020 -0.0016 -0.0008
S-4258	danke .
T-4258	thank you .
H-4258	-0.02872866950929165	thank you .
D-4258	-0.02872866950929165	thank you .
P-4258	-0.1105 -0.0020 -0.0016 -0.0008
S-3878	danke .
T-3878	thank you .
H-3878	-0.02872866950929165	thank you .
D-3878	-0.02872866950929165	thank you .
P-3878	-0.1105 -0.0020 -0.0016 -0.0008
S-722	danke .
T-722	thank you .
H-722	-0.02872866950929165	thank you .
D-722	-0.02872866950929165	thank you .
P-722	-0.1105 -0.0020 -0.0016 -0.0008
S-737	wow !
T-737	who@@ a !
H-737	-0.03911665081977844	wow !
D-737	-0.03911665081977844	wow !
P-737	-0.0722 -0.0380 -0.0071
S-3017	danke .
T-3017	thank you .
H-3017	-0.02872866950929165	thank you .
D-3017	-0.02872866950929165	thank you .
P-3017	-0.1105 -0.0020 -0.0016 -0.0008
S-3016	danke .
T-3016	thank you .
H-3016	-0.02872866950929165	thank you .
D-3016	-0.02872866950929165	thank you .
P-3016	-0.1105 -0.0020 -0.0016 -0.0008
S-2283	danke .
T-2283	thank you .
H-2283	-0.02872866950929165	thank you .
D-2283	-0.02872866950929165	thank you .
P-2283	-0.1105 -0.0020 -0.0016 -0.0008
S-1450	ja ?
T-1450	you see ?
H-1450	-0.5477239489555359	yes ?
D-1450	-0.5477239489555359	yes ?
P-1450	-1.5915 -0.0413 -0.0104
S-1411	ja ?
T-1411	you see ?
H-1411	-0.5477239489555359	yes ?
D-1411	-0.5477239489555359	yes ?
P-1411	-1.5915 -0.0413 -0.0104
S-1379	ja ?
T-1379	you see ?
H-1379	-0.5477239489555359	yes ?
D-1379	-0.5477239489555359	yes ?
P-1379	-1.5915 -0.0413 -0.0104
S-1376	ja ?
T-1376	have you ?
H-1376	-0.5477239489555359	yes ?
D-1376	-0.5477239489555359	yes ?
P-1376	-1.5915 -0.0413 -0.0104
S-861	hallo !
T-861	hell@@ o !
H-861	-0.39362114667892456	hell@@ o !
D-861	-0.39362114667892456	hell@@ o !
P-861	-1.2499 -0.0094 -0.2900 -0.0251
S-1267	warum ?
T-1267	why ?
H-1267	-0.08431965112686157	why ?
D-1267	-0.08431965112686157	why ?
P-1267	-0.1428 -0.0986 -0.0115
  2%|█                                                      | 1/53 [00:00<00:27,  1.92it/s, wps=1012]S-6361	1.000 , sehr gut .
T-6361	1,000 , very good .
H-6361	-0.22952622175216675	1,000 , very good .
D-6361	-0.22952622175216675	1,000 , very good .
P-6361	-0.7804 -0.1400 -0.3192 -0.1267 -0.0093 -0.0015
S-6545	was ist hier passiert ?
T-6545	so what happens here ?
H-6545	-0.47709667682647705	what happened here ?
D-6545	-0.47709667682647705	what happened here ?
P-6545	-0.2133 -0.8939 -1.2292 -0.0292 -0.0198
S-5321	zentr@@ en wurden aufgebaut .
T-5321	centers were establi@@ shed .
H-5321	-1.265661358833313	central people have been built .
D-5321	-1.265661358833313	central people have been built .
P-5321	-1.5778 -4.0508 -2.3358 -0.2286 -0.6004 -0.0645 -0.0017
S-5384	ich weiß es nicht .
T-5384	i don &apos;t know .
H-5384	-0.1484214812517166	i don &apos;t know .
D-5384	-0.1484214812517166	i don &apos;t know .
P-5384	-0.1423 -0.1544 0.0000 -0.0361 -0.5461 -0.0116
S-5553	und das ist wichtig .
T-5553	and that &apos;s important .
H-5553	-0.22449010610580444	and that &apos;s important .
D-5553	-0.22449010610580444	and that &apos;s important .
P-5553	-0.0920 -0.9090 -0.2430 -0.0818 -0.0195 -0.0016
S-5783	bewer@@ te sie wieder .
T-5783	sc@@ ore them again .
H-5783	-1.5589689016342163	they re@@ gar@@ d@@ less of it .
D-5783	-1.5589689016342163	they re@@ gar@@ d@@ less of it .
P-5783	-2.9785 -3.8259 -3.9397 -0.5502 -0.3246 -0.5325 -1.6996 -0.1778 -0.0019
S-6334	hey , warum nicht ?
T-6334	hey , why not ?
H-6334	-0.11438565701246262	hey , why not ?
D-6334	-0.11438565701246262	hey , why not ?
P-6334	-0.2214 -0.0533 -0.0252 -0.3643 -0.0162 -0.0060
S-5152	aber wissen sie was ?
T-5152	but you know what ?
H-5152	-0.4289693534374237	but do you know what ?
D-5152	-0.4289693534374237	but do you know what ?
P-5152	-0.1493 -2.4321 -0.3227 -0.0618 -0.0266 -0.0077 -0.0026
S-4685	ban ist blan@@ k .
T-4685	ban is blan@@ k .
H-4685	-0.662411093711853	it &apos;s blan@@ k .
D-4685	-0.662411093711853	it &apos;s blan@@ k .
P-4685	-2.4223 -0.0718 -1.4407 -0.0005 -0.0252 -0.0139
S-6636	diese leute verdienen geld .
T-6636	these guys make money .
H-6636	-0.31461307406425476	these people deser@@ ve money .
D-6636	-0.31461307406425476	these people deser@@ ve money .
P-6636	-0.5450 -0.2428 -1.3607 -0.0210 -0.0117 -0.0204 -0.0008
S-241	diese tage sind vorbei .
T-241	and those days are gone .
H-241	-0.5279386043548584	these days are over .
D-241	-0.5279386043548584	these days are over .
P-241	-2.0751 -0.0073 -0.1927 -0.6724 -0.2184 -0.0018
S-629	können wir es ändern ?
T-629	can we do this differently ?
H-629	-0.1793118566274643	can we change it ?
D-629	-0.1793118566274643	can we change it ?
P-629	-0.3211 -0.0537 -0.2604 -0.4129 -0.0136 -0.0142
S-807	dies ist die letzte .
T-807	this is the last one .
H-807	-0.28455108404159546	this is the last one .
D-807	-0.28455108404159546	this is the last one .
P-807	-0.3516 -0.0274 -0.0846 -0.2132 -1.2908 -0.0194 -0.0048
S-863	flie@@ gende kat@@ ze .
T-863	fly away , c@@ at .
H-863	-0.4043155014514923	flying c@@ at .
D-863	-0.4043155014514923	flying c@@ at .
P-863	-0.8136 -0.8599 -0.3413 -0.0054 -0.0014
S-5220	was ist hier passiert ?
T-5220	so what happened here ?
H-5220	-0.47709667682647705	what happened here ?
D-5220	-0.47709667682647705	what happened here ?
P-5220	-0.2133 -0.8939 -1.2292 -0.0292 -0.0198
S-1046	was sind heute medien ?
T-1046	what is the media today ?
H-1046	-0.5003429651260376	what are the media today ?
D-1046	-0.5003429651260376	what are the media today ?
P-1046	-0.2202 -1.1225 -1.6123 -0.0097 -0.4344 -0.0927 -0.0106
S-3680	er öffnet die tür .
T-3680	he answers the door .
H-3680	-0.24765856564044952	he op@@ ens the door .
D-3680	-0.24765856564044952	he op@@ ens the door .
P-3680	-0.4833 -0.6166 -0.0028 -0.5592 -0.0161 -0.0497 -0.0059
S-3737	warum nicht länger leben ?
T-3737	why not live longer ?
H-3737	-0.375837117433548	why not live longer ?
D-3737	-0.375837117433548	why not live longer ?
P-3737	-0.1638 -0.9464 -0.3519 -0.7423 -0.0360 -0.0146
S-3098	ja , ich weiß .
T-3098	yeah , i know .
H-3098	-0.28582990169525146	yes , i know .
D-3098	-0.28582990169525146	yes , i know .
P-3098	-1.2102 -0.1176 -0.0640 -0.2067 -0.1124 -0.0041
S-3564	es gibt kein normal .
T-3564	there &apos;s no normal .
H-3564	-0.4253619313240051	there &apos;s no normal .
D-3564	-0.4253619313240051	there &apos;s no normal .
P-3564	-0.1909 -0.8504 -0.0487 -0.2778 -1.1786 -0.0058
S-3578	oh , also der .
T-3578	oh , that guy .
H-3578	-0.9408615231513977	oh , that &apos;s what .
D-3578	-0.9408615231513977	oh , that &apos;s what .
P-3578	-0.3336 -0.0326 -2.9361 -0.2448 -1.5151 -1.5200 -0.0039
S-3629	das ist sehr gut .
T-3629	that &apos;s very good .
H-3629	-0.367951363325119	this is very good .
D-3629	-0.367951363325119	this is very good .
P-3629	-1.6085 -0.0368 -0.4223 -0.1089 -0.0265 -0.0047
S-3631	schon ein paar mehr .
T-3631	quite a few more .
H-3631	-0.788916289806366	a few more .
D-3631	-0.788916289806366	a few more .
P-3631	-2.9060 -0.4029 -0.0826 -0.5526 -0.0005
S-1401	wir können es anders .
T-1401	we can do it differently .
H-1401	-0.39937931299209595	we can do it differently .
D-1401	-0.39937931299209595	we can do it differently .
P-1401	-0.1468 -0.0925 -1.8314 -0.1905 -0.5262 -0.0066 -0.0018
S-4636	ich war ersta@@ unt .
T-4636	i was ama@@ zed .
H-4636	-0.26189014315605164	i was ama@@ zed .
D-4636	-0.26189014315605164	i was ama@@ zed .
P-4636	-0.1405 -0.1584 -1.2172 -0.0180 -0.0324 -0.0048
S-3776	es hat sensor@@ en --
T-3776	it has sensor@@ s .
H-3776	-0.3141302168369293	it has sensor@@ s .
D-3776	-0.3141302168369293	it has sensor@@ s .
P-3776	-0.2380 -1.0830 -0.0130 -0.0293 -0.5205 -0.0010
S-4217	hier ist das video .
T-4217	here is the video .
H-4217	-0.19856590032577515	here &apos;s the video .
D-4217	-0.19856590032577515	here &apos;s the video .
P-4217	-0.7648 -0.2718 -0.0651 -0.0199 -0.0612 -0.0087
S-4411	man konnte manhattan sehen .
T-4411	you could see manhattan .
H-4411	-0.16989339888095856	you could see manhattan .
D-4411	-0.16989339888095856	you could see manhattan .
P-4411	-0.3368 -0.3481 -0.0872 -0.1631 -0.0756 -0.0086
S-4492	sie wussten also alles .
T-4492	so they know everything .
H-4492	-0.4122472107410431	so they knew everything .
D-4492	-0.4122472107410431	so they knew everything .
P-4492	-0.3421 -0.7522 -0.5881 -0.7265 -0.0626 -0.0019
S-4552	es ist gener@@ isch .
T-4552	it is gener@@ ic .
H-4552	-0.17009933292865753	it &apos;s gener@@ ic .
D-4552	-0.17009933292865753	it &apos;s gener@@ ic .
P-4552	-0.0789 -0.1310 -0.3076 -0.4925 -0.0095 -0.0011
S-1043	ja , das geht .
T-1043	yes , that &apos;s possible .
H-1043	-0.7799897193908691	yes , this is going .
D-1043	-0.7799897193908691	yes , this is going .
P-1043	-1.0903 -0.2163 -1.4181 -0.1497 -2.0322 -0.5426 -0.0109
S-2880	aber wissen sie was ?
T-2880	but you know what ?
H-2880	-0.4289693534374237	but do you know what ?
D-2880	-0.4289693534374237	but do you know what ?
P-2880	-0.1493 -2.4321 -0.3227 -0.0618 -0.0266 -0.0077 -0.0026
S-6300	was brauchen diese menschen ?
T-6300	what will these people want ?
H-6300	-0.5987564921379089	what do these people need ?
D-6300	-0.5987564921379089	what do these people need ?
P-6300	-0.1421 -2.0565 -0.8049 -0.3103 -0.8498 -0.0231 -0.0046
S-6358	irgendwie funktioniert das nicht .
T-6358	somehow this isn &apos;t working .
H-6358	-0.6194919347763062	somehow it doesn &apos;t work .
D-6358	-0.6194919347763062	somehow it doesn &apos;t work .
P-6358	-2.0047 -1.6307 -0.3515 0.0000 -0.0579 -0.2842 -0.0075
S-5205	wie funktioniert das also ?
T-5205	so how does it happen ?
H-5205	-0.2778445780277252	so how does this work ?
D-5205	-0.2778445780277252	so how does this work ?
P-5205	-0.0600 -0.4095 -0.4625 -0.8761 -0.0273 -0.1062 -0.0033
S-5630	sie benutzen ihre hände .
T-5630	they &apos;re using their hands .
H-5630	-0.2031392604112625	they use your hands .
D-5630	-0.2031392604112625	they use your hands .
P-5630	-0.4056 -0.1795 -0.5465 -0.0449 -0.0380 -0.0044
S-5978	ich finde das interessant .
T-5978	it &apos;s interesting to me .
H-5978	-0.4271286725997925	i think that &apos;s interesting .
D-5978	-0.4271286725997925	i think that &apos;s interesting .
P-5978	-0.3996 -0.8391 -1.3539 -0.3480 -0.0175 -0.0295 -0.0024
S-6032	ich dachte darüber nach .
T-6032	and i thought about that .
H-6032	-0.5024871826171875	i thought about that .
D-6032	-0.5024871826171875	i thought about that .
P-6032	-0.3970 -0.2503 -0.7482 -1.5618 -0.0527 -0.0050
S-6094	ich glaube das nicht .
T-6094	i don &apos;t believe that .
H-6094	-0.38082170486450195	i don &apos;t think so .
D-6094	-0.38082170486450195	i don &apos;t think so .
P-6094	-0.1925 -0.2550 -0.0000 -0.5178 -1.4615 -0.2284 -0.0106
S-4644	das wäre okay gewesen .
T-4644	this would have been fine .
H-4644	-0.38170790672302246	that would have been okay .
D-4644	-0.38170790672302246	that would have been okay .
P-4644	-1.1773 -0.4600 -0.3805 -0.0735 -0.5342 -0.0405 -0.0061
S-4563	ich habe eine frage .
T-4563	i &apos;ve got a question .
H-4563	-0.18910089135169983	i have a question .
D-4563	-0.18910089135169983	i have a question .
P-4563	-0.1281 -0.7261 -0.1885 -0.0344 -0.0534 -0.0041
S-871	also schauen wir mal .
T-871	so let &apos;s have a look .
H-871	-0.5577333569526672	so let &apos;s look at this .
D-871	-0.5577333569526672	so let &apos;s look at this .
P-871	-0.1005 -1.1209 -0.0150 -1.3038 -0.2695 -1.4810 -0.1701 -0.0010
S-2206	sie schle@@ mmen . &quot;
T-2206	they &apos;re fe@@ ast@@ ing . &quot;
H-2206	-1.066295862197876	they &apos;re bad . &quot;
D-2206	-1.066295862197876	they &apos;re bad . &quot;
P-2206	-0.6115 -3.1056 -2.5903 -0.0457 -0.0440 -0.0006
S-3331	also schaffen sie das .
T-3331	so , you can do it .
H-3331	-0.7867337465286255	so they create this .
D-3331	-0.7867337465286255	so they create this .
P-3331	-0.0681 -1.2052 -2.1846 -1.1566 -0.1044 -0.0015
S-3378	spieler sitzen nicht herum .
T-3378	gam@@ ers don &apos;t sit around .
H-3378	-0.27917641401290894	players don &apos;t sit around .
D-3378	-0.27917641401290894	players don &apos;t sit around .
P-3378	-0.4634 -0.6166 0.0000 -0.4425 -0.2271 -0.1942 -0.0105
S-4297	ich stand ihm gegenüber .
T-4297	i was ex@@ posed to him .
H-4297	-0.7071062922477722	i was fac@@ ed with him .
D-4297	-0.7071062922477722	i was fac@@ ed with him .
P-4297	-0.2136 -1.8736 -1.6967 -0.2282 -0.9278 -0.6951 -0.0211 -0.0008
S-5181	wie interessiert sie sind .
T-5181	how enga@@ ged you are .
H-5181	-0.7355652451515198	how interested they are .
D-5181	-0.7355652451515198	how interested they are .
P-5181	-0.9984 -1.6592 -1.2926 -0.3181 -0.1436 -0.0014
S-2743	klingt vertraut , was ?
T-2743	sound familiar ? right .
H-2743	-0.39617234468460083	sounds familiar , what ?
D-2743	-0.39617234468460083	sounds familiar , what ?
P-2743	-0.9899 -0.9215 -0.2598 -0.1702 -0.0296 -0.0060
S-2889	passiert nicht sehr oft .
T-2889	doesn &apos;t happen very much .
H-2889	-0.6831169128417969	doesn &apos;t happen very often .
D-2889	-0.6831169128417969	doesn &apos;t happen very often .
P-2889	-2.4819 0.0000 -1.6371 -0.4231 -0.2154 -0.0191 -0.0053
S-2973	warum tun wir das ?
T-2973	why do we do that ?
H-2973	-0.20002132654190063	why do we do that ?
D-2973	-0.20002132654190063	why do we do that ?
P-2973	-0.1138 -0.3893 -0.0367 -0.0860 -0.7614 -0.0029 -0.0100
S-1484	nicht schul@@ d@@ management .
T-1484	not gu@@ il@@ t management .
H-1484	-0.6898623108863831	not de@@ bt management .
D-1484	-0.6898623108863831	not de@@ bt management .
P-1484	-0.5286 -3.2598 -0.2454 -0.0193 -0.0832 -0.0028
S-2115	nennen wir ihn don .
T-2115	let &apos;s call him don .
H-2115	-0.9163247346878052	let &apos;s call it .
D-2115	-0.9163247346878052	let &apos;s call it .
P-2115	-0.9320 -0.0752 -0.3661 -0.4686 -3.5793 -0.0768
S-2352	und ich war künstler .
T-2352	and i was an artist .
H-2352	-0.17539754509925842	and i was an artist .
D-2352	-0.17539754509925842	and i was an artist .
P-2352	-0.1068 -0.1023 -0.1029 -0.8640 -0.0276 -0.0198 -0.0043
S-2635	das ist ein wunsch .
T-2635	so this is a wish .
H-2635	-0.28780677914619446	this is a wish .
D-2635	-0.28780677914619446	this is a wish .
P-2635	-1.1573 -0.0311 -0.0946 -0.4244 -0.0163 -0.0032
S-2716	b@@ g : danke .
T-2716	b@@ g : thank you .
H-2716	-0.027800651267170906	b@@ g : thank you .
D-2716	-0.027800651267170906	b@@ g : thank you .
P-2716	-0.0288 -0.0699 -0.0001 -0.0937 -0.0008 -0.0008 -0.0006
S-1435	und der muss zurück .
T-1435	and it must go back .
H-1435	-0.7165371775627136	and it has to be back .
D-1435	-0.7165371775627136	and it has to be back .
P-1435	-0.1719 -1.9160 -1.2641 -0.2860 -1.7935 -0.2342 -0.0657 -0.0011
S-4256	sie können hinein@@ gehen .
T-4256	you can enter inside it .
H-4256	-0.6927236914634705	you can go in .
D-4256	-0.6927236914634705	you can go in .
P-4256	-1.0698 -0.0775 -0.4427 -1.5469 -1.0184 -0.0010
S-2983	sie ist kein land .
T-2983	it &apos;s not a country .
H-2983	-0.284136563539505	it &apos;s not a country .
D-2983	-0.284136563539505	it &apos;s not a country .
P-2983	-1.0693 -0.1399 -0.3286 -0.2623 -0.1641 -0.0166 -0.0082
S-3005	denn wissen sie was ?
T-3005	because , you know something ?
H-3005	-0.6217170357704163	because you know what ?
D-3005	-0.6217170357704163	because you know what ?
P-3005	-1.0891 -2.4098 -0.1757 -0.0298 -0.0220 -0.0039
S-3162	woher wissen wir das ?
T-3162	how do we know that ?
H-3162	-0.3973141312599182	how do we know that ?
D-3162	-0.3973141312599182	how do we know that ?
P-3162	-1.1623 -0.4077 -0.0954 -0.1239 -0.9779 -0.0065 -0.0075
S-3763	es ist also wahr .
T-3763	so this is for real .
H-3763	-0.12510980665683746	so it &apos;s true .
D-3763	-0.12510980665683746	so it &apos;s true .
P-3763	-0.1716 -0.4378 -0.0979 -0.0205 -0.0209 -0.0018
S-4010	wohin gehen wir also ?
T-4010	so where do we go ?
H-4010	-0.37031620740890503	so where do we go ?
D-4010	-0.37031620740890503	so where do we go ?
P-4010	-0.7700 -0.2089 -1.0600 -0.0695 -0.1546 -0.3225 -0.0068
S-2903	ziemlich gut , nicht ?
T-2903	pretty good , right ?
H-2903	-0.4202946424484253	pretty good , not ?
D-2903	-0.4202946424484253	pretty good , not ?
P-2903	-0.9128 -0.2131 -0.1203 -1.1818 -0.0905 -0.0032
S-5509	okay . vielen dank .
T-5509	okay . thank you very much .
H-5509	-0.14568214118480682	okay . thank you .
D-5509	-0.14568214118480682	okay . thank you .
P-5509	-0.2825 -0.3305 -0.2425 -0.0015 -0.0152 -0.0020
S-2766	was war das ergebnis ?
T-2766	what was the result ?
H-2766	-0.2027645707130432	what was the result ?
D-2766	-0.2027645707130432	what was the result ?
P-2766	-0.2405 -0.2878 -0.2263 -0.0579 -0.3907 -0.0134
S-2142	das bin ich sogar .
T-2142	i actually am .
H-2142	-0.8820325136184692	this is me even .
D-2142	-0.8820325136184692	this is me even .
P-2142	-2.5367 -0.3559 -1.1798 -0.8899 -0.3224 -0.0075
S-2742	es ist eine krise .
T-2742	it &apos;s a crisis .
H-2742	-0.08422517776489258	it &apos;s a crisis .
D-2742	-0.08422517776489258	it &apos;s a crisis .
P-2742	-0.1579 -0.1054 -0.1343 -0.0822 -0.0239 -0.0016
S-396	danke ihnen viel@@ mals .
T-396	thank you very much .
H-396	-0.1471499651670456	thank you very much .
D-396	-0.1471499651670456	thank you very much .
P-396	-0.0799 -0.0013 -0.7720 -0.0027 -0.0264 -0.0006
S-2110	nicht sehr nachhal@@ tig .
T-2110	not very sustainable .
H-2110	-0.18473991751670837	not very sustainable .
D-2110	-0.18473991751670837	not very sustainable .
P-2110	-0.3957 -0.3652 -0.1283 -0.0303 -0.0042
S-5516	also treffen sie al .
T-5516	so meet al .
H-5516	-0.8512448668479919	so they meet al .
D-5516	-0.8512448668479919	so they meet al .
P-5516	-0.0642 -1.8095 -2.7771 -0.4127 -0.0423 -0.0017
S-3874	sondern essen@@ ti@@ ell .
T-3874	play &apos;s essential .
H-3874	-0.49476587772369385	but essential .
D-3874	-0.49476587772369385	but essential .
P-3874	-0.7695 -1.1483 -0.0596 -0.0016
S-3814	sie sind alle wichtig .
T-3814	they all matter .
H-3814	-0.29662010073661804	they &apos;re all important .
D-3814	-0.29662010073661804	they &apos;re all important .
P-3814	-0.4289 -0.9235 -0.1503 -0.2424 -0.0302 -0.0044
S-3222	sehr unterschied@@ licher begriff .
T-3222	very different notion .
H-3222	-0.49635016918182373	very different term .
D-3222	-0.49635016918182373	very different term .
P-3222	-0.3514 -0.7977 -1.1769 -0.1543 -0.0013
S-2468	unge@@ heu@@ er aufregend .
T-2468	tremend@@ ously exciting .
H-2468	-1.0532981157302856	incredibly exciting .
D-2468	-1.0532981157302856	incredibly exciting .
P-2468	-3.8896 -0.1413 -0.1816 -0.0007
S-6449	und sie tat es .
T-6449	and they did .
H-6449	-0.19589833915233612	and she did it .
D-6449	-0.19589833915233612	and she did it .
P-6449	-0.1347 -0.4100 -0.0773 -0.5217 -0.0252 -0.0066
S-887	ja , guten tag .
T-887	yes , hell@@ o .
H-887	-0.27216461300849915	yes , good day .
D-887	-0.27216461300849915	yes , good day .
P-887	-1.0539 -0.0595 -0.0437 -0.4093 -0.0625 -0.0041
S-2095	was für ein fisch .
T-2095	what a fish .
H-2095	-0.23811891674995422	what a fish .
D-2095	-0.23811891674995422	what a fish .
P-2095	-0.4155 -0.2420 -0.0330 -0.4981 -0.0020
S-1874	da passiert überhaupt nichts .
T-1874	nothing will happen .
H-1874	-0.5661464333534241	there &apos;s nothing what@@ so@@ ever .
D-1874	-0.5661464333534241	there &apos;s nothing what@@ so@@ ever .
P-1874	-0.7927 -0.6076 -0.3496 -2.2578 -0.1739 -0.0008 -0.3460 -0.0009
S-1467	das ist effizi@@ ent .
T-1467	that &apos;s efficient .
H-1467	-0.5128282904624939	this is efficient .
D-1467	-0.5128282904624939	this is efficient .
P-1467	-1.2311 -0.0195 -1.3015 -0.0079 -0.0041
S-1466	wunderbar , &apos; ne ?
T-1466	wonderful , right ?
H-1466	-0.7000365257263184	wonderful , flat ?
D-1466	-0.7000365257263184	wonderful , flat ?
P-1466	-0.6903 -0.1547 -2.5777 -0.0175 -0.0600
S-1452	völlig in@@ effizi@@ ent .
T-1452	totally in@@ efficient .
H-1452	-0.4030940532684326	completely in@@ efficient .
D-1452	-0.4030940532684326	completely in@@ efficient .
P-1452	-1.6754 -0.2640 -0.0682 -0.0064 -0.0015
S-1008	das war &apos; s .
T-1008	that was all .
H-1008	-0.45118921995162964	that &apos;s it .
D-1008	-0.45118921995162964	that &apos;s it .
P-1008	-1.1717 -0.4356 -0.5961 -0.0453 -0.0072
S-217	haben sie vielen dank .
T-217	thanks so much .
H-217	-0.41346725821495056	thank you .
D-217	-0.41346725821495056	thank you .
P-217	-0.5624 -0.0015 -1.0809 -0.0091
S-540	dies war das ergebnis .
T-540	this was the result .
H-540	-0.2596958577632904	this was the result .
D-540	-0.2596958577632904	this was the result .
P-540	-0.7418 -0.1965 -0.3440 -0.1997 -0.0731 -0.0032
S-4835	stö@@ ber@@ n sie .
T-4835	explore .
H-4835	-1.2999893426895142	hi@@ ber@@ n them .
D-4835	-1.2999893426895142	hi@@ ber@@ n them .
P-4835	-4.2233 -0.8475 -0.0804 -2.6055 -0.0426 -0.0007
S-858	gute arbeit ! ja !
T-858	good job ! yeah !
H-858	-0.29473239183425903	good work ! yes !
D-858	-0.29473239183425903	good work ! yes !
P-858	-0.1267 -0.5258 -0.0022 -1.0719 -0.0291 -0.0127
S-1966	das geht doch schon .
T-1966	that is do@@ able .
H-1966	-1.0276625156402588	that &apos;s what &apos;s going on .
D-1966	-1.0276625156402588	that &apos;s what &apos;s going on .
P-1966	-2.3610 -0.2187 -1.9498 -2.6713 -0.7781 -0.1200 -0.1216 -0.0008
S-1902	ja , guten tag .
T-1902	yes , hell@@ o .
H-1902	-0.27216461300849915	yes , good day .
D-1902	-0.27216461300849915	yes , good day .
P-1902	-1.0539 -0.0595 -0.0437 -0.4093 -0.0625 -0.0041
S-2624	das ist super wichtig .
T-2624	that &apos;s super important .
H-2624	-0.8667939305305481	this is great important .
D-2624	-0.8667939305305481	this is great important .
P-2624	-1.3681 -0.0387 -1.7575 -2.0137 -0.0195 -0.0033
S-2445	das war faszinier@@ end .
T-2445	and this was fascinating .
H-2445	-0.4781874418258667	that was fascinating .
D-2445	-0.4781874418258667	that was fascinating .
P-2445	-1.6152 -0.2662 -0.4979 -0.0090 -0.0027
S-2258	darf ich ehrlich sein ?
T-2258	can i be honest ?
H-2258	-0.42572999000549316	can i be honest ?
D-2258	-0.42572999000549316	can i be honest ?
P-2258	-1.7193 -0.1011 -0.2467 -0.1134 -0.2081 -0.1658
S-2174	die farm ist unglaublich .
T-2174	the farm &apos;s incredible .
H-2174	-0.43324586749076843	the farm is incredible .
D-2174	-0.43324586749076843	the farm is incredible .
P-2174	-1.0483 -0.0808 -0.0550 -1.4022 -0.0097 -0.0034
S-2168	was haben sie gemacht ?
T-2168	what did they do ?
H-2168	-0.26710695028305054	what did they do ?
D-2168	-0.26710695028305054	what did they do ?
P-2168	-0.1677 -0.2753 -0.7779 -0.3085 -0.0464 -0.0268
S-2088	robert gu@@ p@@ ta .
T-2088	robert gu@@ p@@ ta .
H-2088	-0.18374603986740112	robert gu@@ p@@ ta .
D-2088	-0.18374603986740112	robert gu@@ p@@ ta .
P-2088	-0.0496 -0.6403 -0.3366 -0.0587 -0.0161 -0.0012
S-2016	das ist das system .
T-2016	that &apos;s the system .
H-2016	-0.22337405383586884	this is the system .
D-2016	-0.22337405383586884	this is the system .
P-2016	-1.1690 -0.0184 -0.0814 -0.0484 -0.0176 -0.0054
S-713	mache einfach deinen job .
T-713	just do your job .
H-713	-0.7591133713722229	just make your job .
D-713	-0.7591133713722229	just make your job .
P-713	-2.2594 -1.4336 -0.5766 -0.0183 -0.2630 -0.0038
S-1063	die haben ähnliche prinzipien .
T-1063	they have similar principles .
H-1063	-0.29149317741394043	they have similar principles .
D-1063	-0.29149317741394043	they have similar principles .
P-1063	-1.0858 -0.2005 -0.3442 -0.0759 -0.0408 -0.0017
S-1654	len@@ to , langsam .
T-1654	len@@ to , slow .
H-1654	-0.5834300518035889	len@@ to , slowly .
D-1654	-0.5834300518035889	len@@ to , slowly .
P-1654	-1.1983 -0.8469 -0.1255 -1.1524 -0.1624 -0.0151
S-1593	das alles interessiert mich .
T-1593	all that interests me .
H-1593	-0.6850883960723877	it &apos;s all interested in me .
D-1593	-0.6850883960723877	it &apos;s all interested in me .
P-1593	-2.7766 -0.5404 -0.7459 -0.7026 -0.6196 -0.0899 -0.0029 -0.0028
S-1553	das können wir machen .
T-1553	we can do that .
H-1553	-0.45949751138687134	we can do that .
D-1553	-0.45949751138687134	we can do that .
P-1553	-1.0044 -0.1403 -0.4924 -1.0259 -0.0902 -0.0039
S-1488	was sagen sie dann ?
T-1488	what would you say ?
H-1488	-0.38781261444091797	what do you say ?
D-1488	-0.38781261444091797	what do you say ?
P-1488	-0.3570 -0.7564 -0.6843 -0.2258 -0.2407 -0.0627
S-1472	die ganze umwelt@@ diskussion .
T-1472	the whole environmental discussion .
H-1472	-0.7453427910804749	the whole environmental debate .
D-1472	-0.7453427910804749	the whole environmental debate .
P-1472	-1.3301 -0.3243 -1.5577 -1.1607 -0.0950 -0.0043
S-1243	ja , milli@@ sekunden .
T-1243	yes , milli@@ seconds .
H-1243	-0.20917953550815582	yes , milli@@ seconds .
D-1243	-0.20917953550815582	yes , milli@@ seconds .
P-1243	-0.9874 -0.1943 -0.0364 -0.0009 -0.0347 -0.0013
S-1237	das ist unsere gegenwart .
T-1237	that &apos;s our present .
H-1237	-0.28952983021736145	this is our present .
D-1237	-0.28952983021736145	this is our present .
P-1237	-1.3726 -0.0173 -0.0871 -0.1526 -0.0981 -0.0094
S-5311	nach hause wohin ?
T-5311	go home to where ?
H-5311	-1.4432175159454346	at home ?
D-5311	-1.4432175159454346	at home ?
P-5311	-4.5232 -0.0925 -1.0695 -0.0877
S-5261	was bedeutet das ?
T-5261	what does this mean ?
H-5261	-0.3416077494621277	what does this mean ?
D-5261	-0.3416077494621277	what does this mean ?
P-5261	-0.2833 -0.3890 -1.3487 -0.0120 -0.0153 -0.0014
S-5807	wir heben ab .
T-5807	we &apos;re taking off .
H-5807	-1.0179541110992432	we &apos;re going to li@@ ft .
D-5807	-1.0179541110992432	we &apos;re going to li@@ ft .
P-5807	-0.3650 -2.3622 -1.8558 -0.3409 -2.1915 -0.0172 -1.0105 -0.0005
S-5263	was bedeutet das ?
T-5263	what does it mean ?
H-5263	-0.3416077494621277	what does this mean ?
D-5263	-0.3416077494621277	what does this mean ?
P-5263	-0.2833 -0.3890 -1.3487 -0.0120 -0.0153 -0.0014
S-6187	oh mein gott .
T-6187	oh , my god .
H-6187	-0.27253803610801697	oh my god .
D-6187	-0.27253803610801697	oh my god .
P-6187	-0.4636 -0.7739 -0.0673 -0.0460 -0.0120
S-5209	oh mein gott !
T-5209	oh , my god !
H-5209	-0.2773992121219635	oh my god !
D-5209	-0.2773992121219635	oh my god !
P-5209	-0.4530 -0.7692 -0.0772 -0.0747 -0.0129
S-5184	wir verlieren sie .
T-5184	we &apos;re losing them .
H-5184	-0.3427006006240845	we lose them .
D-5184	-0.3427006006240845	we lose them .
P-5184	-0.1564 -0.4257 -1.0973 -0.0322 -0.0020
S-4700	ich weiß nicht .
T-4700	i don &apos;t know .
H-4700	-0.08248241990804672	i don &apos;t know .
D-4700	-0.08248241990804672	i don &apos;t know .
P-4700	-0.1260 -0.2281 -0.0000 -0.0407 -0.0946 -0.0055
S-4624	ich kann helfen .
T-4624	i can help you .
H-4624	-0.1167621910572052	i can help .
D-4624	-0.1167621910572052	i can help .
P-4624	-0.3060 -0.1112 -0.0514 -0.1112 -0.0040
S-4586	eine ganze menge .
T-4586	it &apos;s a lot .
H-4586	-0.5710412859916687	a whole lot .
D-4586	-0.5710412859916687	a whole lot .
P-4586	-1.3707 -0.7509 -0.4700 -0.2633 -0.0003
S-6042	wir lieben unterhaltung .
T-6042	we love entertain@@ ment .
H-6042	-0.09700096398591995	we love entertain@@ ment .
D-6042	-0.09700096398591995	we love entertain@@ ment .
P-6042	-0.1590 -0.1897 -0.1928 -0.0040 -0.0357 -0.0007
S-4075	klingt das gut ?
T-4075	does that sound good ?
H-4075	-0.7288435697555542	sounds good ?
D-4075	-0.7288435697555542	sounds good ?
P-4075	-1.4763 -1.2916 -0.1240 -0.0236
S-6345	drei aus 10 .
T-6345	three times out of 10 .
H-6345	-0.4670812785625458	three out of 10 .
D-6345	-0.4670812785625458	three out of 10 .
P-6345	-0.0804 -2.5823 -0.0876 -0.0203 -0.0315 -0.0004
S-2651	danke . danke !
T-2651	thank you . thank you .
H-2651	-0.08404310792684555	thank you . thank you !
D-2651	-0.08404310792684555	thank you . thank you !
P-2651	-0.0773 -0.0015 -0.0287 -0.0322 -0.0027 -0.4452 -0.0008
S-4334	auch kein krankenhaus .
T-4334	there &apos;s no hospital that can say &quot; no . &quot;
H-4334	-0.5885528326034546	no hospital .
D-4334	-0.5885528326034546	no hospital .
P-4334	-1.2164 -0.3320 -0.7872 -0.0187
S-4806	vielen dank euch .
T-4806	thank you very much , guys .
H-4806	-0.06602688878774643	thank you .
D-4806	-0.06602688878774643	thank you .
P-4806	-0.2413 -0.0002 -0.0217 -0.0009
S-6641	ste@@ phen watt .
T-6641	this is ste@@ phen watt .
H-6641	-0.1565714031457901	ste@@ phen wat@@ ts .
D-6641	-0.1565714031457901	ste@@ phen wat@@ ts .
P-6641	-0.7251 -0.0277 -0.0400 -0.1202 -0.0263 -0.0001
S-855	die sonne geht auf
T-855	the sun is ris@@ ing .
H-855	-0.5995712280273438	the sun is going on .
D-855	-0.5995712280273438	the sun is going on .
P-855	-0.4000 -0.1227 -1.2616 -1.2344 -0.5817 -0.5962 -0.0003
S-4323	das war unglaublich .
T-4323	which was a@@ wes@@ ome .
H-4323	-0.5030770897865295	that was incredible .
D-4323	-0.5030770897865295	that was incredible .
P-4323	-1.5680 -0.2116 -0.7267 -0.0051 -0.0040
S-1032	fantastisch , oder ?
T-1032	fantastic , isn &apos;t it ?
H-1032	-0.4615587890148163	fantastic , right ?
D-1032	-0.4615587890148163	fantastic , right ?
P-1032	-0.3782 -0.1323 -1.7954 -0.0003 -0.0016
S-1551	ja , sozusagen .
T-1551	yes , so to speak .
H-1551	-0.8828498125076294	yes , kind of .
D-1551	-0.8828498125076294	yes , kind of .
P-1551	-0.9295 -0.0564 -3.2282 -0.0044 -1.0079 -0.0706
S-1374	die einzige chance .
T-1374	that &apos;s your only chance .
H-1374	-0.48214441537857056	the only opportunity .
D-1374	-0.48214441537857056	the only opportunity .
P-1374	-0.4005 -0.2540 -1.6481 -0.1058 -0.0023
S-1236	lang , ja .
T-1236	a long time , yes .
H-1236	-0.4728691875934601	long , yeah .
D-1236	-0.4728691875934601	long , yeah .
P-1236	-1.2443 -0.0965 -0.9621 -0.0548 -0.0067
S-2884	es ist schrecklich .
T-2884	it &apos;s horri@@ ble .
H-2884	-0.24421179294586182	it &apos;s terrible .
D-2884	-0.24421179294586182	it &apos;s terrible .
P-2884	-0.1051 -0.1118 -0.9852 -0.0149 -0.0042
  4%|██                                                     | 2/53 [00:00<00:22,  2.24it/s, wps=1633]S-5262	o@@ h@@ h@@ h@@ h .
T-5262	o@@ h@@ h@@ h@@ h .
H-5262	-0.1269717514514923	o@@ h@@ h@@ h@@ h .
D-5262	-0.1269717514514923	o@@ h@@ h@@ h@@ h .
P-5262	-0.3688 -0.2949 -0.0180 -0.0824 -0.1087 -0.0143 -0.0016
S-5280	er ist mein groß@@ vater .
T-5280	he is my grand@@ father .
H-5280	-0.11679331958293915	he &apos;s my grand@@ father .
D-5280	-0.11679331958293915	he &apos;s my grand@@ father .
P-5280	-0.3894 -0.1980 -0.0643 -0.0213 -0.1203 -0.0192 -0.0051
S-4884	die antwort darauf ist nein .
T-4884	and the answer is no .
H-4884	-0.23160377144813538	the answer is no .
D-4884	-0.23160377144813538	the answer is no .
P-4884	-0.8747 -0.0450 -0.4076 -0.0099 -0.0422 -0.0102
S-5160	okay , ein paar mehr .
T-5160	okay , a bit more .
H-5160	-0.22556911408901215	okay , a few more .
D-5160	-0.22556911408901215	okay , a few more .
P-5160	-0.1317 -0.0561 -0.9147 -0.3948 -0.0290 -0.0521 -0.0005
S-5191	lassen sie mich zusammen@@ fassen .
T-5191	so let me rec@@ ap .
H-5191	-0.5590499639511108	let me wra@@ p up .
D-5191	-0.5590499639511108	let me wra@@ p up .
P-5191	-0.4147 -0.0270 -2.2936 -0.1052 -0.6632 -0.4081 -0.0015
S-5247	es gab re@@ mix@@ e .
T-5247	there were re@@ mix@@ es .
H-5247	-0.562419056892395	there was re@@ mix@@ ing .
D-5247	-0.562419056892395	there was re@@ mix@@ ing .
P-5247	-0.3670 -0.6694 -0.6912 -0.3637 -0.9422 -0.9018 -0.0017
S-5249	und dann wurde es international .
T-5249	and then it went international .
H-5249	-0.15236012637615204	and then it became international .
D-5249	-0.15236012637615204	and then it became international .
P-5249	-0.1003 -0.0685 -0.2042 -0.5321 -0.1145 -0.0311 -0.0158
S-4810	mein name ist am@@ it .
T-4810	my name is am@@ it .
H-4810	-0.09033806622028351	my name is am@@ it .
D-4810	-0.09033806622028351	my name is am@@ it .
P-4810	-0.1276 -0.0011 -0.0813 -0.0315 -0.3591 -0.0175 -0.0143
S-4808	das ist eine großartige frage .
T-4808	that is a fantastic question .
H-4808	-0.24952639639377594	this is a great question .
D-4808	-0.24952639639377594	this is a great question .
P-4808	-1.0173 -0.0309 -0.1550 -0.4882 -0.0257 -0.0251 -0.0045
S-5388	ist es überhaupt fotogra@@ fie ?
T-5388	or is it photograph@@ y ?
H-5388	-0.6868124604225159	is it ever photograph@@ y ?
D-5388	-0.6868124604225159	is it ever photograph@@ y ?
P-5388	-0.9794 -0.3905 -2.0190 -1.0220 -0.0236 -0.3657 -0.0075
S-5431	hier ist ein anderes beispiel .
T-5431	so here &apos;s another example .
H-5431	-0.19715678691864014	here &apos;s another example .
D-5431	-0.19715678691864014	here &apos;s another example .
P-5431	-0.8195 -0.1749 -0.0904 -0.0367 -0.0603 -0.0012
S-5461	es gibt aber noch etwas .
T-5461	but there &apos;s another thing .
H-5461	-0.7401381731033325	there &apos;s something else .
D-5461	-0.7401381731033325	there &apos;s something else .
P-5461	-1.0484 -1.1946 -0.5789 -0.3263 -1.2883 -0.0044
S-5616	habt ihr diese jungs gesehen ?
T-5616	have you seen these guys ?
H-5616	-0.4790385365486145	did you see these guys ?
D-5616	-0.4790385365486145	did you see these guys ?
P-5616	-2.1304 -0.1174 -0.4994 -0.4419 -0.0819 -0.0681 -0.0141
S-5763	es hat milliarden von denen .
T-5763	there are billions of them .
H-5763	-0.43381237983703613	it has billions of these .
D-5763	-0.43381237983703613	it has billions of these .
P-5763	-0.1468 -0.5207 -0.0947 -0.0138 -2.1696 -0.0893 -0.0018
S-4834	nur weiter . viel spaß .
T-4834	keep going . have fun .
H-4834	-0.819023072719574	only further . much fun .
D-4834	-0.819023072719574	only further . much fun .
P-4834	-1.9767 -1.0950 -0.6089 -1.9268 -0.0336 -0.0406 -0.0517
S-5927	ich werde das nie vergessen .
T-5927	i will never forget it .
H-5927	-0.33327487111091614	i &apos;ll never forget that .
D-5927	-0.33327487111091614	i &apos;ll never forget that .
P-5927	-0.1483 -1.5365 -0.0414 -0.0506 -0.4908 -0.0572 -0.0081
S-4149	hier sehen sie die stadt .
T-4149	here you see the city .
H-4149	-0.45874595642089844	here you see the city .
D-4149	-0.45874595642089844	here you see the city .
P-4149	-1.4548 -0.9611 -0.3062 -0.1462 -0.2339 -0.1006 -0.0084
S-4194	es hat ihm sehr gefallen .
T-4194	he liked it very much .
H-4194	-1.003698468208313	it &apos;s really enjo@@ ying him .
D-4194	-1.003698468208313	it &apos;s really enjo@@ ying him .
P-4194	-0.8495 -1.4417 -3.2384 -1.2631 -1.0463 -0.0855 -0.1037 -0.0014
S-3566	ich denke , eher nicht .
T-3566	i don &apos;t think so .
H-3566	-0.6726145148277283	i think rather not .
D-3566	-0.6726145148277283	i think rather not .
P-3566	-0.1439 -0.1809 -1.9816 -1.4342 -0.2775 -0.0175
S-3649	einige sind gekommen und gegangen .
T-3649	some have come and gone .
H-3649	-0.8140187859535217	some of them came and went .
D-3649	-0.8140187859535217	some of them came and went .
P-3649	-0.3688 -1.7593 -0.8276 -1.1395 -0.9572 -0.9616 -0.4962 -0.0018
S-4059	o@@ oh , danke schön .
T-4059	aw@@ w , thank you .
H-4059	-0.047839678823947906	o@@ oh , thank you .
D-4059	-0.047839678823947906	o@@ oh , thank you .
P-4059	-0.2651 -0.0162 -0.0185 -0.0255 -0.0013 -0.0051 -0.0032
S-4064	werde ich die farbe mögen ?
T-4064	will i like the color ?
H-4064	-0.5644997954368591	am i going to like the color ?
D-4064	-0.5644997954368591	am i going to like the color ?
P-4064	-2.8860 -0.0181 -0.3035 -0.0076 -0.4600 -1.0659 -0.3040 -0.0220 -0.0133
S-4111	guten tag , alle zusammen .
T-4111	good after@@ noon , everybody .
H-4111	-0.3461855947971344	good after@@ noon , all together .
D-4111	-0.3461855947971344	good after@@ noon , all together .
P-4111	-0.1399 -0.8377 -0.0008 -0.2653 -1.2643 -0.2295 -0.0306 -0.0015
S-5977	wir haben den kontakt verloren .
T-5977	we &apos;ve been dis@@ connected .
H-5977	-0.4797907769680023	we have lost touch .
D-5977	-0.4797907769680023	we have lost touch .
P-5977	-0.1985 -1.4160 -0.2265 -0.6501 -0.3837 -0.0039
S-4691	es hält dich fr@@ isch .
T-4691	it kee@@ ps you fresh .
H-4691	-0.6432688236236572	it kee@@ ps you fresh .
D-4691	-0.6432688236236572	it kee@@ ps you fresh .
P-4691	-0.1962 -1.4625 -0.0009 -0.2113 -2.6101 -0.0203 -0.0016
S-4383	das tat sie hei@@ mlich ,
T-4383	she did it in secret .
H-4383	-1.1396253108978271	that &apos;s what they did .
D-4383	-1.1396253108978271	that &apos;s what they did .
P-4383	-3.3836 -0.9117 -0.9533 -1.0323 -0.6633 -1.0239 -0.0093
S-4516	und es wurde noch besser .
T-4516	and it even got better .
H-4516	-0.6695685386657715	and it &apos;s been better .
D-4516	-0.6695685386657715	and it &apos;s been better .
P-4516	-0.0930 -0.2502 -1.5211 -0.9767 -1.3664 -0.4772 -0.0023
S-4519	eine fast identi@@ sche struktur .
T-4519	an almost iden@@ tical structure .
H-4519	-0.3395744860172272	almost iden@@ tical structure .
D-4519	-0.3395744860172272	almost iden@@ tical structure .
P-4519	-1.2343 -0.7857 -0.0032 -0.0068 -0.0058 -0.0016
S-4565	ist das auch deine meinung ?
T-4565	is that your opini@@ on ?
H-4565	-0.9360611438751221	is that even your opini@@ on ?
D-4565	-0.9360611438751221	is that even your opini@@ on ?
P-4565	-1.2609 -1.1300 -3.7253 -1.1967 -0.0967 -0.0245 -0.0400 -0.0143
S-4686	das ist eine großartige frage .
T-4686	that &apos;s a great question .
H-4686	-0.24952639639377594	this is a great question .
D-4686	-0.24952639639377594	this is a great question .
P-4686	-1.0173 -0.0309 -0.1550 -0.4882 -0.0257 -0.0251 -0.0045
S-5920	ich werde es nie vergessen .
T-5920	i never will forget it .
H-5920	-0.3313862085342407	i &apos;ll never forget it .
D-5920	-0.3313862085342407	i &apos;ll never forget it .
P-5920	-0.1291 -1.4757 -0.0353 -0.0399 -0.5954 -0.0365 -0.0079
S-3147	es gibt eine alte studie .
T-3147	this is an old study .
H-3147	-0.23632892966270447	there &apos;s an old study .
D-3147	-0.23632892966270447	there &apos;s an old study .
P-3147	-0.2261 -0.5567 -0.1746 -0.0268 -0.5699 -0.0975 -0.0027
S-2194	aber was essen deine fische ?
T-2194	but what are your fish eating ?
H-2194	-0.44975733757019043	but what does your fish eat ?
D-2194	-0.44975733757019043	but what does your fish eat ?
P-2194	-0.2878 -0.1377 -1.9475 -0.6592 -0.1044 -0.4198 -0.0374 -0.0043
S-2259	ich mag diese frage nicht .
T-2259	i don &apos;t love that question .
H-2259	-0.2771775424480438	i don &apos;t like this question .
D-2259	-0.2771775424480438	i don &apos;t like this question .
P-2259	-0.2447 -0.6463 0.0000 -0.1267 -1.1411 -0.0173 -0.0380 -0.0034
S-1037	sie hatten keine massen@@ medien .
T-1037	they didn &apos;t have mass media .
H-1037	-0.2515459656715393	they had no mass media .
D-1037	-0.2515459656715393	they had no mass media .
P-1037	-0.1212 -1.3157 -0.1226 -0.1134 -0.0032 -0.0802 -0.0045
S-1394	aber wo sind wir nützlich ?
T-1394	but where are we of use ?
H-1394	-0.17186708748340607	but where are we useful ?
D-1394	-0.17186708748340607	but where are we useful ?
P-1394	-0.1969 -0.0869 -0.6400 -0.1755 -0.0916 -0.0042 -0.0079
S-1561	die arbeit des interpre@@ ten .
T-1561	the work of the interpre@@ ter .
H-1561	-0.5810925364494324	the work of the interpre@@ ter .
D-1561	-0.5810925364494324	the work of the interpre@@ ter .
P-1561	-0.9100 -1.4064 -0.1488 -0.7562 -0.3602 -1.0222 -0.0441 -0.0008
S-2035	das ist vor@@ bil@@ dlich .
T-2035	that &apos;s ex@@ em@@ pl@@ ary .
H-2035	-1.1464574337005615	this is pre@@ cise .
D-2035	-1.1464574337005615	this is pre@@ cise .
P-2035	-1.2184 -0.0355 -3.5425 -2.0306 -0.0500 -0.0017
S-2130	nennen wir ihn ebenfalls don .
T-2130	let &apos;s call him don too .
H-2130	-1.0165163278579712	let &apos;s do it as well .
D-2130	-1.0165163278579712	let &apos;s do it as well .
P-2130	-1.4847 -0.0616 -3.2897 -0.6106 -2.3571 -0.1880 -0.1285 -0.0119
S-860	... seine eigene geschichte erfinden .
T-860	inv@@ enting his own narra@@ tive .
H-860	-0.8782151341438293	... his own story is inv@@ enting .
D-860	-0.8782151341438293	... his own story is inv@@ enting .
P-860	-1.9292 -1.2954 -0.2660 -0.4403 -2.4686 -0.6834 -0.1547 -0.6655 -0.0009
S-712	sei nicht ent@@ mu@@ tigt .
T-712	don &apos;t be da@@ un@@ ted .
H-712	-0.9818721413612366	don &apos;t dis@@ coura@@ ge .
D-712	-0.9818721413612366	don &apos;t dis@@ coura@@ ge .
P-712	-2.6963 0.0000 -2.8316 -1.0569 -0.2229 -0.0602 -0.0053
S-2521	zuerst haben wir die bevölkerung .
T-2521	first , we &apos;ve got population .
H-2521	-0.36243417859077454	first , we have the population .
D-2521	-0.36243417859077454	first , we have the population .
P-2521	-0.1827 -1.3056 -0.0973 -0.3450 -0.2674 -0.1641 -0.5326 -0.0048
S-2567	wer wird es sicher@@ stellen ?
T-2567	who &apos;s going to be sure ?
H-2567	-0.6285547614097595	who will make it safe ?
D-2567	-0.6285547614097595	who will make it safe ?
P-2567	-0.1196 -1.0527 -0.5296 -0.4574 -2.2331 -0.0003 -0.0073
S-2771	was glauben sie passierte nun ?
T-2771	what do you think happened then ?
H-2771	-0.9357751607894897	what do you think that happened ?
D-2771	-0.9357751607894897	what do you think that happened ?
P-2771	-0.3501 -1.5513 -0.4780 -0.6943 -2.9102 -1.4398 -0.0604 -0.0021
S-2900	er heißt paul off@@ it .
T-2900	his name is paul off@@ it .
H-2900	-0.3094208538532257	he &apos;s called paul off@@ it .
D-2900	-0.3094208538532257	he &apos;s called paul off@@ it .
P-2900	-0.5196 -0.2814 -0.0225 -0.0007 -0.2531 -1.3562 -0.0393 -0.0025
S-3271	das gilt nicht für emotionen .
T-3271	that does not hold for emotions .
H-3271	-0.434767484664917	it &apos;s not true for emotions .
D-3271	-0.434767484664917	it &apos;s not true for emotions .
P-3271	-1.6506 -0.3220 -0.1078 -0.7661 -0.4963 -0.1249 -0.0045 -0.0059
S-1010	wie funktionieren wir menschen eigentlich ?
T-1010	how do we humans actually function ?
H-1010	-0.6112445592880249	how do we make people work ?
D-1010	-0.6112445592880249	how do we make people work ?
P-1010	-0.2313 -0.5497 -0.0871 -3.4821 -0.2646 -0.2181 -0.0512 -0.0060
S-3107	ja , hier ist es .
T-3107	yes , there it is .
H-3107	-0.3649776577949524	yes , here it is .
D-3107	-0.3649776577949524	yes , here it is .
P-3107	-0.9739 -0.0863 -0.6090 -0.8012 -0.0618 -0.0207 -0.0019
S-6451	und dann passierte es wieder .
T-6451	and then it happened again .
H-6451	-0.1251843124628067	and then it happened again .
D-6451	-0.1251843124628067	and then it happened again .
P-6451	-0.1255 -0.0371 -0.4103 -0.1825 -0.0938 -0.0254 -0.0016
S-6454	er zeigte immer hier hin .
T-6454	he kept poin@@ ting here .
H-6454	-0.6558620929718018	he always showed up here .
D-6454	-0.6558620929718018	he always showed up here .
P-6454	-0.2232 -1.1711 -0.0663 -2.1717 -0.9093 -0.0400 -0.0095
S-6005	es ist nicht unser kampf .
T-6005	it &apos;s not our struggle .
H-6005	-0.21682864427566528	it &apos;s not our fight .
D-6005	-0.21682864427566528	it &apos;s not our fight .
P-6005	-0.0959 -0.1904 -0.0146 -0.5446 -0.6293 -0.0366 -0.0064
S-6034	es wäre uner@@ trä@@ glich .
T-6034	i couldn &apos;t bear it .
H-6034	-0.35613393783569336	it would be in@@ toler@@ able .
D-6034	-0.35613393783569336	it would be in@@ toler@@ able .
P-6034	-0.1231 -0.2662 -0.0543 -2.0536 -0.2656 -0.0803 -0.0053 -0.0008
S-6186	krise . tod . katastrophe .
T-6186	crisis . death . disaster .
H-6186	-0.19877617061138153	crisis . death . disaster .
D-6186	-0.19877617061138153	crisis . death . disaster .
P-6186	-0.2200 -0.0937 -0.2512 -0.0445 -0.7705 -0.0087 -0.0028
S-6211	und viele menschen vergessen dies .
T-6211	and many people forget this .
H-6211	-0.5129100680351257	and many people forget this .
D-6211	-0.5129100680351257	and many people forget this .
P-6211	-0.1702 -0.7855 -0.0433 -1.4059 -1.0979 -0.0862 -0.0012
S-6390	ich fühlte mich richtig gut .
T-6390	and i felt really good .
H-6390	-0.33633938431739807	i felt really good .
D-6390	-0.33633938431739807	i felt really good .
P-6390	-0.1527 -0.2518 -1.1067 -0.4075 -0.0914 -0.0079
S-6003	es ist nicht unser problem .
T-6003	it &apos;s not our problem .
H-6003	-0.16818350553512573	it &apos;s not our problem .
D-6003	-0.16818350553512573	it &apos;s not our problem .
P-6003	-0.1030 -0.1738 -0.0158 -0.5402 -0.3038 -0.0364 -0.0043
S-94	also , was bedeutet das ?
T-94	well , what does that mean ?
H-94	-0.349958211183548	so , what does this mean ?
D-94	-0.349958211183548	so , what does this mean ?
P-94	-0.1533 -1.1558 -0.0567 -0.2037 -1.2170 -0.0054 -0.0061 -0.0016
S-6558	es waren immer noch fische .
T-6558	there were still fis@@ hes .
H-6558	-0.7339407801628113	it was still fish .
D-6558	-0.7339407801628113	it was still fish .
P-6558	-1.3564 -1.2891 -0.3138 -1.2647 -0.1760 -0.0036
S-6632	wir haben drei haupt@@ gruppen .
T-6632	we have three main groups .
H-6632	-0.18587256968021393	we have three main groups .
D-6632	-0.18587256968021393	we have three main groups .
P-6632	-0.1310 -0.1434 -0.0168 -0.9431 -0.0315 -0.0305 -0.0049
Traceback (most recent call last):                                                                   
  File "/root/miniconda3/envs/myconda/bin/fairseq-generate", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-generate')())
  File "/mnt/fairseq/fairseq/fairseq_cli/generate.py", line 274, in cli_main
    main(args)
  File "/mnt/fairseq/fairseq/fairseq_cli/generate.py", line 38, in main
    return _main(args, sys.stdout)
  File "/mnt/fairseq/fairseq/fairseq_cli/generate.py", line 169, in _main
    src_str = src_dict.string(src_tokens, args.remove_bpe)
  File "/mnt/fairseq/fairseq/fairseq/data/dictionary.py", line 98, in string
    sent = " ".join(
  File "/mnt/fairseq/fairseq/fairseq/data/dictionary.py", line 99, in <genexpr>
    token_string(i)
  File "/mnt/fairseq/fairseq/fairseq/data/dictionary.py", line 87, in token_string
    if i == self.unk():
KeyboardInterrupt

(myconda) root@576918a23912:/mnt/fairseq/fairseq# 


