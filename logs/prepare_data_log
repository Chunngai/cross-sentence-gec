neko@neko:~/Crosentgec/crosentgec$ sh prepare_data_modified.sh ../datasets/lang-8-20111007-L1-v2.dat ../datasets/release3.3.tar.gz 
+ '[' 2 -ne 2 ']'
+ LANG8V2=../datasets/lang-8-20111007-L1-v2.dat
+ NUCLE_TAR=../datasets/release3.3.tar.gz
+ mkdir -p data/tmp
+ tmp_dir=data/tmp
++ date
+ echo '[Wed 22 Jul 2020 11:25:16 AM CST] Preparing Lang-8 data... (NOTE:Can take several hours, due to LangID.py filtering...)'
[Wed 22 Jul 2020 11:25:16 AM CST] Preparing Lang-8 data... (NOTE:Can take several hours, due to LangID.py filtering...)
+ python3 scripts/lang8_preprocess.py --dataset ../datasets/lang-8-20111007-L1-v2.dat --language English --id en --output data/tmp/lang-8-20111007-L1-v2.xml
177847 essays have been added.
1318368 sentence pairs can be used.
+ python3 scripts/partition_data_into_train_and_dev.py --dataset data/tmp/lang-8-20111007-L1-v2.xml --train data/tmp/lang8-train.xml --dev data/tmp/lang8-dev.xml --limit 0
train data contains 177847 essays
development data contains 0 essays
+ python2 scripts/sentence_pairs_with_ctx.py --train --tokenize --maxtokens 80 --mintokens 1 --input data/tmp/lang8-train.xml --src-ctx data/tmp/lang8.src-trg.ctx --src-src data/tmp/lang8.src-trg.src --trg-trg data/tmp/lang8.src-trg.trg
('data/tmp/lang8-train.xml', ':', 177847, 'essays,', 1960174, 'source sentences,', 22531428, 'tokens.')
('The number of source sentences / essays :', 11)
('The number of tokens / essays :', 126)
(1289824, 'sentence pairs have been added with tokenization.')
('data/tmp/lang8.src-trg.ctx', 27587357, 'tokens')
('data/tmp/lang8.src-trg.src', 17656998, 'tokens')
('data/tmp/lang8.src-trg.trg', 21167581, 'tokens')
++ date
+ echo '[Wed 22 Jul 2020 01:51:53 PM CST] Preparing NUCLE data...'
[Wed 22 Jul 2020 01:51:53 PM CST] Preparing NUCLE data...
+ tar -zxvf ../datasets/release3.3.tar.gz -C data/tmp/
release3.3/
release3.3/bea2019/
release3.3/bea2019/readme.txt
release3.3/bea2019/nucle.train.gold.bea19.m2
release3.3/README
release3.3/data/
release3.3/data/conll14st-preprocessed.conll.ann
release3.3/data/conll14st-preprocessed.m2
release3.3/data/nucle3.2.sgml
release3.3/CoNLL-preproc-README
release3.3/scripts/
release3.3/scripts/nucle_doc.py
release3.3/scripts/README
release3.3/scripts/iparser.py
release3.3/scripts/nuclesgmlparser.py
release3.3/scripts/preprocess.py
release3.3/scripts/parser_feature.py
+ nucle_dir=data/tmp/release3.3
+ python2 data/tmp/release3.3/scripts/preprocess.py -l data/tmp/release3.3/data/nucle3.2.sgml data/tmp/nucle3.2-preprocessed.conll data/tmp/nucle3.2-preprocessed.conll.ann data/tmp/nucle3.2-preprocessed.conll.m2
Traceback (most recent call last):
  File "data/tmp/release3.3/scripts/preprocess.py", line 379, in <module>
    ppr = PreProcessor()
  File "data/tmp/release3.3/scripts/preprocess.py", line 36, in __init__
    self.sentenceTokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
  File "/usr/local/lib/python2.7/dist-packages/nltk-2.0b7-py2.7.egg/nltk/data.py", line 593, in load
    resource_val = pickle.load(_open(resource_url))
  File "/usr/local/lib/python2.7/dist-packages/nltk-2.0b7-py2.7.egg/nltk/data.py", line 672, in _open
    return find(path).open()
  File "/usr/local/lib/python2.7/dist-packages/nltk-2.0b7-py2.7.egg/nltk/data.py", line 454, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource 'tokenizers/punkt/english.pickle' not found.  Please
  use the NLTK Downloader to obtain the resource: >>>
  nltk.download().
  Searched in:
    - '/home/neko/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ 
neko@neko:~/Crosentgec/crosentgec$ vim prepare_data_modified.sh 
neko@neko:~/Crosentgec/crosentgec$ cp prepare_data_modified.sh prepare_data_modified_modified.sh 
neko@neko:~/Crosentgec/crosentgec$ vim prepare_data_modified_modified.sh 
neko@neko:~/Crosentgec/crosentgec$ sh prepare_data_modified_modified.sh ../datasets/lang-8-20111007-L1-v2.dat ../datasets/release3.3.tar.gz 
+ '[' 2 -ne 2 ']'
+ LANG8V2=../datasets/lang-8-20111007-L1-v2.dat
+ NUCLE_TAR=../datasets/release3.3.tar.gz
+ mkdir -p data/tmp
+ tmp_dir=data/tmp
++ date
+ echo '[Wed 22 Jul 2020 01:55:38 PM CST] Preparing Lang-8 data... (NOTE:Can take several hours, due to LangID.py filtering...)'
[Wed 22 Jul 2020 01:55:38 PM CST] Preparing Lang-8 data... (NOTE:Can take several hours, due to LangID.py filtering...)
+ python3 scripts/partition_data_into_train_and_dev.py --dataset data/tmp/lang-8-20111007-L1-v2.xml --train data/tmp/lang8-train.xml --dev data/tmp/lang8-dev.xml --limit 0
train data contains 177847 essays
development data contains 0 essays
+ python2 scripts/sentence_pairs_with_ctx.py --train --tokenize --maxtokens 80 --mintokens 1 --input data/tmp/lang8-train.xml --src-ctx data/tmp/lang8.src-trg.ctx --src-src data/tmp/lang8.src-trg.src --trg-trg data/tmp/lang8.src-trg.trg
('data/tmp/lang8-train.xml', ':', 177847, 'essays,', 1960174, 'source sentences,', 22531428, 'tokens.')
('The number of source sentences / essays :', 11)
('The number of tokens / essays :', 126)
(1289824, 'sentence pairs have been added with tokenization.')
('data/tmp/lang8.src-trg.ctx', 27587357, 'tokens')
('data/tmp/lang8.src-trg.src', 17656998, 'tokens')
('data/tmp/lang8.src-trg.trg', 21167581, 'tokens')
++ date
+ echo '[Wed 22 Jul 2020 02:01:08 PM CST] Preparing NUCLE data...'
[Wed 22 Jul 2020 02:01:08 PM CST] Preparing NUCLE data...
+ tar -zxvf ../datasets/release3.3.tar.gz -C data/tmp/
release3.3/
release3.3/bea2019/
release3.3/bea2019/readme.txt
release3.3/bea2019/nucle.train.gold.bea19.m2
release3.3/README
release3.3/data/
release3.3/data/conll14st-preprocessed.conll.ann
release3.3/data/conll14st-preprocessed.m2
release3.3/data/nucle3.2.sgml
release3.3/CoNLL-preproc-README
release3.3/scripts/
release3.3/scripts/nucle_doc.py
release3.3/scripts/README
release3.3/scripts/iparser.py
release3.3/scripts/nuclesgmlparser.py
release3.3/scripts/preprocess.py
release3.3/scripts/parser_feature.py
+ nucle_dir=data/tmp/release3.3
+ python2 data/tmp/release3.3/scripts/preprocess.py -l data/tmp/release3.3/data/nucle3.2.sgml data/tmp/nucle3.2-preprocessed.conll data/tmp/nucle3.2-preprocessed.conll.ann data/tmp/nucle3.2-preprocessed.conll.m2
+ python3 scripts/nucle_preprocess.py data/tmp/nucle3.2-preprocessed.conll data/tmp/nucle3.2-preprocessed.conll.m2 data/tmp/nucle3.2.xml
+ python3 scripts/partition_data_into_train_and_dev.py --dataset data/tmp/nucle3.2.xml --train data/tmp/nucle-train.xml --dev data/tmp/nucle-dev.xml --limit 5000 --m2 data/tmp/nucle3.2-preprocessed.conll.m2 --dev-m2 data/tmp/nucle-dev.raw.m2
train data contains 1107 essays
development data contains 290 essays
+ python2 scripts/sentence_pairs_with_ctx.py --train --maxtokens 80 --mintokens 1 --input data/tmp/nucle-train.xml --src-ctx data/tmp/nucle.src-trg.ctx --src-src data/tmp/nucle.src-trg.src --trg-trg data/tmp/nucle.src-trg.trg
('data/tmp/nucle-train.xml', ':', 1107, 'essays,', 45662, 'source sentences,', 921381, 'tokens.')
('The number of source sentences / essays :', 41)
('The number of tokens / essays :', 832)
(16295, 'sentence pairs have been added without tokenization.')
('data/tmp/nucle.src-trg.ctx', 736107, 'tokens')
('data/tmp/nucle.src-trg.src', 422283, 'tokens')
('data/tmp/nucle.src-trg.trg', 418184, 'tokens')
+ python2 scripts/sentence_pairs_with_ctx.py --dev --maxtokens 80 --mintokens 1 --input data/tmp/nucle-dev.xml --src-ctx data/tmp/nucle-dev.src-trg.ctx --src-src data/tmp/nucle-dev.src-trg.src --trg-trg data/tmp/nucle-dev.src-trg.trg
('data/tmp/nucle-dev.xml', ':', 290, 'essays,', 11454, 'source sentences,', 235884, 'tokens.')
('The number of source sentences / essays :', 39)
('The number of tokens / essays :', 813)
(5003, 'sentence pairs have been added without tokenization.')
('data/tmp/nucle-dev.src-trg.ctx', 228299, 'tokens')
('data/tmp/nucle-dev.src-trg.src', 129235, 'tokens')
('data/tmp/nucle-dev.src-trg.trg', 128090, 'tokens')
+ python3 scripts/m2_preprocess.py --nucle-dev data/tmp/nucle-dev.src-trg.src --dev-m2 data/tmp/nucle-dev.raw.m2 --processed-m2 data/tmp/nucle-dev.preprocessed.m2
5003
+ mkdir -p data/processed
+ BPE_MODEL=models/bpe/mlconvgec_aaai18_bpe.model
+ out_dir=data/processed
+ for ext in ctx src trg
+ cat data/tmp/lang8.src-trg.ctx
+ cat data/tmp/nucle.src-trg.ctx
+ cp data/tmp/nucle-dev.src-trg.ctx data/tmp/valid.src-trg.ctx
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ for ext in ctx src trg
+ cat data/tmp/lang8.src-trg.src
+ cat data/tmp/nucle.src-trg.src
+ cp data/tmp/nucle-dev.src-trg.src data/tmp/valid.src-trg.src
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ for ext in ctx src trg
+ cat data/tmp/lang8.src-trg.trg
+ cat data/tmp/nucle.src-trg.trg
+ cp data/tmp/nucle-dev.src-trg.trg data/tmp/valid.src-trg.trg
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ scripts/apply_bpe.py -c models/bpe/mlconvgec_aaai18_bpe.model
+ cp data/tmp/nucle-dev.preprocessed.m2 data/nucle-dev.preprocessed.m2
+ BASEURL=http://sterling8.d2.comp.nus.edu.sg/downloads/GEC/mlconvgec2018/models
+ curl -L -o data/processed/dict.ctx.txt http://sterling8.d2.comp.nus.edu.sg/downloads/GEC/mlconvgec2018/models/data_bin/dict.src.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  304k  100  304k    0     0  77104      0  0:00:04  0:00:04 --:--:-- 77104
+ curl -L -o data/processed/dict.src.txt http://sterling8.d2.comp.nus.edu.sg/downloads/GEC/mlconvgec2018/models/data_bin/dict.src.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  304k  100  304k    0     0  24282      0  0:00:12  0:00:12 --:--:-- 24109
+ curl -L -o data/processed/dict.trg.txt http://sterling8.d2.comp.nus.edu.sg/downloads/GEC/mlconvgec2018/models/data_bin/dict.trg.txt
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  306k  100  306k    0     0  25554      0  0:00:12  0:00:12 --:--:-- 30388
neko@neko:~/Crosentgec/crosentgec$ 

